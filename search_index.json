[
["index.html", "R para Ciencia de Datos Bienvenida 0.1 Sobre la traducción 0.2 Sobre la versión original en inglés", " R para Ciencia de Datos Garrett Grolemund Hadley Wickham Bienvenida Este es el sitio web de la versión en español de “R for Data Science”, de Hadley Wickham y Garrett Grolemund. Este texto te enseñará cómo hacer ciencia de datos con R: aprenderás a importar datos, llevarlos a la estructura más conveniente, transformarlos, visualizarlos y modelarlos. Con él podrás poner en pŕactica las habilidades necesarias para hacer ciencia de datos. Tal como los químicos aprenden a limpiar tubos de ensayo y ordenar un laboratorio, aprenderás a limpiar datos y crear gráficos— junto a muchas otras habilidades que permiten que la ciencia de datos tenga lugar. En este libro encontrarás las mejores prácticas para desarrollar dichas tareas usando R. También aprenderás a usar la gramática de gráficos, programación letrada e investigación reproducible para ahorrar tiempo. Además, aprenderás a manejar recursos cognitivos para facilitar el hacer descubrimientos al momento de manipular, visualizar y explorar datos. 0.1 Sobre la traducción La traducción de “R para Ciencia de Datos” es un proyecto colaborativo de la comunidad de R de Latinoamérica, que tiene por objetivo hacer R más accesible en la región. El proceso se encuentra actualmente en curso, por lo que progresivamente irán apareciendo en este sitio las versiones en español de los capítulos. En la traducción del libro están participando las siguientes personas (en orden alfabético): Marcela Alfaro, Mónica Alonso, Fernando Álvarez, Zulemma Bazurto, Yanina Bellini, Juliana Benítez, María Paula Caldas, Elio Campitelli, Florencia D’Andrea, Rocío Espada, Joshua Kunst, Patricia Loto, Pamela Matías, Lina Moreno, Paola Prieto, Riva Quiroga, Lucía Rodríguez, Mauricio Vargas, Daniela Vázquez, Melina Vidoni, Roxana N. Villafañe. ¡Muchas gracias por su trabajo! Agradecemos también a Laura Ación y Edgar Ruiz, que hicieron la convocatoria inicial a participar. La administración del repositorio con la traducción ha estado cargo de Mauricio Vargas. La coordinación general y la edición, a cargo de Riva Quiroga. Este proyecto no solo implica la traducción del texto, sino también de los sets de datos que se utilizan a lo largo de él. Para ello, se creó el paquete datos, que contiene las versiones traducidas de estos. El paquete ha estado a cargo de Edgar Ruiz, Riva Quiroga y Mauricio Vargas. Para su creación, se utilizó el paquete datalang de Edgar Ruiz. Si quieres conocer más sobre los principios que han orientado nuestro trabajo y saber cómo participar en el proceso de revisión de las traducciones, puedes leer la documentación del proyecto aquí. 0.2 Sobre la versión original en inglés Puedes consultar la versión original del libro en r4ds.had.co.nz/. Existe una edición impresa, que fue publicada por O’Reilly en enero de 2017. Puedes adquirir una copia en Amazon. (El libro “R for Data Science” primero se llamó “Data Science with R” en “Hands-On Programming with R”) Esta obra se distribuye bajo los términos y condiciones de la licencia Creative Commons Atribución-No Comercial-Sin Derivados 3.0 vigente en los Estados Unidos de América. "],
["comunicar-con-graficos.html", "1 Comunicar con gráficos 1.1 Introducción 1.2 Etiquetas 1.3 Anotaciones 1.4 Escalas 1.5 Haciendo Zoom 1.6 Temas 1.7 Guardando tus gráficos 1.8 Aprendiendo más", " 1 Comunicar con gráficos 1.1 Introducción En [análisis de datos exploratorios] aprendistes a usar gráficos como herramientas de exploración. Cuando haces gráficos exploratorios, sabes incluso antes de mirar, qué variables mostrará el gráfico. Hicistes cada gráfico con un propósito, lo miraste rápidamente y luego pasaste al siguiente. En el transcurso de la mayoría de los análisis, producirás decenas o cientos de gráficos, muchos de los cuales se desecharán inmediatamente. Ahora que comprendes tus datos, debes comunicar tu conocimiento a los demás. Es probable que tu audiencia no comparta tus conocimientos previos y no esté profundamente involucrada en los datos. Para ayudar a otros a construir rápidamente un buen modelo mental de los datos, deberás invertir un esfuerzo considerable para que tus gráficos se expliquen por sí solos . En este capítulo, aprenderás algunas de las herramientas que proporciona ggplot2 para hacerlo. Este capítulo se centra en las herramientas necesarias para crear buenos gráficos. Supongo que sabes lo que quieres y solo te falta saber cómo hacerlo. Por esa razón, recomiendo combinar este capítulo con un buen libro de visualización general. Me gusta especialmente The Truthful Art, de Albert Cairo. No enseña la mecánica de crear visualizaciones, sino que se enfoca en lo que necesitas pensar para crear gráficos efectivos. 1.1.1 Prerrequisitos En este capítulo, nos centraremos una vez más en ggplot2. También usaremos un poco el paquete dplyr para la manipulación de datos y algunos paquetes como ggrepel y viridis que extienden las funciones de ggplot2. En lugar de cargar esas extensiones aquí, nos referiremos a sus funciones de forma explícita, utilizando la notación :: . Esto ayudará a aclarar qué funciones están integradas en ggplot2 y cuáles vienen de otros paquetes. No olvides que deberás instalar esos paquetes con install.packages() si aún no los tienes. library(tidyverse) library(datos) 1.2 Etiquetas El punto de inicio más sencillo para convertir un gráfico exploratorio en un gráfico expositivo es con buenas etiquetas. Agrega etiquetas con la función labs(). Este ejemplo agrega un título al gráfico: ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + labs(title = &quot;La eficiencia en el uso de combustible disminuye con el tamaño del motor&quot;) El propósito del título de un gráfico es resumir el hallazgo principal. Evita títulos que simplemente describen el gráfico, por ejemplo “Diagrama de dispersión del desplazamiento del motor frente al ahorro de combustible”. Si necesitas agregar más texto, hay otras dos etiquetas útiles que puedes usar en ggplot2 versión 2.2.0 y superiores (que deberían estar disponibles para cuando estés leyendo este libro): el subtítulo, del inglés subtitle, agrega detalles adicionales en una fuente más pequeña debajo del título. la leyenda, del inglés caption, agrega texto en la parte inferior derecha del gráfico, suele usarse para describir la fuente de los datos. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + labs( title = &quot;La eficiencia en el uso de combustible disminuye con el tamaño del motor&quot;, subtitle = &quot;Los automóviles deportivos de dos asientos son la excepción debido a su peso liviano&quot;, caption = &quot;Datos de fueleconomy.gov&quot; ) También puedes usar labs() para reemplazar los títulos de ejes y leyendas. Por lo general, es una buena idea reemplazar los nombres cortos de las variables con descripciones más detalladas e incluir las unidades. ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_smooth(se = FALSE) + labs( x = &quot;Desplazamiento del motor (L)&quot;, y = &quot;Economía de combustible de carretera (millas)&quot;, colour = &quot;Tipo de automóvil&quot; ) Es posible usar ecuaciones matemáticas en lugar de cadenas de texto. Simplemente cambia &quot;&quot; por quote() y lee acerca de las opciones disponibles en ?plotmath: df &lt;- tibble( x = runif(10), y = runif(10) ) ggplot(df, aes(x, y)) + geom_point() + labs( x = quote(sum(x[i]^2, i == 1, n)), y = quote(alpha + beta + frac(delta, theta)) ) 1.2.1 Ejercicios Crea un gráfico partiendo de los datos de economía de combustible con etiquetas para title , subtitle, caption, x, y y color personalizadas. La función geom_smooth() es un poco engañosa porque autopista está sesgada positivamente para motores grandes, debido a la inclusión de autos deportivos livianos con motores grandes. Usa tus herramientas de modelado para ajustar y mostrar un modelo mejor. Elige un gráfico exploratorio que hayas creado en el último mes y agrégale títulos informativos para volverlo más fácil de comprender para otros. 1.3 Anotaciones Además de etiquetar las partes principales de tu gráfico, a menudo es útil etiquetar observaciones individuales o grupos de observaciones. La primera herramienta que tienes a tu disposición es geom_text(). La función geom_text() es similar a geom_point(), pero tiene una estética adicional: label. Esto hace posible agregar etiquetas textuales a tus gráficos. Hay dos posibles fuentes de etiquetas. En primer lugar, es posible tener un tibble que proporcione las etiquetas. El siguiente gráfico no es en sí terriblemente útil, pero si lo es su enfoque: filtrar el auto más eficiente de cada clase con dplyr, y luego etiquetarlo en el gráfico: mejor_de_su_clase &lt;- millas %&gt;% group_by(clase) %&gt;% filter(row_number(desc(autopista)) == 1) ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_text(aes(label = modelo), data = mejor_de_su_clase) Esto es difícil de leer porque las etiquetas se superponen entre sí y con los puntos. Podemos mejorar ligeramente las cosas cambiando por geom_label(), que dibuja un rectángulo detrás del texto. También usamos el parámetro nudge_y para mover las etiquetas ligeramente por encima de los puntos correspondientes: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_label(aes(label = modelo), data = mejor_de_su_clase, nudge_y = 2, alpha = 0.5) Esto ayuda un poco, pero si te fijas bien en la esquina superior izquierda verás que hay dos etiquetas prácticamente una encima de la otra. Esto sucede porque el kilometraje y el desplazamiento para los mejores automóviles en las categorías de compactos y subcompactos son exactamente los mismos. No hay forma de que podamos solucionar esto aplicando la misma transformación para cada etiqueta. En cambio, podemos usar el paquete ggrepel de Kamil Slowikowski. Este paquete es muy útil ya que ajusta automáticamente las etiquetas para que no se superpongan: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_point(size = 3, shape = 1, data = mejor_de_su_clase) + ggrepel::geom_label_repel(aes(label = modelo), data = mejor_de_su_clase) Ten en cuenta otra técnica muy práctica utilizada aquí: agregué una segunda capa de puntos grandes y huecos para resaltar los puntos que etiqueté. A veces puedes usar la misma idea para reemplazar la leyenda con etiquetas colocadas directamente en tu gráfico. Los resultados no son maravillosos para este gráfico en particular, pero tampoco son tan malos. (theme(legend.position = &quot;none&quot;) desactiva la leyenda — hablaremos de ello en breve). clase_promedio &lt;- millas %&gt;% group_by(clase) %&gt;% summarise( motor = median(motor), autopista = median(autopista) ) ggplot(millas, aes(motor, autopista, colour = clase)) + ggrepel::geom_label_repel(aes(label = clase), data = clase_promedio, size = 6, label.size = 0, segment.color = NA ) + geom_point() + theme(legend.position = &quot;none&quot;) Alternativamente puede que quieras agregar una única etiqueta al gráfico, pero de todas formas necesitarás generar un conjunto de datos. Puede ocurrir que desees ubicar la etiqueta en la esquina del gráfico, en ese caso es conveniente crear un nuevo marco de datos usando summarise() para calcular los valores máximos de x e y. etiqueta &lt;- millas %&gt;% summarise( motor = max(motor), autopista = max(autopista), etiqueta = &quot;El aumento del tamaño del motor está \\nrelacionado con la disminución en el gasto de combustible.&quot; ) ggplot(millas, aes(motor, autopista)) + geom_point() + geom_text(aes(label = etiqueta), data = etiqueta, vjust = &quot;top&quot;, hjust = &quot;right&quot;) Si deseas colocar el texto exactamente en los bordes del gráfico puedes usar +Inf y -Inf. Como ya no estamos calculando las posiciones de millas, podemos usar tibble() para crear el conjunto de datos: etiqueta &lt;- millas %&gt;% summarise( motor = Inf, autopista = Inf, etiqueta = &quot;El aumento del tamaño del motor está \\nrelacionado con la disminución en el gasto de combustible.&quot; ) ggplot(millas, aes(motor, autopista)) + geom_point() + geom_text(aes(label = etiqueta), data = etiqueta, vjust = &quot;top&quot;, hjust = &quot;right&quot;) En estos ejemplos, separé manualmente la etiqueta en líneas usando “”. Otra posibilidad es usar stringr::str_wrap() para agregar saltos de línea automáticamente, dado el número de caracteres que deseas por línea: &quot;El aumento del tamaño del motor está relacionado con la disminución en el gasto de combustible.&quot; %&gt;% stringr::str_wrap(width = 40) %&gt;% writeLines() #&gt; El aumento del tamaño del motor está #&gt; relacionado con la disminución en el #&gt; gasto de combustible. Ten en cuenta el uso de hjust y vjust para controlar la alineación de la etiqueta. La figura 1.1 muestra las nueve combinaciones posibles. Figure 1.1: Las nueve combinaciones posibles con hjust y vjust. Recuerda que además de geom_text(), en ggplot2 tienes muchos otros geoms disponibles para ayudar a agregar notas a tu gráfico . Algunas ideas: Emplea geom_hline() y geom_vline() para agregar líneas de referencia. A menudo las hago gruesas (size = 2) y blancas (color = white), y las dibujo debajo de la primera capa de datos. Eso las hace fáciles de ver, sin distraer la atención de los datos. Emplea geom_rect() para dibujar un rectángulo alrededor de los puntos de interés. Los límites del rectángulo están definidos por las estéticas xmin,xmax, ymin,ymax. Emplea geom_segment() con el argumento arrow para destacar un punto en particular con una flecha. Usa la estética x e y para definir la ubicación inicial, y xend y yend para definir la ubicación final. ¡El único límite es tu imaginación! (y tu paciencia para posicionar las anotaciones de forma estéticamente agradable) 1.3.1 Ejercicios Usa las infinitas posiciones que permite geom_text() para colocar texto en cada una de las cuatro esquinas del gráfico. Lee la documentación de la función annotate(). ¿Cómo puedes usarla para agregar una etiqueta de texto a un gráfico sin tener que crear un tibble? ¿Cómo interactúan las etiquetas producidas por geom_text() con la separación en facetas? ¿Cómo puedes agregar una etiqueta a una sola faceta? ¿Cómo puedes poner una etiqueta diferente en cada faceta? (Sugerencia: piensa en los datos subyacentes). ¿Qué argumentos para geom_label() controlan la apariencia de la caja que se ve atrás? ¿Cuáles son los cuatro argumentos de arrow()? ¿Cómo funcionan? Crea una serie de gráficos que demuestren las opciones más importantes. 1.4 Escalas La tercera forma en que puedes mejorar tu gráfico para comunicar es ajustar las escalas. Las escalas controla el mapeo de los valores de los datos a cosas que puedes percibir. Normalmente, ggplot2 agrega escalas automáticamente. Por ejemplo, cuando tipeas: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) ggplot2 agrega automáticamente escalas predeterminadas detrás de escena: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + scale_x_continuous() + scale_y_continuous() + scale_colour_discrete() Ten en cuenta que los nombres de las escalas comienzan siempre igual: scale_ seguido del nombre de la estética, luego _ y finalmente el nombre de la escala. Las escalas predeterminadas se nombran según el tipo de variable con la que se alinean: continua, discreta, fecha y hora (datetime) o fecha. Hay muchas escalas no predeterminadas que aprenderás a continuación. Las escalas predeterminadas se han elegido cuidadosamente para ser adecuadas para una gama amplia de valores. Sin embargo, es posible que desees sobrescribir los valores predeterminados por dos razones: Es posible que desees modificar algunos de los parámetros de la escala predeterminada. Esto te permite hacer cosas como cambiar los intervalos de valores en los ejes o las etiquetas de cada valor visible. Es posible que desees reemplazar la escala por completo, y utilizar un algoritmo completamente diferente. Por lo general tu opción será mejor que la predeterminada ya que sabes más acerca de los datos. 1.4.1 Marcas de los ejes y leyendas Hay dos argumentos principales que afectan la apariencia de las marcas, del inglés ticks, en los ejes y las leyendas:breaks y labels, del inglés quiebre y etiqueta respectivamente. Los breaks controlan la posición de las marcas en los ejes o los valores asociados con las leyendas. Las labels controlan la etiqueta de texto asociada con cada marca/leyenda. El uso más común de los breaks es redefinir la opción predeterminada: ggplot(millas, aes(motor, autopista)) + geom_point() + scale_y_continuous(breaks = seq(15, 40, by = 5)) Puedes usar labels de la misma manera (un vector de caracteres de la misma longitud que breaks), o puedes establecerlas como NULL del inglés nulo, para suprimir las etiquetas por completo. Esto es útil para mapas, o para publicar gráficos donde no puedes compartir los números absolutos. ggplot(millas, aes(motor, autopista)) + geom_point() + scale_x_continuous(labels = NULL) + scale_y_continuous(labels = NULL) También puedes usar breaks y labels para controlar la apariencia de las leyendas. En conjunto, los ejes y las leyendas se llaman guías. Los ejes se usan para la estética de x e y; las leyendas se usan para todo lo demás. Otro uso de los breaks es cuando tienes relativamente pocos puntos de datos y deseas resaltar exactamente dónde se producen las observaciones. Como ejemplo, el siguiente gráfico muestra cuándo comenzó y terminó su mandato cada presidente de los Estados Unidos. presidencial %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(inicio, id)) + geom_point() + geom_segment(aes(xend = fin, yend = id)) + scale_x_date(NULL, breaks = presidencial$inicio, date_labels = &quot;&#39;%y&quot;) Ten en cuenta que la especificación de breaks y labels para escalas en formato de fecha y fecha y hora es ligeramente diferente: date_labels toma en cuenta la especificación de formato, en la misma forma que parse_datetime(). date_breaks (no se muestra aquí), toma una cadena como “2 días” o “1 mes”. 1.4.2 Diseño de leyendas Con mayor frecuencia utilizarás breaks y labels para ajustar los ejes. Aunque ambos también funcionan con leyendas, hay algunas otras técnicas que podrías usar. Para controlar la posición general de la leyenda, debes usar una configuración de theme() del inglés tema. Volveremos a los temas al final del capítulo, pero en resumen, controlan las partes del gráfico que no son de datos. La configuración del tema legend.position del inglés posición de la leyenda, controla dónde se dibuja la leyenda: base &lt;- ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) base + theme(legend.position = &quot;left&quot;) base + theme(legend.position = &quot;top&quot;) base + theme(legend.position = &quot;bottom&quot;) base + theme(legend.position = &quot;right&quot;) # the default También puedes usar legend.position = &quot;none&quot; para suprimir por completo la visualización de la leyenda. Para controlar la visualización de leyendas individuales, usa guides() junto con guide_legend() o guide_colourbar(). El siguiente ejemplo muestra dos configuraciones importantes: controlar el número de filas que usa la leyenda con nrow, y redefinir una de las estéticas para agrandar los puntos. Esto es particularmente útil si has usado un valor de alfa bajo para mostrar muchos puntos en un diagrama. ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_smooth(se = FALSE) + theme(legend.position = &quot;bottom&quot;) + guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4))) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 1.4.3 Reemplazando una escala En lugar de simplemente modificar un poco los detalles, puedes reemplazar la escala por completo. Hay dos tipos de escalas que es probable que desees cambiar: escalas de posición continua y escalas de color. Afortunadamente, los mismos principios se aplican a todos los demás aspectos estéticos, por lo que una vez que hayas dominado la posición y el color serás capaz de realizar rápidamente otros reemplazos de escala. Esto es muy útil para graficar transformaciones de tu variable. A modo de ejemplo, como hemos visto en diamond prices, es más fácil ver la relación precisa entre quilate y precio si aplicamos una transformación logarítmica en base 10: ggplot(diamantes, aes(quilate, precio)) + geom_bin2d() ggplot(diamantes, aes(log10(quilate), log10(precio))) + geom_bin2d() Sin embargo, la desventaja de esta transformación es que los ejes ahora están etiquetados con los valores transformados, por lo que se vuelve difícil interpretar el gráfico. En lugar de hacer la transformación en el mapeo estético, podemos hacerlo con la escala. Esto es visualmente idéntico, excepto que los ejes están etiquetados en la escala original de los datos. ggplot(diamantes, aes(quilate, precio)) + geom_bin2d() + scale_x_log10() + scale_y_log10() Otra escala que se personaliza con frecuencia es el color. La escala categórica predeterminada selecciona los colores que están espaciados uniformemente alrededor del círculo cromático. Otras alternativas útiles son las escalas de ColorBrewer que han sido ajustadas manualmente para que funcionen mejor para personas con tipos comunes de daltonismo. Los dos gráficos de abajo se ven similares, sin embargo hay suficiente diferencia en los tonos rojo y verde tal que los puntos de la derecha pueden distinguirse incluso por personas con daltonismo rojo-verde. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion)) ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion)) + scale_colour_brewer(palette = &quot;Set1&quot;) No olvides las técnicas más simples. Si solo hay unos pocos colores, puedes agregar un mapeo de forma redundante. Esto también ayudará a asegurar que tu gráfico sea interpretable en blanco y negro. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion, shape = traccion)) + scale_colour_brewer(palette = &quot;Set1&quot;) Las escalas de ColorBrewer están documentadas en línea en http://colorbrewer2.org/ y están disponibles en R en el paquete RColorBrewer de Erich Neuwirth. La figura 1.2 muestra la lista completa de paletas de colores disponibles. Las paletas secuenciales (arriba) y divergentes (abajo) son particularmente útiles si sus valores categóricos están ordenados o tienen un “centro”. Esto a menudo ocurre si has utilizado cut() para convertir una variable continua en una variable categórica. Figure 1.2: All ColourBrewer scales. Cuando tengas un mapeo predefinido entre valores y colores, usa scale_colour_manual(). Por ejemplo, si mapeamos los partidos presidenciales de Estados Unidos en color, queremos usar el mapeo estándar de color rojo para republicanos y azul para demócratas: presidencial %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(inicio, id, colour = partido)) + geom_point() + geom_segment(aes(xend = fin, yend = id)) + scale_colour_manual(values = c(Republicano = &quot;red&quot;, Demócrata = &quot;blue&quot;)) Para generar una escala de color para variables continuas , puedes usar built in scale_colour_gradient() o scale_fill_gradient(). Si tienes una escala divergente, puedes usar scale_colour_gradient2(). Eso tepermite dar, por ejemplo, diferentes colores a valores positivos y negativos. Esto a veces también es útil si quieres distinguir puntos por encima o por debajo de la media. Otra opción es scale_colour_viridis() proporcionada por el paquete viridis. Es un análogo continuo de las escalas categóricas de ColorBrewer. Los diseñadores, Nathaniel Smith y Stéfan van der Walt, adaptaron cuidadosamente una paleta de color para variables continuas que tiene buenas propiedades perceptuales. Aquí hay un ejemplo de viridis: df &lt;- tibble( x = rnorm(10000), y = rnorm(10000) ) ggplot(df, aes(x, y)) + geom_hex() + coord_fixed() ggplot(df, aes(x, y)) + geom_hex() + viridis::scale_fill_viridis() + coord_fixed() Ten en cuenta que todas las escalas de color vienen en dos variedades: scale_colour_x() y scale_fill_x() para la estética color y fill, respectivamente (las escalas de color se expresan tanto en inglés americano como británico). 1.4.4 Ejercicios ¿Por qué el siguiente código no reemplaza la escala predeterminada? ggplot(df, aes(x, y)) + geom_hex() + scale_colour_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() ¿Cuál es el primer argumento para cada escala? ¿Cómo se compara con labs()? Cambia la visualización de los términos presidenciales de las siguientes maneras: Combinando las dos variantes que se muestran arriba. Mejorando la visualización del eje y. Etiquetando cada término con el nombre del presidente. Agregandoetiquetas informativas al gráfico. Poniendo intervalos de 4 años (¡esto es más complicado de lo que parece!). Utiliza override.aes para que la leyenda en el siguiente gráfico sea más fácil de ver: ggplot(diamantes, aes(quilate, precio)) + geom_point(aes(colour = corte, alpha = 1 / 20)) 1.5 Haciendo Zoom Hay tres formas de controlar los límites de un gráfico: Modificando los datos que se grafican Estableciendo los límites en cada escala Estableciendo xlim y ylim en coord_cartesian() Para ampliar una región del gráfico, generalmente es mejor usar coord_cartesian().Compara los siguientes dos gráficos: ggplot(millas, mapping = aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth() + coord_cartesian(xlim = c(5, 7), ylim = c(10, 30)) millas %&gt;% filter(motor &gt;= 5, motor &lt;= 7, autopista &gt;= 10, autopista &lt;= 30) %&gt;% ggplot(aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth() También puedes establecer limits del inglés límites en escalas individuales. La reducción de los límites del gráfico es equivalente a seleccionar un subconjunto de los datos. En general es más útil si deseas expandir los límites, por ejemplo, cuando quieres hacer coincidir escalas de diferentes gráficos. A modo de ejemplo, si extraemos dos clases de automóviles y los graficamos por separado, son difíciles de comparar ya que las tres escalas (el eje x, el eje y y la estética del color) tienen rangos diferentes. suv &lt;- millas %&gt;% filter(clase == &quot;suv&quot;) compacto &lt;- millas %&gt;% filter(clase == &quot;compact&quot;) ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() ggplot(compacto, aes(motor, autopista, colour = traccion)) + geom_point() Una forma de superar este problema es compartir la escala entre varios gráficos, estableciendo una escala única a partir de los límites del conjunto de datos completo. x_scale &lt;- scale_x_continuous(limits = range(millas$motor)) y_scale &lt;- scale_y_continuous(limits = range(millas$autopista)) col_scale &lt;- scale_colour_discrete(limits = unique(millas$traccion)) ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() + x_scale + y_scale + col_scale ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() + x_scale + y_scale + col_scale En este caso particular podrías haber simplemente empleado la separación en facetas, pero esta técnica es más útil en general, por ejemplo, si deseas realizar gráficos en varias páginas de un informe. 1.6 Temas Finalmente, puedes personalizar los elementos de tu gráfico que no son datos aplicando un tema: ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + theme_bw() ggplot2 incluye ocho temas por defecto, como se muestra en la Figura 1.3. Muchos otros están incluidos en paquetes adicionales como ggthemes (https://github.com/jrnold/ggthemes), de Jeffrey Arnold. Figure 1.3: The eight themes built-in to ggplot2. Muchas personas se preguntan por qué el tema predeterminado tiene un fondo gris. Esta fue una elección deliberada ya que el fondo gris pone los datos por delante mientras siguen siendo visibles las líneas de la cuadrícula. Las líneas blancas de la cuadrícula son visibles (lo cual es importante porque ayudan significativamente a evaluar la posición), pero tienen poco impacto visual y son fáciles de eliminar. El fondo gris le da a al gráfico un color tipográfico similar al del texto, asegurando que los gráficos encajen con el flujo de un documento sin saltar con un fondo blanco brillante. Finalmente, el fondo gris crea un campo continuo de color que asegura que el gráfico se perciba como una sola entidad visual. También es posible controlar componentes individuales de cada tema, como el tamaño y el color de la fuente utilizada para el eje y. Desafortunadamente, este nivel de detalle está fuera del alcance de este libro, por lo que deberás leer el libro ggplot2 book para obtener todos los detalles. También puedes crear tus propios temas, si estás tratando de hacer coincidir un estilo corporativo o de revista en particular. 1.7 Guardando tus gráficos Hay dos formas principales de obtener tus gráficos desde R: ggsave() y knitr. ggsave() guardarán el gráfico más reciente en el disco. ggplot(millas, aes(motor, autopista)) + geom_point() ggsave(&quot;my-plot.pdf&quot;) #&gt; Saving 6 x 3.7 in image Si no especificas width y height, del inglés el ancho y el alto, se usarán las dimensiones del dispositivo empleado para graficar. Para que el código sea reproducible, necesitarás especificarlos. En general, sin embargo, creo que deberías armar tus informes finales utilizando R Markdown, por lo que quiero centrarme en las opciones importantes para los bloques de código que debes conocer para graficar. Puedes obtener más información sobre ggsave() en la documentación. 1.7.1 Redimensionar una figura El mayor desafío de los gráficos en R Markdown es conseguir que tus figuras tengan el tamaño y la forma correctos. Hay cinco opciones principales que controlan el tamaño de la figura: fig.width, fig.height, fig.asp, out.width y out.height. El tamaño de la imagen es un desafío porque hay dos tamaños (el tamaño de la figura creada por R y el tamaño al que se inserta en el documento de salida) y varias formas de especificarlo (es decir, altura, ancho y relación de aspecto: elige dos de tres). Solo uso tres de las cinco opciones: Encuentro estéticamente más agradable que los gráficos tengan un ancho consistente. Para hacer cumplir esto, configuro fig.width = 6 (6 &quot;) y fig.asp = 0.618 (la proporción áurea) en los valores predeterminados. Luego, en bloques individuales, solo ajusto fig.asp. Controlo el tamaño de salida con out.width y lo configuro a un porcentaje del ancho de línea). De manera predeterminada, out.width = &quot;70%&quot; y fig.align = &quot;center&quot;. Eso le da a los gráficos cierto espacio para respirar, sin ocupar demasiado espacio. Para poner múltiples gráficos en una sola fila, establezco out.width en 50% para dos gráficos, 33% en 3 gráficos, o 25% en 4 gráficos, y setfig.align = &quot;default&quot;. Dependiendo de lo que intento ilustrar (por ejemplo, mostrar datos o variacionesdel gráfico), también modificaré fig.width cómo se explica a continuación. Si observas que tienes que entrecerrar los ojos para leer el texto de tu gráfico, debes ajustar fig.width. Si fig.width es mayor que el tamaño de la figura en el documento final, el texto será demasiado pequeño; si fig.width es más pequeño, el texto será demasiado grande. A menudo necesitarás experimentar un poco para calcular la proporción correcta entre fig.width y el ancho asociado en tu documento. Para ilustrar el principio, los siguientes tres gráficos tienen fig.width de 4, 6 y 8, respectivamente: Si deseas asegurarte que el tamaño de fuente es el mismo en todas tus figuras, al establecer out.width, también necesitarás ajustar fig.width para mantener la misma proporción en relación al out.width predeterminado. Por ejemplo, si tu valor predeterminado de fig.width es 6 y out.width es 0.7, cuando establezcas out.width = &quot;50%&quot; necesitarás establecer fig.width a 4.3 (6 * 0.5 / 0.7). 1.7.2 Otras opciones importantes Al mezclar código y texto, como hago en este libro, recomiendo configurar fig.show = &quot;hold&quot; para que los gráficos se muestren después del código. Esto tiene el agradable efecto secundario de obligarte a dividir grandes bloques de código con sus explicaciones. Para agregar un título al gráfico, usa fig.cap. En R Markdown esto cambiará la figura de “inline” a “floating”. Si estás produciendo resultados en formato PDF, el tipo de gráficos predeterminado es PDF. Esta es una buena configuración predeterminada porque los PDF son gráficos vectoriales de alta calidad. Sin embargo, pueden generar gráficos muy grandes y lentos si muestras miles de puntos. En ese caso, configura dev = &quot;png&quot; para forzar el uso de PNG. Son de calidad ligeramente inferior, pero serán mucho más compactos. Es una buena idea darles nombres a los bloques de código que producen figuras, incluso si no etiquetas rutinariamente otros bloques. Etiquetar el bloque se utiliza para generar el nombre de archivo del gráfico en el disco, por lo que darle un nombre a los bloques hace que sea mucho más fácil seleccionar gráficas y reutilizarlas en otras circunstancias (por ejemplo, si deseas colocar rápidamente un solo gráfico en un correo electrónico o un tweet). 1.8 Aprendiendo más El mejor lugar para aprender más es el libro de ggplot2: ggplot2: Elegant graphics for data analysis. Este explica con mucha más profundidad la teoría subyacente y tiene muchos más ejemplos de cómo combinar las piezas individuales para resolver problemas prácticos. Desafortunadamente, el libro no está disponible en línea de forma gratuita, aunque puede encontrar el código fuente en https://github.com/hadley/ggplot2-book. Otro gran recurso es la guía de extensiones ggplot2 http://www.ggplot2-exts.org/. Este sitio enumera muchos de los paquetes que amplían ggplot2 con nuevos geoms y escalas. Es un buen lugar para comenzar si tratas de hacer algo que parece difícil con ggplot2. "],
["introduccion-1.html", "2 Introducción", " 2 Introducción Hasta aquí, hemos aprendido a usar las herramientas para importar tus datos en R, ordenarlos de una manera conveniente para el análisis, y luego interpretarlos a través de su transformación, visualización y modelado. Sin embargo, no importa lo bien que esté hecho tu análisis si no puedes explicarlo de manera sencilla a otros: es decir, que es necesario comunicar tus resultados. La comunicación de resultados es el tema de los siguientes cuatro capítulos. En [el capítulo: R Markdown], aprenderás sobre dicho paquete, el cual es una herramienta para integrar texto, código y resultados. Puedes usarlo en modo notebook, es decir, en un entorno interactivo de ejecución de código para la comunicación de analista-a-analista, y en modo reporte para la comunicación de analista-a-tomadores-de-decisión. Gracias al potencial de los formatos de R Markdown, incluso puedes usar el mismo documento para ambos propósitos. En [el capítulo: Gráficos para la comunicación], aprenderás cómo convertir tus gráficos exploratorios en gráficos explicativos, los cuales ayudarán a quien ve tu análisis por primera vez a comprender de qué se trata de manera fácil y sencilla. En [el capítulo: Formatos de R Markdown], aprenderás un poco sobre la gran variedad de salidas que puedes generar usando la librería R Markdown, incluyendo dashboards (tableros de control), sitios web, y libros. Terminaremos con [el flujo de trabajo de R Markdown], donde aprenderás sobre “analysis notebook”, en otras palabras, aprenderás sobre el modo notebook para realizar el análisis y registrar sistemáticamente tus logros y fallas para que puedas aprender de ellos. Desafortunadamente, estos capítulos se enfocan principalmente en la parte técnica de la comunicación, y no en los verdaderos grandes problemas de comunicar tus pensamientos a otras personas. Sin embargo, existe una gran cantidad de excelentes libros que abordan esta problemática, cuyas referencias estarán disponibles al final de cada capítulo. "],
["contributing.html", "3 Contributing", " 3 Contributing This book has been developed in the open, and it wouldn’t be nearly as good without your contributions. There are a number of ways you can help make the book even better: If you don’t understand something, please let me know. Your feedback on what is confusing or hard to understand is valuable. If you spot a typo, feel free to edit the underlying page and send a pull request. If you’ve never done this before, the process is very easy: Click the edit this page on the sidebar. Make the changes using github’s in-page editor and save. Submit a pull request and include a brief description of your changes. “Fixing typos” is perfectly adequate. If you make significant changes, include the phrase “I assign the copyright of this contribution to Hadley Wickham” - I need this so I can publish the printed book. "],
["horas-y-fechas.html", "4 Horas y fechas 4.1 Introducción 4.2 Creando horas/fechas 4.3 Componentes fecha-hora 4.4 Plazos de tiempo 4.5 Husos horarios", " 4 Horas y fechas 4.1 Introducción Este capítulo te mostrará cómo trabajar con fechas y horas en R. A primera vista, esto parece sencillo. Las usas en todo momento en tu vida regular, y no parecen causar demasiada confusión. Sin embargo, cuanto más aprendes de horas y fechas, más complicadas se vuelven. Para prepararnos, intenta estas preguntas sencillas: ¿Todos los años tienen 365 días? ¿Todos los días tienen 24 horas? ¿Cada minuto tiene 60 segundos? Estoy seguro que sabes que no todos los años tienen 365 días ¿pero acaso conoces la regla entera para determinar si un año es bisiesto? (Tiene tres partes, de hecho). Puedes recordar que muchas partes del mundo usan horarios de verano (las siglas en inglés son DST, por daylight savings time), así que algunos días tienen 23 horas, y otros tienen 25. Puede ser que no supieras que algunos minutos tienen 61 segundos, porque de vez en cuando se agregan segundos adicionales ya que la rotación de la tierra se hace cada vez más lenta. Las fechas y las horas son complicadas porque tienen que reconciliar dos fenómenos físicos (la rotación de la Tierra, y su órbita alrededor del sol), con todo un conjunto de fenómenos geopolíticos que incluyen a los meses, los husos horarios y los horarios de verano (DST). Este capítulo no te enseñará cada detalle sobre las horas y fechas, pero te dará un sólido fundamento de habilidades prácticas que te ayudarán con los desafíos más comunes de análisis de datos. 4.1.1 Requisitos previos Este capítulo se centra en el paquete lubridate, que simplifica el trabajo con fechas y tiempo en R. lubridate no es parte de los paquetes centrales de tidyverse porque sólo se necesita al trabajar con fechas/horas. A su vez, necesitaremos vuelos del conjunto de datos traducidos que acompañan a este libro. library(tidyverse) library(lubridate) library(datos) 4.2 Creando horas/fechas Hay tres tipos de datos de horas/fechas que se refieren a un instante en el tiempo: Un date o fecha (del inglés, fecha). Un tibble lo imprime como &lt;date&gt;. Un time u hora (del ingles, hora) dentro de un día. Los tibbles lo imprimen como &lt;time&gt;. Un date-time o fecha-hora (del inglés, fecha-hora) es una fecha con una hora adjunta: identifica de forma única como un instante en el tiempo (típicamente al segundo más cercano). Los tibbles imprimen esto como &lt;dttm&gt;. En otras partes de R, éstos se llaman POSIXct, pero yo no creo que sea un nombre muy útil. En este capítulo sólo nos concentraremos en fechas (date) y fechas con horas (date-time) ya que R no tiene una clase nativa para almacenar horas. Si necesitas una, puedes usar el paquete hms. Siempre deberías usar el tipo de datos más sencillo que se ajusta a tus necesidades. Esto significa que si puedes usar un date en lugar de un date-time, deberías hacerlo. Las fechas-horas son sustancialmente más complicadas porque necesitas gestionar los husos horarios, a los cuales estudiaremos al final del capítulo. Para obtener la fecha (date) o fecha-hora (date-time) actual, puedes usar today() (del inglés, hoy) o now() (del inglés, ahora): today() #&gt; [1] &quot;2019-05-14&quot; now() #&gt; [1] &quot;2019-05-14 20:15:46 UTC&quot; De otra forma, hay tres modos en los que puedes crear una fecha/hora: Desde una cadena de caracteres (o string, en inglés). Desde componentes date-time individuales. Desde un objeto fecha-hora existente. Éstos funcionan de la siguiente manera. 4.2.1 Desde cadenas de caracteres Los datos de fecha/hora a menudo vienen como cadenas de caracteres. Ya has visto una forma de análisis gramatical de strings hacia date-times en date-times. Otra forma es usar los ayudantes provistos por lubridate. Ellos automáticamente trabajan el formato una vez que especificas el orden de los componentes. Para usarlos, identifica el orden en el cual el año, mes y día aparecen en tus fechas, y luego agrega “y” (del inglés year), “m” (mes) y “d” (día) en el mismo orden. Esto te da el nombre de la función lubridate que traducirá tu fecha. Por ejemplo: ymd(&quot;2017-01-31&quot;) #&gt; [1] &quot;2017-01-31&quot; mdy(&quot;Enero 31, 2017&quot;) #&gt; Warning: All formats failed to parse. No formats found. #&gt; [1] NA dmy(&quot;31-Ene-2017&quot;) #&gt; Warning: All formats failed to parse. No formats found. #&gt; [1] NA Estas funciones también reciben números sin comillas. Esta es la forma más concisa de crear un sólo objeto fecha/hora, ya que puedes necesitarlo cuando filtres datos temporales. ymd() (del inglés año-mes-día) es corto y no ambigüo: ymd(20170131) #&gt; [1] &quot;2017-01-31&quot; ymd() y funciones amigas crean fechas (date). Para generar un date-time, agrega un guión bajo y uno o más de “h”, “m” y “s” al nombre de la función de análisis: ymd_hms(&quot;2017-01-31 20:11:59&quot;) #&gt; [1] &quot;2017-01-31 20:11:59 UTC&quot; mdy_hm(&quot;01/31/2017 08:01&quot;) #&gt; [1] &quot;2017-01-31 08:01:00 UTC&quot; También puedes forzar la creación de un date-time desde una fecha, al proveer un huso horario: ymd(20170131, tz = &quot;UTC&quot;) #&gt; [1] &quot;2017-01-31 UTC&quot; 4.2.2 Desde componentes individuales En lugar de una cadena de caracteres simple, a veces tienes los componentes individuales de una fecha/hora esparcidos en múltiples columnas. Esto es lo que tenemos en los datos de vuelos: vuelos %&gt;% select(anio, mes, dia, hora, minuto) #&gt; # A tibble: 336,776 x 5 #&gt; anio mes dia hora minuto #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 15 #&gt; 2 2013 1 1 5 29 #&gt; 3 2013 1 1 5 40 #&gt; 4 2013 1 1 5 45 #&gt; 5 2013 1 1 6 0 #&gt; 6 2013 1 1 5 58 #&gt; # … with 3.368e+05 more rows Para crear una fecha y hora desde este tipo de entrada, usa make_date() (del inglés, crear fechas) para las fechas, o make_datetime() (del inglés crear fecha-hora) para las fechas-horas: vuelos %&gt;% select(anio, mes, dia, hora, minuto) %&gt;% mutate(salida = make_datetime(anio, mes, dia, hora, minuto)) #&gt; # A tibble: 336,776 x 6 #&gt; anio mes dia hora minuto salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 2013 1 1 5 15 2013-01-01 05:15:00 #&gt; 2 2013 1 1 5 29 2013-01-01 05:29:00 #&gt; 3 2013 1 1 5 40 2013-01-01 05:40:00 #&gt; 4 2013 1 1 5 45 2013-01-01 05:45:00 #&gt; 5 2013 1 1 6 0 2013-01-01 06:00:00 #&gt; 6 2013 1 1 5 58 2013-01-01 05:58:00 #&gt; # … with 3.368e+05 more rows Hagamos esto mismo para cada una de las cuatro columnas de tiempo en vuelos. Las horas están representadas en un formato ligeramente más extraño, así que usamos el módulo aritmético para extraer los componentes de horas y minutos. Una vez que hayamos creado las variables date-time, nos centraremos en las variables que usaremos por el resto del capítulo1. hacer_fechahora_100 &lt;- function(anio, mes, dia, tiempo) { make_datetime(anio, mes, dia, tiempo %/% 100, tiempo %% 100) } vuelos_dt &lt;- vuelos %&gt;% filter(!is.na(horario_salida), !is.na(horario_llegada)) %&gt;% mutate( horario_salida = hacer_fechahora_100(anio, mes, dia, horario_salida), horario_llegada = hacer_fechahora_100(anio, mes, dia, horario_llegada), salida_programada = hacer_fechahora_100(anio, mes, dia, salida_programada), llegada_programada = hacer_fechahora_100(anio, mes, dia, llegada_programada) ) %&gt;% select(origen, destino, starts_with(&quot;atraso&quot;), starts_with(&quot;horario&quot;), ends_with(&quot;programada&quot;), tiempo_vuelo) vuelos_dt #&gt; # A tibble: 328,063 x 9 #&gt; origen destino atraso_salida atraso_llegada horario_salida #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 EWR IAH 2 11 2013-01-01 05:17:00 #&gt; 2 LGA IAH 4 20 2013-01-01 05:33:00 #&gt; 3 JFK MIA 2 33 2013-01-01 05:42:00 #&gt; 4 JFK BQN -1 -18 2013-01-01 05:44:00 #&gt; 5 LGA ATL -6 -25 2013-01-01 05:54:00 #&gt; 6 EWR ORD -4 12 2013-01-01 05:54:00 #&gt; # … with 3.281e+05 more rows, and 4 more variables: #&gt; # horario_llegada &lt;dttm&gt;, salida_programada &lt;dttm&gt;, #&gt; # llegada_programada &lt;dttm&gt;, tiempo_vuelo &lt;dbl&gt; Con estos datos, puedo visualizar la distribución de las horas de salida a lo largo del año: vuelos_dt %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 86400) # 86400 segundos = 1 día O para un solo día: vuelos_dt %&gt;% filter(horario_salida &lt; ymd(20130102)) %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 600) # 600 segundos = 10 minutos Mira con detenimiento, ya que cuando usas date-times en un contexto numérico (como en un histograma), 1 significa un segundo, entonces un binwidth (del inglés, ancho del cajón) de 86400 significa un día. Para las fechas, 1 significa un día. 4.2.3 Desde otros tipos Puedes querer cambiar entre una fecha-hora y una fecha. Ese es el trabajo de as_datetime() (del inglés, como fecha-hora) y as_date() (del inglés, como fecha): as_datetime(today()) #&gt; [1] &quot;2019-05-14 UTC&quot; as_date(now()) #&gt; [1] &quot;2019-05-14&quot; A veces, tendrás fechas/horas como desfasajes numéricos de la “Época Unix” similares a 1970-01-01. Si el desfasaje es un segundos, usa as_datetime(); si es en días, usa as_date(). as_datetime(60 * 60 * 10) #&gt; [1] &quot;1970-01-01 10:00:00 UTC&quot; as_date(365 * 10 + 2) #&gt; [1] &quot;1980-01-01&quot; 4.2.4 Ejercicios ¿Qué sucede si analizas una cadena de caracteres que contiene fechas inválidas? ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) ¿Qué hace el argumento tzone (del inglés, huso horario abreviado) para today()? ¿Por qué es importante? Usa la función de lubridate apropiada para analizar las siguientes fechas: d1 &lt;- &quot;Enero 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;Agosto 19 (2015)&quot;, &quot;Julio 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Diciembre 30, 2014 4.3 Componentes fecha-hora Ahora que ya conoces cómo tener datos de fechas y horas en las estructuras de datos de R, vamos a explorar qué puedes hacer con ellos. Esta sección se concentrará en las funciones de acceso que te permiten obtener y configurar componentes individuales. La siguiente sección mirará el trabajo aritmético con fechas-horas. 4.3.1 Obteniendo los componentes Puedes obtener las partes individuales de una fecha con las funciones de acceso year() (del inglés, año), month() (del inglés, mes), mday() (del inglés, día del mes), yday() (del inglés, día del año), wday() (del inglés, día de la semana), hour() (del inglés, hora), minute() (del inglés, minuto), y second() (del inglés, segundo). fechahora &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;) year(fechahora) #&gt; [1] 2016 month(fechahora) #&gt; [1] 7 mday(fechahora) #&gt; [1] 8 yday(fechahora) #&gt; [1] 190 wday(fechahora) #&gt; [1] 6 Para month() y wday() puedes configurar label = TRUE para retornar el nombre abreviado del mes o del día de la semana. Usa abbr = FALSE para retornar el nombre completo. month(fechahora, label = TRUE) #&gt; [1] Jul #&gt; 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec wday(fechahora, label = TRUE, abbr = FALSE) #&gt; [1] Friday #&gt; 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday Podemos usar wday() para ver que más vuelos salen durante la semana que durante el fin de semana: vuelos_dt %&gt;% mutate(dia_semana = wday(horario_salida, label = TRUE)) %&gt;% ggplot(aes(x = dia_semana)) + geom_bar() Hay un patrón interesante si miramos a la demora promedio por minuto dentro de la hora. ¡Parece que los vuelos que salen en los minutos 20-30 y 50-30 tienen mucho más demora que en el resto de las horas! vuelos_dt %&gt;% mutate(minuto = minute(horario_salida)) %&gt;% group_by(minuto) %&gt;% summarise( atraso_promedio = mean(atraso_llegada, na.rm = TRUE), n = n() ) %&gt;% ggplot(aes(minuto, atraso_promedio)) + geom_line() De forma interesante, si miramos al horario programado de salida, no vemos un patrón tan prominente: salida_programada &lt;- vuelos_dt %&gt;% mutate(minuto = minute(salida_programada)) %&gt;% group_by(minuto) %&gt;% summarise( atraso_promedio = mean(atraso_llegada, na.rm = TRUE), n = n() ) ggplot(salida_programada, aes(minuto, atraso_promedio)) + geom_line() Entonces ¿por qué vemos ese patrón con los horarios reales de salida? Bueno, como muchos datos recolectados por los humanos, hay un sesgo importante hacia los vuelos que salen en horas “lindas”. ¡Mantente siempre alerta respecto a este tipo de patrón, cuando sea que trabajes con datos que involucran al juicio humano! ggplot(salida_programada, aes(minuto, n)) + geom_line() 4.3.2 Redondeo Un método alternativo para graficar los componentes individuales es redondear la fecha a una unidad de tiempo cercana, con floor_date() (del inglés, redondear fecha hacia abajo), round_date() (del inglés, redondear fecha), y ceiling_date() (del inglés, redondear fecha hacia arriba). Cada función toma un vector de fechas para ajustarlas, y luego el nombre de la unidad redondeada hacia abajo (con floor), hacia arriba (con ceiling), o al mas cercano. Esto, por ejemplo, nos permite graficiar el número de vuelos por semana: vuelos_dt %&gt;% count(semana = floor_date(horario_salida, &quot;week&quot;)) %&gt;% ggplot(aes(semana, n)) + geom_line() Calcular la diferencia entre una fecha redondeada y una sin redondear puede ser particularmente útil. 4.3.3 Configurando componentes También puedes usar las funciones de acceso para darle un valor a los componentes de las fechas/horas: (fechahora &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;)) #&gt; [1] &quot;2016-07-08 12:34:56 UTC&quot; year(fechahora) &lt;- 2020 fechahora #&gt; [1] &quot;2020-07-08 12:34:56 UTC&quot; month(fechahora) &lt;- 01 fechahora #&gt; [1] &quot;2020-01-08 12:34:56 UTC&quot; hour(fechahora) &lt;- hour(fechahora) + 1 fechahora #&gt; [1] &quot;2020-01-08 13:34:56 UTC&quot; Alternativamente, en lugar de modificar en un sólo lugar, puedes crear una nueva fecha-hora con update() (del inglés, actualizar). Esto también te permite configurar múltiples valores al mismo tiempo. update(fechahora, year = 2020, month = 2, mday = 2, hour = 2) #&gt; [1] &quot;2020-02-02 02:34:56 UTC&quot; Si los valores son demasiado grandes, darán la vuelta: ymd(&quot;2015-02-01&quot;) %&gt;% update(mday = 30) #&gt; [1] &quot;2015-03-02&quot; ymd(&quot;2015-02-01&quot;) %&gt;% update(hour = 400) #&gt; [1] &quot;2015-02-17 16:00:00 UTC&quot; Puedes usar update() para mostrar la distribución de los vuelos a lo largo del día para cada día del año: vuelos_dt %&gt;% mutate(horario_salida = update(horario_salida, yday = 1)) %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 300) Fijar los componentes más grandes de una fecha con una constante es una técnica que te permite explorar patrones en los componentes más pequeños. 4.3.4 Ejercicios ¿Cómo cambia la distribución de las horas de los vuelos dentro de un día, a lo largo del año? Compara horario_salida, salida_programada and atraso_salida. ¿Son consistentes? Explica tus hallazgos. Compara tiempo_vuelo con la duración entre la salida y la llegada. Explica tus hallazgos. (Pista: considera la ubicación del aeropuerto). ¿Cómo cambia la demora promedio durante el curso de un día? ¿Deberías usar horario_salida o salida_programada? ¿Por qué? ¿En qué día de la semana deberías salir, si quieres minimizar las posibilidades de una demora? ¿Qué hace que la distribución de diamantes$carat y vuelos$salida_programada sean similares? Confirma mi hipótesis de que salidas programas en los minutos 20-30 y 50-60 están casuadas por los vuelos programados que salen más temprano. Pista: crea una variable binaria que te diga si un vuelo tuvo o no demora. 4.4 Plazos de tiempo Ahora, aprenderás cómo trabaja la aritmética con fechas, incluyendo la sustracción, adición y división. En el camino, aprenderás sobre tres importantes clases que representan períodos de tiempo: durations (del inglés, duraciones), que representa un número exacto de segundos. periods (del inglés, períodos), que representan unidades humanas como semanas o meses. intervals (del inglés, intervalos), que representan un punto de inicio y uno de finalización. 4.4.1 Duraciones En R, cuando restas dos fechas, obtienes un objeto de diferencia temporal (en inglés, difftimes): # ¿Cuán viejo es Hadley? edad_h &lt;- today() - ymd(19791014) edad_h #&gt; Time difference of 14457 days Un objeto de clase difftime registra un período de tiempo se segundos, minutos, horas, días o semanas. Esta ambiguedad hacia que los difftimes sean un poco complicados de trabjar, por lo que lubridate provee una alternativa que siempre usa segundos: la duration. as.duration(edad_h) #&gt; [1] &quot;1249084800s (~39.58 years)&quot; Las duraciones traen un conveniente grupo de constructores: dseconds(15) #&gt; [1] &quot;15s&quot; dminutes(10) #&gt; [1] &quot;600s (~10 minutes)&quot; dhours(c(12, 24)) #&gt; [1] &quot;43200s (~12 hours)&quot; &quot;86400s (~1 days)&quot; ddays(0:5) #&gt; [1] &quot;0s&quot; &quot;86400s (~1 days)&quot; &quot;172800s (~2 days)&quot; #&gt; [4] &quot;259200s (~3 days)&quot; &quot;345600s (~4 days)&quot; &quot;432000s (~5 days)&quot; dweeks(3) #&gt; [1] &quot;1814400s (~3 weeks)&quot; dyears(1) #&gt; [1] &quot;31536000s (~52.14 weeks)&quot; Las duraciones siempre registran el período de tiempo en segundos. Las unidades más grandes se crean al convertir minutos, horas, días, semanas y años a segundos, mediante una conversión estándar (60 segundos en un minuto, 60 minutos en una hora, 24 horas en un día, 7 días en una semana, 365 días en un año). Puedes agregar y multiplicar duraciones: 2 * dyears(1) #&gt; [1] &quot;63072000s (~2 years)&quot; dyears(1) + dweeks(12) + dhours(15) #&gt; [1] &quot;38847600s (~1.23 years)&quot; Puedes sumar y restar duraciones a días: ayer &lt;- today() + ddays(1) anio_pasado &lt;- today() - dyears(1) Sin embargo, como las duraciones representan un número exacto de segundos, a veces puedes obtener un resultado inesperado: una_pm &lt;- ymd_hms(&quot;2016-03-12 13:00:00&quot;, tz = &quot;America/New_York&quot;) una_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; una_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; ¿¡Por qué un día antes de la 1PM del 12 de Marzo, resulta ser las 2PM del 13 de Marzo!? Si miras con cuidado a la fecha, te darás cuenta que los husos horarios han cambiado. Debido al horario de verano (DST, o EDT para el horario de verano de la costa Este), el 12 de Marzo sólo tiene 23 horas, por lo que si agregamos un día entero de segundos, terminamos con una hora diferente. 4.4.2 Períodos Para resolver este problema, lubridate provee a periods (del inglés, períodos). Estos son plazos de tiempo que no tienen un largo fijo en segundos, sino que funcionan con tiempos “humanos”, como días o meses. Esto les permite trabajar en una forma más intuitiva: una_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; una_pm + days(1) #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; Como las duraciones, los períodos pueden ser creados mediante un número de funciones constructoras amigables. seconds(15) #&gt; [1] &quot;15S&quot; minutes(10) #&gt; [1] &quot;10M 0S&quot; hours(c(12, 24)) #&gt; [1] &quot;12H 0M 0S&quot; &quot;24H 0M 0S&quot; days(7) #&gt; [1] &quot;7d 0H 0M 0S&quot; months(1:6) #&gt; [1] &quot;1m 0d 0H 0M 0S&quot; &quot;2m 0d 0H 0M 0S&quot; &quot;3m 0d 0H 0M 0S&quot; &quot;4m 0d 0H 0M 0S&quot; #&gt; [5] &quot;5m 0d 0H 0M 0S&quot; &quot;6m 0d 0H 0M 0S&quot; weeks(3) #&gt; [1] &quot;21d 0H 0M 0S&quot; years(1) #&gt; [1] &quot;1y 0m 0d 0H 0M 0S&quot; Puedes sumar y multiplicar períodos: 10 * (months(6) + days(1)) #&gt; [1] &quot;60m 10d 0H 0M 0S&quot; days(50) + hours(25) + minutes(2) #&gt; [1] &quot;50d 25H 2M 0S&quot; Y, por supuesto, puedes sumarlos a las fechas. Comparados a las duraciones, los períodos son más propensos a hacer lo que esperas que hagan: # Un año bisiesto ymd(&quot;2016-01-01&quot;) + dyears(1) #&gt; [1] &quot;2016-12-31&quot; ymd(&quot;2016-01-01&quot;) + years(1) #&gt; [1] &quot;2017-01-01&quot; # Horarios de verano una_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; una_pm + days(1) #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; Usemos los períodos para arreglar una rareza relacionada a nuestras fechas de vuelos. Algunos aviones parecen arrivar a su destino antes de salir de la ciudad de Nueva York. vuelos_dt %&gt;% filter(horario_llegada &lt; horario_salida) #&gt; # A tibble: 10,633 x 9 #&gt; origen destino atraso_salida atraso_llegada horario_salida #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 EWR BQN 9 -4 2013-01-01 19:29:00 #&gt; 2 JFK DFW 59 NA 2013-01-01 19:39:00 #&gt; 3 EWR TPA -2 9 2013-01-01 20:58:00 #&gt; 4 EWR SJU -6 -12 2013-01-01 21:02:00 #&gt; 5 EWR SFO 11 -14 2013-01-01 21:08:00 #&gt; 6 LGA FLL -10 -2 2013-01-01 21:20:00 #&gt; # … with 1.063e+04 more rows, and 4 more variables: #&gt; # horario_llegada &lt;dttm&gt;, salida_programada &lt;dttm&gt;, #&gt; # llegada_programada &lt;dttm&gt;, tiempo_vuelo &lt;dbl&gt; Estos son vuelos nocturnos. Usamos la misma información de fecha para los horarios de salida y llegada, pero estos vuelos llegaron en el día siguiente. Podemos arreglarlo al sumar days(1) a la fecha de llegada de cada vuelo nocturno. vuelos_dt &lt;- vuelos_dt %&gt;% mutate( nocturno = horario_llegada &lt; horario_salida, horario_llegada = horario_llegada + days(nocturno * 1), llegada_programada = llegada_programada + days(nocturno * 1) ) Ahora todos los vuelos obedecen a las leyes de la física. vuelos_dt %&gt;% filter(nocturno, horario_llegada &lt; horario_salida) #&gt; # A tibble: 0 x 10 #&gt; # … with 10 variables: origen &lt;chr&gt;, destino &lt;chr&gt;, atraso_salida &lt;dbl&gt;, #&gt; # atraso_llegada &lt;dbl&gt;, horario_salida &lt;dttm&gt;, horario_llegada &lt;dttm&gt;, #&gt; # salida_programada &lt;dttm&gt;, llegada_programada &lt;dttm&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, nocturno &lt;lgl&gt; 4.4.3 Intervalos Resulta obvio lo que dyears(1) / ddays(365) debería retornar: uno, porque las duraciones siempre se representan por un número de segundos, y la duración de un año se define como 365 días convertidos a segundos. ¿Qué debería devolver years(1) / days(1)? Bueno, si el año fuera 2015 debería retornar 365, ¡pero si fuera 2016 debería retornar 366! No hay suficiente información para que lubridate nos de una sóla respuesta sencilla. Entonces, lo que hace es darnos una estimación, con una advertencia: years(1) / days(1) #&gt; estimate only: convert to intervals for accuracy #&gt; [1] 365 Si quieres una medida más precisa, tendrás que usar un interval (del inglés, intervalo). Un intervalo es una duración con un punto de partida: eso lo hace preciso, porque puedes determinar exactamente cuán largo es: siguiente_anio &lt;- today() + years(1) (today() %--% siguiente_anio) / ddays(1) #&gt; [1] 366 Para encontrar cuántos períodos caen dentro de un intervalo, tienes que usar la división entera: (today() %--% siguiente_anio) %/% days(1) #&gt; Note: method with signature &#39;Timespan#Timespan&#39; chosen for function &#39;%/%&#39;, #&gt; target signature &#39;Interval#Period&#39;. #&gt; &quot;Interval#ANY&quot;, &quot;ANY#Period&quot; would also be valid #&gt; [1] 366 4.4.4 Resumen ¿Cómo eliges entre duraciones, períodos e intervalos? Como siempre, selecciona la estructura de datos más sencilla que resuelva tu problema. Si sólo necesitas tiempo físico, usa una duración; si necesitas agregar tiempos humanos, usa un período; si tienes que deducir cuán largo es un plazo en unidades humanas, usa un intervalo. La figura 4.1 resume las operaciones artiméticas permitidas entre los tipos de datos. Figure 4.1: Las operaciones artiméticas permitidas entre pares de clases fecha/hora. 4.4.5 Ejercicios ¿Por qué hay months() pero no dmonths() (del inglés, días del mes)? Explica days(nocturno * 1) a alguien que apenas comienza a aprender R. ¿Cómo funciona? Crea un vector de fechas dando el primer día de cada mes en 2015. Crea un vector de fechas dando el primer día de cada mes del año actual. Crea una función en la cual, dado tu cumpleaños (como una fecha), retorne cuán viejo eres en años. ¿Por qué no puedes hacer funcionar a (today() %--% (today() + years(1)) / months(1) ? 4.5 Husos horarios Los husos horarios son un tema enormemente complicado, debido a su interacción con entidades geopolíticas. Afortunadamente, no necesitamos excarbar en todos los detalles, ya que no son tan necesarios para el análisis de datos, pero hay algunos desafíos sobre los que tenemos que trabajar. El primer desafío es que los nombres comunes de los husos horarios tienden a ser ambiguos. Por ejemplo, si eres Americano probablemente estés familiarizado con EST, (siglas en inglés de Tiempo Este Estándar). Sin embargo, ¡Canadá y Australia también tienen EST! Para evitar la confusión, R usa el estándar internacional IANA para husos horarios. Estos tienen un esquema de nombres que sigue el formato “/”, típicamente escrito como “&lt;continente&gt;/&lt;ciudad&gt;”, en idioma inglés (hay algunas pocas excepciones porque no todos los países están ubicados en un continente). Ejemplos incluyen: “America/New_York”, “Europe/Paris”, and “Pacific/Auckland”. Puedes preguntarte por qué un huso horario usa una ciudad, cuando típicamente piensas en ellos como asociados a un país, o a una región dentro de un país. Esto es porque la base de datos de IANA tiene que registrar décadas de reglamentos sobre husos horarios. En el curso de las décadas, los países cambian nombres (o desaparecen) de forma bastante frecuente, pero los nombres de las ciudades tienden a mantenerse igual. Otro problema es que los nombres tienen que reflejar no sólo el comportamiento actual, sino también la historia completa. Por ejemplo, hay husos horarios tanto para “America/New_York” como para “America/Detroit”. Actualmente, ambas ciudades usan el EST, pero entre 1969 y 1972, Michigan (el estado en el cual Detroit está ubicado), no empleaba el horario de verano, así que necesita un nombre diferente. ¡Vale mucho leer la base de datos cruda (disponible en http://www.iana.org/time-zones) sólo para enterarse de algunas de estas historias! Puedes encontrar cuál es tu huso horario actual para R, usando Sys.timezone() (del inglés, sistema, huso horario): Sys.timezone() #&gt; [1] &quot;UTC&quot; (Si R no lo sabe, obtendrás un NA.) Y puedes ver la lista completa de todos los husos horarios con OlsonNames(): length(OlsonNames()) #&gt; [1] 606 head(OlsonNames()) #&gt; [1] &quot;Africa/Abidjan&quot; &quot;Africa/Accra&quot; &quot;Africa/Addis_Ababa&quot; #&gt; [4] &quot;Africa/Algiers&quot; &quot;Africa/Asmara&quot; &quot;Africa/Asmera&quot; En R, el huso horario es un atributo de la fecha-hora (date-time) que sólo controla la impresión. Por ejemplo, estos tres objetos representan el mismo instante en el tiempo: (x1 &lt;- ymd_hms(&quot;2015-06-01 12:00:00&quot;, tz = &quot;America/New_York&quot;)) #&gt; [1] &quot;2015-06-01 12:00:00 EDT&quot; (x2 &lt;- ymd_hms(&quot;2015-06-01 18:00:00&quot;, tz = &quot;Europe/Copenhagen&quot;)) #&gt; [1] &quot;2015-06-01 18:00:00 CEST&quot; (x3 &lt;- ymd_hms(&quot;2015-06-02 04:00:00&quot;, tz = &quot;Pacific/Auckland&quot;)) #&gt; [1] &quot;2015-06-02 04:00:00 NZST&quot; Puedes verificar que son lo mismo al usar una resta: x1 - x2 #&gt; Time difference of 0 secs x1 - x3 #&gt; Time difference of 0 secs Excepto que sea especificado claramente, lubridate siempre usa UTC. UTC significa Tiempo Universal Coordinado (por las siglas en inglés), y es el huso horario estándar empleado por la comunidad científica, y es aproximadamente equivalente a su predecesor GMT (Tiempo del Meridiano de Greenwich, por las siglas en inglés). No tiene horario de verano, por lo que resulta una representación conveniente para la computación. Las operaciones que combinan fechas y horas, como c(), a menudo descartan el huso horario. En ese caso, las fechas y horas se muestran en tu huso local: x4 &lt;- c(x1, x2, x3) x4 #&gt; [1] &quot;2015-06-01 12:00:00 EDT&quot; &quot;2015-06-01 12:00:00 EDT&quot; #&gt; [3] &quot;2015-06-01 12:00:00 EDT&quot; Puedes cambiar el huso horario de dos formas: Mantener el instante en el tiempo igual, y cambiar sólo cómo se representa. Usa esto cuando el instante es correcto, pero quieres una visualización más natural. x4a &lt;- with_tz(x4, tzone = &quot;Australia/Lord_Howe&quot;) x4a #&gt; [1] &quot;2015-06-02 02:30:00 +1030&quot; &quot;2015-06-02 02:30:00 +1030&quot; #&gt; [3] &quot;2015-06-02 02:30:00 +1030&quot; x4a - x4 #&gt; Time differences in secs #&gt; [1] 0 0 0 (Esto también ilustra otro desafío de los husos horarios: ¡no todos los desfasajes son horas como números enteros!) Cambia el instante en el tiempo. Usa esto cuando tienes un instante que ha sido etiquetado con un huso horario incorrecto, y necesitas arreglarlo. x4b &lt;- force_tz(x4, tzone = &quot;Australia/Lord_Howe&quot;) x4b #&gt; [1] &quot;2015-06-01 12:00:00 +1030&quot; &quot;2015-06-01 12:00:00 +1030&quot; #&gt; [3] &quot;2015-06-01 12:00:00 +1030&quot; x4b - x4 #&gt; Time differences in hours #&gt; [1] -14.5 -14.5 -14.5 N. del T.: El ultimo select de esta parte difiere del original o de lo contrario obtenemos un dataset con menos columnas que en el original haciendo que fallen los ejemplos posteriores. Esto se hizo ya que el castellano algunas palabras se invierten de orden.↩ "],
["exploratory-data-analysis.html", "5 Exploratory Data Analysis 5.1 Introduction 5.2 Questions 5.3 Variation 5.4 Missing values 5.5 Covariation 5.6 Patterns and models 5.7 ggplot2 calls 5.8 Learning more", " 5 Exploratory Data Analysis 5.1 Introduction This chapter will show you how to use visualisation and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. Use what you learn to refine your questions and/or generate new questions. EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others. EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling. 5.1.1 Prerequisites In this chapter we’ll combine what you’ve learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions. library(tidyverse) 5.2 Questions “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make. EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find. There is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as: What type of variation occurs within my variables? What type of covariation occurs between my variables? The rest of this chapter will look at these two questions. I’ll explain what variation and covariation are, and I’ll show you several ways to answer each question. To make the discussion easier, let’s define some terms: A variable is a quantity, quality, or property that you can measure. A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement. An observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. I’ll sometimes refer to an observation as a data point. Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row. So far, all of the data that you’ve seen has been tidy. In real-life, most data isn’t tidy, so we’ll come back to these ideas again in [tidy data]. 5.3 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. This is true even if you measure quantities that are constant, like the speed of light. Each of your measurements will include a small amount of error that varies from measurement to measurement. Categorical variables can also vary if you measure across different subjects (e.g. the eye colors of different people), or different times (e.g. the energy levels of an electron at different moments). Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable’s values. 5.3.1 Visualising distributions How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In R, categorical variables are usually saved as factors or character vectors. To examine the distribution of a categorical variable, use a bar chart: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) The height of the bars displays how many observations occurred with each x value. You can compute these values manually with dplyr::count(): diamonds %&gt;% count(cut) #&gt; # A tibble: 5 x 2 #&gt; cut n #&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 Fair 1610 #&gt; 2 Good 4906 #&gt; 3 Very Good 12082 #&gt; 4 Premium 13791 #&gt; 5 Ideal 21551 A variable is continuous if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram: ggplot(data = diamonds) + geom_histogram(mapping = aes(x = carat), binwidth = 0.5) You can compute this by hand by combining dplyr::count() and ggplot2::cut_width(): diamonds %&gt;% count(cut_width(carat, 0.5)) #&gt; # A tibble: 11 x 2 #&gt; `cut_width(carat, 0.5)` n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 [-0.25,0.25] 785 #&gt; 2 (0.25,0.75] 29498 #&gt; 3 (0.75,1.25] 15977 #&gt; 4 (1.25,1.75] 5313 #&gt; 5 (1.75,2.25] 2002 #&gt; 6 (2.25,2.75] 322 #&gt; # … with 5 more rows A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that almost 30,000 observations have a carat value between 0.25 and 0.75, which are the left and right edges of the bar. You can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth. smaller &lt;- diamonds %&gt;% filter(carat &lt; 3) ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.1) If you wish to overlay multiple histograms in the same plot, I recommend using geom_freqpoly() instead of geom_histogram(). geom_freqpoly() performs the same calculation as geom_histogram(), but instead of displaying the counts with bars, uses lines instead. It’s much easier to understand overlapping lines than bars. ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) + geom_freqpoly(binwidth = 0.1) There are a few challenges with this type of plot, which we will come back to in visualising a categorical and a continuous variable. Now that you can visualise variation, what should you look for in your plots? And what type of follow-up questions should you ask? I’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?). 5.3.2 Typical values In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. To turn this information into useful questions, look for anything unexpected: Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? As an example, the histogram below suggests several interesting questions: Why are there more diamonds at whole carats and common fractions of carats? Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak? Why are there no diamonds bigger than 3 carats? ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.01) Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask: How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between. ggplot(data = faithful, mapping = aes(x = eruptions)) + geom_histogram(binwidth = 0.25) Many of the questions above will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable. We’ll get to that shortly. 5.3.3 Unusual values Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis. ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) There are so many observations in the common bins that the rare bins are so short that you can’t see them (although maybe if you stare intently at 0 you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian(): ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) + coord_cartesian(ylim = c(0, 50)) (coord_cartesian() also has an xlim() argument for when you need to zoom into the x-axis. ggplot2 also has xlim() and ylim() functions that work slightly differently: they throw away the data outside the limits.) This allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr: unusual &lt;- diamonds %&gt;% filter(y &lt; 3 | y &gt; 20) %&gt;% select(price, x, y, z) %&gt;% arrange(y) unusual #&gt; # A tibble: 9 x 4 #&gt; price x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 5139 0 0 0 #&gt; 2 6381 0 0 0 #&gt; 3 12800 0 0 0 #&gt; 4 15686 0 0 0 #&gt; 5 18034 0 0 0 #&gt; 6 2130 0 0 0 #&gt; 7 2130 0 0 0 #&gt; 8 2075 5.15 31.8 5.12 #&gt; 9 12210 8.09 58.9 8.06 The y variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars! It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up. 5.3.4 Exercises Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? 5.4 Missing values If you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options. Drop the entire row with the strange values: diamonds2 &lt;- diamonds %&gt;% filter(between(y, 3, 20)) I don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left! Instead, I recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA: diamonds2 &lt;- diamonds %&gt;% mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y)) ifelse() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is false. Alternatively to ifelse, use dplyr::case_when(). case_when() is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed: ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + geom_point() #&gt; Warning: Removed 9 rows containing missing values (geom_point). To suppress that warning, set na.rm = TRUE: ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + geom_point(na.rm = TRUE) Other times you want to understand what makes observations with missing values different to observations with recorded values. For example, in nycflights13::flights, missing values in the dep_time variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable with is.na(). nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(sched_dep_time)) + geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1 / 4) However this plot isn’t great because there are many more non-cancelled flights than cancelled flights. In the next section we’ll explore some techniques for improving this comparison. 5.4.1 Exercises What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? What does na.rm = TRUE do in mean() and sum()? 5.5 Covariation If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should again depend on the type of variables involved. 5.5.1 A categorical and continuous variable It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. For example, let’s explore how the price of a diamond varies with its quality: ggplot(data = diamonds, mapping = aes(x = price)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) It’s hard to see the difference in distribution because the overall counts differ so much: ggplot(diamonds) + geom_bar(mapping = aes(x = cut)) To make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one. ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot. Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of: A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side. Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually. A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution. Let’s take a look at the distribution of price by cut using geom_boxplot(): ggplot(data = diamonds, mapping = aes(x = cut, y = price)) + geom_boxplot() We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counterintuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why. cut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is with the reorder() function. For example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes: ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() To make the trend easier to see, we can reorder class based on the median value of hwy: ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) If you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that with coord_flip(). ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip() 5.5.1.1 Exercises Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()? One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots? Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method? If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does. 5.5.2 Two categorical variables To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to rely on the built-in geom_count(): ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color)) The size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values. Another approach is to compute the count with dplyr: diamonds %&gt;% count(color, cut) #&gt; # A tibble: 35 x 3 #&gt; color cut n #&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 D Fair 163 #&gt; 2 D Good 662 #&gt; 3 D Very Good 1513 #&gt; 4 D Premium 1603 #&gt; 5 D Ideal 2834 #&gt; 6 E Fair 224 #&gt; # … with 29 more rows Then visualise with geom_tile() and the fill aesthetic: diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n)) If the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, you might want to try the d3heatmap or heatmaply packages, which create interactive plots. 5.5.2.1 Exercises How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above? 5.5.3 Two continuous variables You’ve already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with geom_point(). You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the carat size and price of a diamond. ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above). You’ve already seen one way to fix the problem: using the alpha aesthetic to add transparency. ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100) But using transparency can be challenging for very large datasets. Another solution is to use bin. Previously you used geom_histogram() and geom_freqpoly() to bin in one dimension. Now you’ll learn how to use geom_bin2d() and geom_hex() to bin in two dimensions. geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex(). ggplot(data = smaller) + geom_bin2d(mapping = aes(x = carat, y = price)) # install.packages(&quot;hexbin&quot;) ggplot(data = smaller) + geom_hex(mapping = aes(x = carat, y = price)) Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot: ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) cut_width(x, width), as used above, divides x into bins of width width. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarises a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with varwidth = TRUE. Another approach is to display approximately the same number of points in each bin. That’s the job of cut_number(): ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_number(carat, 20))) 5.5.3.1 Exercises Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price? Visualise the distribution of carat, partitioned by price. How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you? Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. ggplot(data = diamonds) + geom_point(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) Why is a scatterplot a better display than a binned plot for this case? 5.6 Patterns and models Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself: Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above. ggplot(data = faithful) + geom_point(mapping = aes(x = eruptions, y = waiting)) Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second. Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. library(modelr) mod &lt;- lm(log(price) ~ log(carat), data = diamonds) diamonds2 &lt;- diamonds %&gt;% add_residuals(mod) %&gt;% mutate(resid = exp(resid)) ggplot(data = diamonds2) + geom_point(mapping = aes(x = carat, y = resid)) Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive. ggplot(data = diamonds2) + geom_boxplot(mapping = aes(x = cut, y = resid)) You’ll learn how models, and the modelr package, work in the final part of the book, model. We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand. 5.7 ggplot2 calls As we move on from these introductory chapters, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning: ggplot(data = faithful, mapping = aes(x = eruptions)) + geom_freqpoly(binwidth = 0.25) Typically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, and the first two arguments to aes() are x and y. In the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back in [functions]. Rewriting the previous plot more concisely yields: ggplot(faithful, aes(eruptions)) + geom_freqpoly(binwidth = 0.25) Sometimes we’ll turn the end of a pipeline of data transformation into a plot. Watch for the transition from %&gt;% to +. I wish this transition wasn’t necessary but unfortunately ggplot2 was created before the pipe was discovered. diamonds %&gt;% count(cut, clarity) %&gt;% ggplot(aes(clarity, cut, fill = n)) + geom_tile() 5.8 Learning more If you want to learn more about the mechanics of ggplot2, I’d highly recommend grabbing a copy of the ggplot2 book: https://amzn.com/331924275X. It’s been recently updated, so it includes dplyr and tidyr code, and has much more space to explore all the facets of visualisation. Unfortunately the book isn’t generally available for free, but if you have a connection to a university you can probably get an electronic version for free through SpringerLink. Another useful resource is the R Graphics Cookbook by Winston Chang. Much of the contents are available online at http://www.cookbook-r.com/Graphs/. I also recommend Graphical Data Analysis with R, by Antony Unwin. This is a book-length treatment similar to the material covered in this chapter, but has the space to go into much greater depth. "],
["explora-introduccion.html", "6 Introducción", " 6 Introducción El objetivo de la primera parte del libro es conducirte a las herramientas básicas de exploración de datos de la manera más veloz posible. La exploración de datos es el arte de mirar tus datos, generar hipótesis rápidamente, testearlas con celeridad y luego repetir el proceso iterativamente. El objetivo de la exploración de datos es generar muchos hallazgos prometedores que luego puedes retomar para explorarlos en mayor profundidad. En esta parte del libro aprenderás algunas herramientas que traen un beneficio inmediato: * Visualización es una buena arista para comenzar a programar en R, ya que el retorno es claro: puedes crear gráficos elegantes e informativos que te ayudan a entender los datos. En visualización de datos vas a profundizar en visualización y aprenderás la estructura básica de gráficos en ggplot2 junto con transformar datos en gráficos. Visualización por si sola a menudo no es suficiente, por lo que en transformación de datos aprenderás los verbos clave que te permitirán seleccionar variables importantes, filtrar observaciones, crear nuevas variables y sintetizar la información. Finalmente, en [análisis exploratorio de datos], vas a combinar visualización y transformación con tu curiosidad y escepticismo para formular y responder preguntas en torno a los datos. Modelar es un aspecto importante del proceso exploratorio, pero no tienes las habilidades para aprenderlo con efectividad o aplicarlo de momento. Volveremos a dicho tópico en [modelamiento], una vez que ya tengas las herramientas de manipulación de datos y programación. Entre estos tres capítulos que enseñan las herramientas de exploración de datos hay otros tres capítulos que se enfocan en el flujo de trabajo en R. En [flujo de trabajo: básico], [flujo de trabajo: scripts] y [flujo de trabajo: proyectos] aprenderás buenas prácticas para escribir y organizar código R. Tales prácticas te preparan para el éxito en el largo plazo, en cuanto te entregan las herramientas para organizarse y abordar proyectos reales. "],
["factores.html", "7 Factores 7.1 Introducción 7.2 Creando factores 7.3 Encuesta social general 7.4 Modificar el orden de los factores 7.5 Modificar los niveles de los factores", " 7 Factores 7.1 Introducción En R, los factores se usan para trabajar con variables categóricas, es decir, variables que tienen un conjunto fijo y conocido de valores posibles. Además, son útiles cuando se quiere mostrar vectores de caracteres en un orden no alfabético. Históricamente, los factores eran más sencillos de trabajar que los caracteres. Como resultado, muchas de las funciones de R base automáticamente convierten los caracteres a factores. Esto significa que, a menudo, los factores aparecen en lugares donde no son realmente útiles. Afortunadamente, no tienes que preocuparte de eso en el tidyverse, y puedes concentrarte en situaciones donde los factores son genuinamente útiles. 7.1.1 Requisitos previos Para trabajar con factores, vamos a usar el paquete forcats (del inglés para cadenas), que provee herramientas para lidear con variables categóricas (¡y es un anagrama de factores!). Este paquete provee un amplio rango de ayudantes para trabajar con factores. forcats no es parte de los paquetes centrales de tidyverse, así que necesitamos cargarlo de forma explícita. library(tidyverse) library(forcats) library(datos) 7.1.2 Aprendiendo más Si quieres aprender más sobre los factores, recomiendo leer el artículo de Amelia McNamara y Nicholas Horton, Wrangling categorical data in R (el nombre significa Luchando con Datos Categóricos en R). Este artículo cuenta parte de la historia discutida en stringsAsFactors: An unauthorized biography (del inglés cadenasComoFactores: Una Biografía No Autorizada) y stringsAsFactors = &lt;sigh&gt; (del inglés cadenasComoFactores = &lt;suspiro&gt;), y compara las propuestas tidy para los datos categóricos demostrados en este libro, contra los métodos base de R. Una versión temprana de este artículo ayudó a motivar y limitar el paquete forcats; ¡Gracias Amelia y Nick! 7.2 Creando factores Imagina que tienes una variable que guarda los meses: x1 &lt;- c(&quot;Dic&quot;, &quot;Abr&quot;, &quot;Ene&quot;, &quot;Mar&quot;) Usar una cadena de caracteres (o string, en inglés) para guardar esta variable tiene dos problemas: Sólo hay doce meses posibles, y no hay nada que te resguarde de errores de tipeo: x2 &lt;- c(&quot;Dic&quot;, &quot;Abr&quot;, &quot;Eme&quot;, &quot;Mar&quot;) No ordena de una forma útil: sort(x1) #&gt; [1] &quot;Abr&quot; &quot;Dic&quot; &quot;Ene&quot; &quot;Mar&quot; Puedes solucionar ambos problemas con un factor. Para crearlo debes empezar armando una lista de los niveles válidos: niveles_meses &lt;- c( &quot;Ene&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Abr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Ago&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dic&quot; ) Ahora puedes crear un factor: y1 &lt;- factor(x1, levels = niveles_meses) y1 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic sort(y1) #&gt; [1] Ene Mar Abr Dic #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic Y cualquier valor no fijado en el conjunto, será convertido a NA de forma silenciosa: y2 &lt;- factor(x2, levels = niveles_meses) y2 #&gt; [1] Dic Abr &lt;NA&gt; Mar #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic Si quieres una advertencia, puedes usar readr::parse_factor() (del inglés analizar gramaticalmente un factor): y2 &lt;- parse_factor(x2, levels = niveles_meses) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 3 -- value in level set Eme Si omites los niveles, se van a tomar desde los datos en orden alfabético: factor(x1) #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Abr Dic Ene Mar En algunos momentos, es preferible que el orden de los niveles se corresponda con su primera aparición en los datos. Puedes hacer esto cuando creas el factor, al marcar los niveles con unique(x) (del inglés único), o después del hecho, con fct_inorder() (del inglés factores en orden): f1 &lt;- factor(x1, levels = unique(x1)) f1 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Dic Abr Ene Mar f2 &lt;- x1 %&gt;% factor() %&gt;% fct_inorder() f2 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Dic Abr Ene Mar Si alguna vez necesitas acceso directo al conjunto de niveles válidos, puedes hacerlo con levels() (del inglés niveles): levels(f2) #&gt; [1] &quot;Dic&quot; &quot;Abr&quot; &quot;Ene&quot; &quot;Mar&quot; 7.3 Encuesta social general Por el resto del capítulo, nos vamos a concentrar en encuesta. Ésta es la versión traducida al español de un conjunto de datos de ejemplo de General Social Survey; ésta es una encuesta realizada en Estados Unidos desde hace mucho tiempo, conducida por la organización de investigación independiente llamada NORC, en la Universidad de Chicago. La encuesta tiene miles de preguntas, así que en el conjunto de datos he seleccionado aquellas que ilustran algunos de los desafíos comunes que encontrarás al trabajar con factores. library(datos) encuesta #&gt; # A tibble: 21,483 x 9 #&gt; anio estado_civil edad raza ingreso partido religion denominacion #&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 2000 Nunca se ha… 26 Blan… $8000 … Indepe… Protest… Bautistas d… #&gt; 2 2000 Divorciado 48 Blan… $8000 … Republ… Protest… Bautista, n… #&gt; 3 2000 Viudo 67 Blan… Not ap… Indepe… Protest… No denomina… #&gt; 4 2000 Nunca se ha… 39 Blan… Not ap… Indepe… Cristia… No aplica #&gt; 5 2000 Divorciado 25 Blan… Not ap… Demócr… Ninguna No aplica #&gt; 6 2000 Casado 25 Blan… $20000… Demócr… Protest… Bautistas d… #&gt; # … with 2.148e+04 more rows, and 1 more variable: horas_tv &lt;int&gt; (Recuerda, dado que este conjunto de datos está provisto por un paquete, puedes obtener más información de las variables con ?encuesta.) Cuando los factores están almacenados en un tibble (se pronuncia /tibl/), no puedes ver sus niveles tán fácilmente. Una forma de verlos es con count() (del inglés contar): encuesta %&gt;% count(raza) #&gt; # A tibble: 3 x 2 #&gt; raza n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Otra 1959 #&gt; 2 Negra 3129 #&gt; 3 Blanca 16395 O con un gráfico de barras: ggplot(encuesta, aes(raza)) + geom_bar() Por defecto, ggplot2 retira los niveles que no tienen valores. Puedes forzarlos para que se visualicen con: ggplot(encuesta, aes(raza)) + geom_bar() + scale_x_discrete(drop = FALSE) Estos niveles representan valores válidos que simplemente no ocurren en este conjunto de datos. Desafortunadamente, dplyr no tiene una opción de drop (del inglés descartar, eliminar), pero lo hará en el futuro. Cuando se trabaja con factores, las dos operaciones más comunes son cambiar el orden de los niveles, y cambiar los valores de los niveles. Estas operaciones se describen en las siguientes secciones. 7.3.1 Ejercicios Explora la distribución de ingreso. ¿Qué hace que el gráfico de barras por defecto sea tan difícil de comprender? ¿Cómo podrías mejorarlo? ¿Cuál es la religion más común de la encuesta? ¿Cuál es el partido más común? ¿A qué religion se aplica cada denominacion? ¿Cómo puedes encontrarlo con una tabla? ¿Cómo lo puedes descubrir con una visualización? 7.4 Modificar el orden de los factores A menudo resulta útil cambiar el orden de los niveles de factores en una visualización. Por ejemplo, imagina que quieres explorar el número promedio de horas consumidas mirando televisión por día, para cada religión: resumen_religion &lt;- encuesta %&gt;% group_by(religion) %&gt;% summarise( edad = mean(edad, na.rm = TRUE), horas_tv = mean(horas_tv, na.rm = TRUE), n = n() ) ggplot(resumen_religion, aes(horas_tv, religion)) + geom_point() Este gráfico resulta dificil de interpretar porque no hay un patrón general. Podemos mejorarlo al ordenar los niveles de religion usando fct_reorder() (del inglés, reordenar factores). fct_reorder() requiere tres argumentos: f, el factor cuyos niveles quieres modificar. x, un vector numérico que quieres usar para reordenar los niveles. Opcionalmente, fun, una función que se usa si hay múltiples valores de x para cada valor de f. El valor por defecto es un median (del inglés, mediana). ggplot(resumen_religion, aes(horas_tv, fct_reorder(religion, horas_tv))) + geom_point() Reordenar la columna religión (religion) hace que sea más sencillo ver que las personas en la categoría “No same” ven más televisión, mientras que “Hinduismo” ven mucho menos. Cuando haces transformaciones más complicadas, yo recomiendo que las remuevas de aes() (del inglés, abreviatura de estética) hacia un paso de mutación separado, con mutate() (del inglés, mutar). Por ejemplo, puedes reescribir ese gráfico de la siguiente forma: resumen_religion %&gt;% mutate(religion = fct_reorder(religion, horas_tv)) %&gt;% ggplot(aes(horas_tv, religion)) + geom_point() ¿Qué sucede si creamos un gráfico para observar cómo varía la edad promedio para cada ingreso reportado? resumen_ingreso &lt;- encuesta %&gt;% group_by(ingreso) %&gt;% summarise( edad = mean(edad, na.rm = TRUE), horas_tv = mean(horas_tv, na.rm = TRUE), n = n() ) ggplot(resumen_ingreso, aes(edad, fct_reorder(ingreso, edad))) + geom_point() ¡Aquí, reordenar los niveles arbitrariamente no es una buena idea! Eso es porque ingreso ya tiene un orden basado en un principio determinado, con el cual no deberíamos meternos. Reserva fct_reorder() para factores cuyos niveles están ordenados arbitrariamente. Sin embargo, sí tiene sentido mover “No Aplica” al frente, junto a los otros niveles especiales. Puedes usar fct_relevel() (del inglés cambiar niveles). Ésta función recibe como argumento un factor, f, y luego cualquier número de niveles que quieres mover al principio de la línea. ggplot(resumen_ingreso, aes(edad, fct_relevel(ingreso, &quot;No aplica&quot;))) + geom_point() #&gt; Warning: Unknown levels in `f`: No aplica #&gt; Warning: Unknown levels in `f`: No aplica ¿Por qué crees que la edad promedio para “No aplica” es tan alta? Otro tipo de reordenamiento es útil cuando estás coloreando las líneas de un gráfico. fct_reorder2() (del inglés, reorganizar niveles número dos) reordena el factor mediante los valores y, asociados con los valores x más grandes. Esto hace que el gráfico sea más sencillo de leer, porque los colores de líneas se ajustan con la leyenda. por_edad &lt;- encuesta %&gt;% filter(!is.na(edad)) %&gt;% count(edad, estado_civil) %&gt;% group_by(edad) %&gt;% mutate(prop = n / sum(n)) ggplot(por_edad, aes(edad, prop, colour = estado_civil)) + geom_line(na.rm = TRUE) ggplot(por_edad, aes(edad, prop, colour = fct_reorder2(estado_civil, edad, prop))) + geom_line() + labs(colour = &quot;estado_civil&quot;) Finalmente, para los gráficos de barra, puedes usar fct_infreq() (del inglés, frecuencia incremental de factores) para ordenar los niveles incrementalmente según su frecuencia: este es el ordenamiento más sencillo porque no requiere de variables adicionales. Puedes querer combinarlo con fct_rev() (del inglés, revisar factores). encuesta %&gt;% mutate(estado_civil = estado_civil %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;% ggplot(aes(estado_civil)) + geom_bar() 7.4.1 Ejercicios Hay algunos números sospechosamente grandes en horas_tv. ¿Es la media un buen resumen? Identifica, para cada factor en encuesta, si el orden de los niveles es arbitrario o responde a algún principio. ¿Por qué mover “No aplica” al inicio de los niveles lo llevó al final del gráfico? 7.5 Modificar los niveles de los factores Más poderoso que cambiar el orden de los niveles es cambiar sus valores. Esto te permite clarificar etiquetas para publicación, y colapsar niveles para visualizaciones de alto nivel. La herramienta más general y más poderosa es fct_recode() (del inglés, recodificar factores). Ésta función te permite recodificar, o cambiar, el valor de cada nivel. Por ejemplo, toma la columna encuesta$partido: encuesta %&gt;% count(partido) #&gt; # A tibble: 10 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Sin respuesta 154 #&gt; 2 No sabe 1 #&gt; 3 Otro partido 393 #&gt; 4 Republicano duro 2314 #&gt; 5 Republicano moderano 3032 #&gt; 6 Independiente pro republicano 1791 #&gt; # … with 4 more rows Los niveles son tersos e inconsistentes. Hay que correjirlos un poco para que sean más largos, y poder usar una construcción paralela. encuesta %&gt;% mutate(partido = fct_recode(partido, &quot;Republicano duro&quot; = &quot;Republicano Acérrimo&quot;, &quot;Republicano moderado&quot; = &quot;Republicado No Acérrimo&quot;, &quot;Independiente pro republicano&quot; = &quot;Independiente, pro-Rep&quot;, &quot;Independiente pro demócrata&quot; = &quot;Independiente, pro-Dem&quot;, &quot;Demócrata moderado&quot; = &quot;Demócrata No Acérrimo&quot;, &quot;Demócrata duro&quot; = &quot;Demócrata Acérrimo&quot; )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 10 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Sin respuesta 154 #&gt; 2 No sabe 1 #&gt; 3 Otro partido 393 #&gt; 4 Republicano duro 2314 #&gt; 5 Republicano moderano 3032 #&gt; 6 Independiente pro republicano 1791 #&gt; # … with 4 more rows fct_recode() no modificará los niveles que no han sido mencionados explícitamente, y te advertirá si accidentalmente te refieres a un nivel que no existe. Para combinar grupos, puedes asignar múltiples niveles viejos, al mismo nivel nuevo: encuesta %&gt;% mutate(partido = fct_recode(partido, &quot;Republicano duro&quot; = &quot;Republicano Acérrimo&quot;, &quot;Republicano moderado&quot; = &quot;Republicado No Acérrimo&quot;, &quot;Independiente pro republicano&quot; = &quot;Independiente, pro-Rep&quot;, &quot;Independiente pro demócrata&quot; = &quot;Independiente, pro-Dem&quot;, &quot;Demócrata moderado&quot; = &quot;Demócrata No Acérrimo&quot;, &quot;Demócrata duro&quot; = &quot;Demócrata Acérrimo&quot;, &quot;Otro&quot; = &quot;Sin respuesta&quot;, &quot;Otro&quot; = &quot;No sabe&quot;, &quot;Otro&quot; = &quot;Otro partido&quot; )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 8 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Otro 548 #&gt; 2 Republicano duro 2314 #&gt; 3 Republicano moderano 3032 #&gt; 4 Independiente pro republicano 1791 #&gt; 5 Independiente 4119 #&gt; 6 Independiente pro democrata 2499 #&gt; # … with 2 more rows Debes usar esta técnica con cuidado: si agrupas categorías que son realmente diferentes, obtendrás resultados confusos y/o engañosos. Si quieres colapsar muchos niveles, fct_collapse() (del inglés, colapsar factores) es una variante muy útil de fct_recode(). Para cada nueva variable, puedes proveer un vector de niveles viejos: encuesta %&gt;% mutate(partido = fct_collapse(partido, otro = c(&quot;Sin respuesta&quot;, &quot;No sabe&quot;, &quot;Otro partido&quot;), republicano = c(&quot;Republicano Acérrimo&quot;, &quot;Republicado No Acérrimo&quot;), independiente = c(&quot;Independiente, pro-Rep&quot;, &quot;Independiente&quot;, &quot;Independiente, pro-Dem&quot;), democrata = c(&quot;Demócrata No Acérrimo&quot;, &quot;Demócrata Acérrimo&quot;) )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 8 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 otro 548 #&gt; 2 Republicano duro 2314 #&gt; 3 Republicano moderano 3032 #&gt; 4 Independiente pro republicano 1791 #&gt; 5 independiente 4119 #&gt; 6 Independiente pro democrata 2499 #&gt; # … with 2 more rows A veces, simplemente quieres agrupar todos los grupos pequeños para simplificar un gráfico o tabla. Ese es un trabajo para fct_lump() (del inglés, agrupar factores): encuesta %&gt;% mutate(religion = fct_lump(religion)) %&gt;% count(religion) #&gt; # A tibble: 2 x 2 #&gt; religion n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestante 10846 #&gt; 2 Other 10637 El comportamiento por defecto es agrupar los grupos pequeños de forma progresiva, asegurando que la agregación continúa siendo el grupo más pequeño. En este caso, esto no resulta demasiado útil: es cierto que la mayoría de Americanos en esta encuesta son Protestantes, pero probablemente hemos colapsado en exceso. En cambio, podemos usar el parámetro n para especificar cuántos grupos (excluyendo otros) queremos colapsar: encuesta %&gt;% mutate(religion = fct_lump(religion, n = 10)) %&gt;% count(religion, sort = TRUE) %&gt;% print(n = Inf) #&gt; # A tibble: 11 x 2 #&gt; religion n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestante 10846 #&gt; 2 Católica 5124 #&gt; 3 Ninguna 3523 #&gt; 4 Cristiana 689 #&gt; 5 Judía 388 #&gt; 6 Other 234 #&gt; 7 Otra 224 #&gt; 8 Budismo 147 #&gt; 9 Inter o no confensional 109 #&gt; 10 Musulmana/Islam 104 #&gt; 11 Cristiana ortodoxa 95 7.5.1 Ejercicios ¿Cómo han cambiado en el tiempo las proporciones de personas que se identifican como Demócratas, Republicanas e Independientes? ¿Cómo podrías colapsar ingreso en un grupo más pequeño de categorías? "],
["funciones.html", "8 Funciones 8.1 Introducción 8.2 ¿Cuándo deberías escribir una función? 8.3 Las funciones son para los humanos y las computadoras 8.4 Ejecución condicional 8.5 Function arguments 8.6 Valores de salida 8.7 Entorno", " 8 Funciones 8.1 Introducción Una de las mejores maneras de lograr tener mayor alcance como científica de datos es escribir funciones. Las funciones te permitirán automatizar algunas tareas comunes en una manera poderosa y general que copiar-y-pegar. Escribir funciones tiene 3 grandes ventajas sobre copiar-y-pegar: Puedes dar a la función un nombre evocativo que hace to código más fácil de entender. Como cambien los requerimientos, tu solo necesitas cambiar tu código en un solo lugar, en vez de varios lugares. Tu eliminas las chances de hacer errores accidentales cuando copias y pegas (ej. actualizar el nombre de una variable en un lugar, pero no en otro). Escribir funciones en un viaje de toda la vida. Incluso después de usar R por varios años yo aún aprendo nuevas técnicas y mejores formas de abordar viejos problemas. El objetivo de este capítulo no es enseñarte cada detalle esotérico de funciones pero sí introducirte con consejos pragmáticos que puedes aplicar inmediatamente. Como así también en los avisos práctivos para escribir funciones, en este capítulo también te da consejos para cómo diseñar tu código. El buen diseño del código es la correcta puntuación. Tu puedes hacerlo sin eso, pero te hará las cosas más fácil de leer! Al igual que los estilos de puntuación, hay muchas posibles variaciones. Aquí presentamos el diseño que usamos en nuestro código, pero lo más importante es ser consistente. 8.1.1 Prerequisitos El objetivo de este capítulo es escribir funciones en R base, así no necesitarás paquetes extra. 8.2 ¿Cuándo deberías escribir una función? Debes considerar escribir una función cuando has copiado y pegado un bloque de código más de dos veces (ej. ahora tienes tres copias del mismo código). Por ejemplo, mira a este código. Qué es lo que realiza? df &lt;- tibble::tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE)) df$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE)) Es posible que puedas descifrar que esto reescala cada columna para tener un rango de 0 a 1. ¿Pero has visto el error? He cometido un error copiando-y-pegando el código para df$b: He olvidado de cambiar a a b. Extraer código repetido en una función es una buena idea porque te previene de cometer errores como este: Para escribir una función primero necesitas analizar el código. Cuantos inputs tiene? (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) Este código tiene un solo input df$a. (Si te sorprende que TRUE no es un input, puedes explorar el ejercicio de abajo). Para hacer los inputs mas claros, es buena idea reescribir el código usando variables temporadles con nombres generales. Aca el código solamente requiere un solo vector numérico, por lo que lo llamaré x: x &lt;- df$a (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) #&gt; [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601 Hay algo de duplicación en este código. Estamos computando el rango de datos tres veces, así que tiene razón hacerlo en un solo paso: rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) #&gt; [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601 Sacando cálculos intermedios en variables nombradas es una buena práctica porque deja más claro lo que está haciendo el código. Ahora que has simplificado el código, y chequeado de que aún funciona, puedo convertirlo en una función: rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01(c(0, 5, 10)) #&gt; [1] 0.0 0.5 1.0 Hay 3 pasos clasves para crear una funció nueva: Necesitas elegir un nombre para la función. Aquí he usado rescale01 poque esta función reescala un vector para que se encuentre entre 0 y 1. Listas los inputs, o argumentos, a la función dentro de function. Aquí solo tenemos un argumento. Si tenemos más, la llamada se vería como function(x, y, z). Situas el código que has creado en cuerpo de una función, un { bloque lo que inmediatamente sigue a function(...). Ten en cuenta el proceso general: yo solo creo la función después de darme cuenta como funciona con una entrada simple. Es más fácil de empezar con codigo funcionando y convertirlo en una función; es más dificil de crear una función y luego intentar de hacerlo trabajar. En este punto es una buena idea controlar tu función con algunos inputs diferentes: rescale01(c(-10, 0, 10)) #&gt; [1] 0.0 0.5 1.0 rescale01(c(1, 2, 3, NA, 5)) #&gt; [1] 0.00 0.25 0.50 NA 1.00 A medida que escribas más y más funciones eventualmente querras convertir estos informales, tests interactivos en tests formales y automatizados. Este proceso se llama examen de la unidad. Desafortunadamente, esto está más allá del alcance de este libro, pero puede aprender sobre eso en http://r-pkgs.had.co.nz/tests.html. Podemos simplificar el ejemplo original ahora que tenemos una función: df$a &lt;- rescale01(df$a) df$b &lt;- rescale01(df$b) df$c &lt;- rescale01(df$c) df$d &lt;- rescale01(df$d) Comparado al original, este código es fácil de entender y hemos eliminado erorres del tipo copiar-y-pegar-. Esto es aún un poco de duplicación ya que estamos relizando lo mismo a diferentes columnas. Aprenderemos como eliminar esta duplicación en [iteración], una vez que hayas aprendido más sobre las estructuras de R en vectores. Otra ventaja de las funcioens es que nuestros requerimientos camiban, solo necesitamos hacer cambios en un solo lugar. Por ejemplo, podríamos descubrir que algunas de nuestras variables incluyen valores infinitos y rescale01() falla: x &lt;- c(1:10, Inf) rescale01(x) #&gt; [1] 0 0 0 0 0 0 0 0 0 0 NaN Porque hemos extraído el código en una función, nosotros solo necesitamos corregirlo en un solo lugar: rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE, finite = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01(x) #&gt; [1] 0.000 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000 Inf Esta es una importante parte de “no repetirse a uno mismo” (o DRY) principio. Cuanto más repetición tengas en tu código, la mayor parte de lugares que neceistas recordar de actualizar cuando las cosas cambian (¡y esto siempre sucede!), y es más probable que crees errores (bugs) a lo largo del tiempo. 8.2.1 Práctica ¿Por qué TRUE no es un parámetro para rescale01()? ¿Qué pasaría si x está contenido en un valor único perdido y na.rm fuese FALSE? En la segunda variante de rescale01(), los valores infinitos se dejan sin cambio. Reescribe rescale01() así -Inf is convertido a 0, y Inf es convertido a 1. Practica convertir los siguientes fragmentos de código en funciones. Piensa en lo que hace cada función. ¿Cómo lo llamarías? ¿Cuántos argumentos necesita? ¿Puedes reescribirlo para ser más expresivo o menos duplicado? mean(is.na(x)) x / sum(x, na.rm = TRUE) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) Sigue http://nicercode.github.io/intro/writing-functions.html para escribir tus propias funciones para computar la variancia y el sesgo de un vector numérico. Escribe both_na(), una función que toma dos vectores de la misma longitud y retorna el número de posiciones que tiene NA en ambos vectores. ¿Qué hacen las siguientes funciones? ¿Por qué son tan útiles aún cuando son tan cortas? is_directory &lt;- function(x) file.info(x)$isdir is_readable &lt;- function(x) file.access(x, 4) == 0 Lee el complete lyrics de “Pequeño conejito Foo Foo”. There’s a lot of duplication in this song. Extiende el ejemplo inicial de pipes para recrear la canción completa, usar las funciones para reducir la duplicación. 8.3 Las funciones son para los humanos y las computadoras Es importante recordar que las funciones no son solo para las computadoras, sino también para los seres humanos. A R no le importa el nombre de tu función ni los comentarios que tiene, pero si serán importantes para los humanos que la lean. En esta sección se discutirán algunas cosas que debes tener en mente a la hora de escribir funciones entendibles para los humanos. The name of a function is important. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names. Generalmente, los nombres de las funciones deberían ser verbos, y los argumentos sustantivos. Hay algunas excepciones: usar un sustantivo es correcto si la función computa el valor de un sustantivo muy conocido (ejemplo: mean() — (del inglés media) es mejor que compute_mean()— (del inglés computar media)), o accede a alguna propiedad del objeto (ejemplo: coef()— (abreviatura del inglés coeficientes) es mejor que get_coefficients()— (del inglés obtener coeficientes) ). Una buena señal de que un sustantivo puede ser una mejor elección es analizar si estás usando un verbo muy amplio como “obtener”, “computar”, “calcular” o “determinar”. Utiliza tu mejor criterio y no tengas miedo de renombrar tu función si encuentras un nombre mejor más tarde. # Too short f() # Not a verb, or descriptive my_awesome_function() # Long, but clear impute_missing() collapse_years() Si el nombre de tu función está compuesto por múltiples palabras, recomiendo usar “caso_serpiente”, donde cada palabra en minúscula está separada por un guión bajo. Otra alternativa popular es caso Camello. No importa realmente cual elijas, lo importante es que seas consistente: elije una o la otra y quédate con ella. R mismo no es muy consistente, pero no hay nada que tú puedas hacer con respecto a eso. Asegúrate de no caer en la misma trampa haciendo tu código lo más consistente posible. # Never do this! col_mins &lt;- function(x, y) {} rowMaxes &lt;- function(y, x) {} Si tienes una familia de funciones que hacen cosas similares, asegúrate de que tengan nombres y argumentos consistentes. Usa un prefijo común para indicar que están conectadas. Eso es mejor que usar un sufijo común, ya que el predictivo te permite escribir el prefijo y ver todos los otros miembros de la familia. # Good input_select() input_checkbox() input_text() # Not so good select_input() checkbox_input() text_input() Un buen ejemplo de este diseño es el paquete stringr: si no recuerdas exactamente que función necesitas, puedes escribir str_y refrescar tu memoria. Siempre que sea posible, evita anular funciones y variables pre-existentes. Es imposible hacer esto en general, ya que hay un montón de nombres buenos que ya han sido utilizados por otros paquetes. De todas maneras, evitar el uso de los nombres más comunes de R base te ahorrará confusiones. # ¡No hagas esto! T &lt;- FALSE c &lt;- 10 mean &lt;- function(x) sum(x) Usa comentarios, líneas que comienzan con #, para explicar el “porqué” de tu código. En general deberías evitar comentarios que expliquen el “qué” y el “cómo”. Si no se entiende que es lo que hace el código leyéndolo, deberías pensar cómo reescribirlo de manera que sea más claro. ¿Necesitamos agregar algunas variables intermedias con nombres útiles? ¿Deberíamos dividir una función larga en subcomponentes para que pueda ser nombrada?. Sin embargo, tu código nunca podrá capturar la razón detrás de tus decisiones: ¿Por qué elegiste este enfoque frente a otras alternativas?, ¿Qué otra cosa probaste que no funcionó?. Es una gran idea capturar este tipo de pensamientos en un comentario. Otro uso importante de los comentarios es para dividir tu archivo en partes de modo que resulte más fácil de leer. Utiliza líneas largas de- y = para que resulte más fácil detectar los fragmentos. # Cargar los datos -------------------------------------- # Graficar los datos -------------------------------------- RStudio proporciona un abreviado de teclado para crear estos encabezados (Cmd/Ctrl + Shift + R), y te los muestra en el menú desplegable de navegación de código en la parte inferior izquierda del editor: 8.3.1 Ejercicios Lee el código fuente para cada una de las siguientes tres funciones, interpreta que hacen, y luego propone nombres mejores. f1 &lt;- function(string, prefix) { substr(string, 1, nchar(prefix)) == prefix } f2 &lt;- function(x) { if (length(x) &lt;= 1) return(NULL) x[-length(x)] } f3 &lt;- function(x, y) { rep(y, length.out = length(x)) } Toma una función que hayas escrito recientemente y tómate 5 minutos para pensar un mejor nombre para la función y para sus argumentos. Compara y contrasta rnorm() y MASS::mvrnorm(). ¿Cómo podrías hacerlas más consistentes? Argumenta porqué norm_r(),norm_d() etc sería una mejor opción que rnorm(), dnorm(). Argumenta lo contrario. 8.4 Ejecución condicional Una sentencia ifle permite ejecutar un código condicional. Por ejemplo: if (condition) { # code executed when condition is TRUE } else { # code executed when condition is FALSE } Para obtener ayuda en if necesitas ponerlo entre comillas simples: ?`if`. La ayuda no es especialmente útil si aún no eres un programador experimentado ¡Pero al menos puedes saber cómo llegar a ella! Aquí se presenta una función simple que utiliza una sentencia if. El objetivo de esta función es devolver un vector lógico que describa si se nombra o no cada elemento de un vector. has_name &lt;- function(x) { nms &lt;- names(x) if (is.null(nms)) { rep(FALSE, length(x)) } else { !is.na(nms) &amp; nms != &quot;&quot; } } Esta función aprovecha la regla de retorno estándar: una función devuelve el último valor que calculó. Este es uno de los dos usos de la declaración if. 8.4.1 Condiciones La condición debe evaluar a TRUE o FALSE. Si es un vector, recibirás un mensaje de advertencia; si es una NA, obtendrás un error. Tenga cuidado con estos mensajes en su propio código: if (c(TRUE, FALSE)) {} #&gt; Warning in if (c(TRUE, FALSE)) {: the condition has length &gt; 1 and only the #&gt; first element will be used #&gt; NULL if (NA) {} #&gt; Error in if (NA) {: missing value where TRUE/FALSE needed Puedes usar || (o) y &amp;&amp;(y) para combinar múltiples expresiones lógicas. Estos operadores están “haciendo cortocircuito”: tan pronto como || ve el primer TRUE devuelve TRUE sin calcular nada más. Tan pronto como &amp;&amp; vea el primer FALSE, devuelve FALSE. Nunca debes usar|o &amp; en una sentencia if: estas son operaciones vectorizadas que se aplican a valores múltiples (es por eso que las usas en filter()). Si tienes un vector lógico, puede usar any() o all() para juntarlo en un único valor. Be careful when testing for equality. == is vectorised, which means that it’s easy to get more than one output. Either check the length is already 1, collapse with all() or any(), or use the non-vectorised identical(). identical() is very strict: it always returns either a single TRUE or a single FALSE, and doesn’t coerce types. This means that you need to be careful when comparing integers and doubles: Tenga cuidado al probar la igualdad.== está vectorizado, lo que significa que es fácil obtener más de una salida. Compruebe si la longitud ya es 1, colapse con all() o any(), o usa el no vectorizado identical(). identical() es muy estricto: siempre devuelve un solo TRUE o un solo FALSE, y no coacciona tipos. Esto significa que debe tener cuidado al comparar enteros y dobles: identical(0L, 0) #&gt; [1] FALSE También hay que tener cuidado con los números de punto flotante: x &lt;- sqrt(2) ^ 2 x #&gt; [1] 2 x == 2 #&gt; [1] FALSE x - 2 #&gt; [1] 4.44e-16 En su lugar use dplyr::near() para comparaciones, como se describe en [comparisons]. Y recuerde, x == NA ¡No hace nada útil! 8.4.2 Condiciones múltiples Puede encadenar múltiples sentencias if juntas: if (this) { # do that } else if (that) { # do something else } else { # } Pero si terminas con una larga serie de sentencias if encadenadas, deberías considerar reescribir. Una técnica útil es la función switch() . Esta le permite evaluar el código seleccionado según la posición o el nombre. #&gt; function(x, y, op) { #&gt; switch(op, #&gt; plus = x + y, #&gt; minus = x - y, #&gt; times = x * y, #&gt; divide = x / y, #&gt; stop(&quot;Unknown op!&quot;) #&gt; ) #&gt; } Otra función útil que a menudo puede eliminar largas cadenas de sentencias ifes cut(). Esta es utilizada para discretizar variables continuas. 8.4.3 Estilo de código Ambas sentencias if y function deberían (casi) siempre ir entre llaves ({}), y el contenido debería estar seguido de dos espacios. Esto hace que sea más fácil ver la jerarquía en su código en el margen izquierdo. La llave de apertura nunca debe ir en su propia línea y siempre debe ir seguida de una nueva línea. Una llave de cierre siempre debe ir en su propia línea, a menos que sea seguida por else. Siempre agregar espacio en el código dentro de las llaves. # Good if (y &lt; 0 &amp;&amp; debug) { message(&quot;Y is negative&quot;) } if (y == 0) { log(x) } else { y ^ x } # Bad if (y &lt; 0 &amp;&amp; debug) message(&quot;Y is negative&quot;) if (y == 0) { log(x) } else { y ^ x } Estaría bien evitar las llaves si tienes una sentencia if muy corta que pueda entrar en una línea: y &lt;- 10 x &lt;- if (y &lt; 20) &quot;Too low&quot; else &quot;Too high&quot; Esto se recomienda solo para sentencias if muy breves. De lo contrario, la sentencia completa es más fácil de leer: if (y &lt; 20) { x &lt;- &quot;Too low&quot; } else { x &lt;- &quot;Too high&quot; } 8.4.4 Ejercicios ¿Cuál es la diferencia entre if and ifelse()? Lea cuidadosamente la ayuda y construya tres ejemplos que ilustren las diferencias claves. Escriba una función de saludo que diga “buenos días”, “buenas tardes” o “buenas noches”, según la hora del día. (Sugerencia: use un argumento de tiempo que por defecto es lubridate::now(), eso hará que sea más fácil probar su función). Implemente una función fizzbuzz. Toma un solo número como entrada. Si el número es divisible por tres, devuelve “fizz”. Si es divisible por cinco, devuelve “buzz”. Si es divisible por tres y cinco, devuelve “fizzbuzz”. De lo contrario, devuelve el número. Asegúrese de escribir primero el código de trabajo antes de crear la función. ¿Cómo podría usar cut() para simplificar una sentencia if-else anidada? if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } ¿Cómo cambiarías la sentencia a cut() si hubieras usado &lt;en lugar de &lt;=? ¿Cuál es la otra ventaja principal de cut() para este problema? (Sugerencia: ¿qué sucede si tienes muchos valores en temp?) ¿Qué sucedería si usaras switch() con un valor numérico? ¿Qué haría la sentencia switch()? ¿Qúe sucedería si x fuera “e”? switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) Experimente, luego lea cuidadosamente la documentación. 8.5 Function arguments Los argumentos de las funciones normalmente están dentro de dos grupos: por un lado, se proveen los datos a calcular, y por el otro los argumentos que controlan los detalles del cálculo. Por ejemplo: En log(), los datos son x, y los detalles son la base del algoritmo. En mean(), los datos son x, ay los detalles son la cantidad de datos para recortar de los extremos (trim) y como lidiar con los valores en blanco o perdidos (na.rm). En t.test(), los datos son x y y, y los detalles del test son alternative, mu, paired, var.equal, y conf.level. En str_c() puedes suministrar cualquier número de caracteres a ..., y los detalles de la concatenación son contralos por sep y collapse. Generalmente, los datos de los argumentos deben estar primeros. El detalle de los mismos podría estar al final y con valores predeterminados. Se especifica un valor predeterminado de la misma manera en la que es nombrada una función con su argumento: # Compute confidence interval around mean using normal approximation mean_ci &lt;- function(x, conf = 0.95) { se &lt;- sd(x) / sqrt(length(x)) alpha &lt;- 1 - conf mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2)) } x &lt;- runif(100) mean_ci(x) #&gt; [1] 0.498 0.610 mean_ci(x, conf = 0.99) #&gt; [1] 0.480 0.628 El valor predeterminado debería ser casi siempre el valor más común. Pocas excepciones a esta regla podrían ser seguras. Por ejemplo, tiene sentido que na.rm por defecto sea FALSE porque los valores faltantes son importantes. Aunque na.rm = TRUE que es lo que usualmente pones en tu codigo, es una mala idea ignorar silenciosamente por defecto. Cuando nombras a una función, generalmente omites los nombres de los argumentos de datos justamente porque son los más comúnmente usados. Si anulas los valores predeterminados del detalle de los argumentos, se debería usar el nombre completo: # Good mean(1:10, na.rm = TRUE) # Bad mean(x = 1:10, , FALSE) mean(, TRUE, x = c(1:10, NA)) Puedes referirte al argumento por su único prefijo (ej. mean(x, n = TRUE)), pero generalmente es mejor evitar determinadas posibilidades de confusión. Ten en cuena que cuando invocas a una función, debes colocar un espacio alrededor de = en la invocación a la función, y siempre poner un espacio después de la coma, no antes (como regularmente en inglés). El uso del espacio en blanco hace más fácil ojear la función para los componentes importantes. # Good average &lt;- mean(feet / 12 + inches, na.rm = TRUE) # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) 8.5.1 Elección de nombres Los nombres de los argumentos son también importantes. Para R no es sumamente importante pero si para los usuarios (¡incluyéndote en un futuro!). En general, se podrían preferir nombres más largos y más descriptivos, pero hay un puñado de nombres muy comunes y muy cortos. Vale la pena memorizar estos: x, y, z: vectores. w: un vector de pesos. df: a data frame. i, j: índices numéricos (usualmente filas y columnas). n: longitud, or número de filas. p: número de columnas. Aunque, se debería considerar la coincidencia de nombres de argumentos en funciones de R. Por ejemplo, usa na.rm para determinar si los valores faltantes podrían ser eliminados. 8.5.2 Controlando valores A medida que comienzas a utilizar más funciones, eventualmente llegarás al punto en el que no la recordarás. En este punto es común que suceda la invalidación de la entrada de la función. Para evitar este problema, a menudo es útil hacer las restricciones explícitas. Por ejemplo, imagina que has escrito algunas funciones para calcular las estadísticas de resumen ponderadas: wt_mean &lt;- function(x, w) { sum(x * w) / sum(w) } wt_var &lt;- function(x, w) { mu &lt;- wt_mean(x, w) sum(w * (x - mu) ^ 2) / sum(w) } wt_sd &lt;- function(x, w) { sqrt(wt_var(x, w)) } ¿Qué pasa si x y w no son de la misma longitud? wt_mean(1:6, 1:3) #&gt; [1] 7.67 En este caso, debido a las reglas de los vectores de R, no obtenemos un error. Es una buena práctica verificar las precondiciones, y arrojar un error (con stop()), si esto no es verdadero: wt_mean &lt;- function(x, w) { if (length(x) != length(w)) { stop(&quot;`x` and `w` must be the same length&quot;, call. = FALSE) } sum(w * x) / sum(w) } Ten cuidado de no llevar esto demasiado lejos. Hay una compensación entre la cantidad de tiempo que inviertes en hacer que tu función sea sólida, en comparación con el tiempo que pasas escribiéndola. Por ejemplo, si además agregas a la función un argumento na.rm, probablemente no lo verificaste con cuidado: wt_mean &lt;- function(x, w, na.rm = FALSE) { if (!is.logical(na.rm)) { stop(&quot;`na.rm` must be logical&quot;) } if (length(na.rm) != 1) { stop(&quot;`na.rm` must be length 1&quot;) } if (length(x) != length(w)) { stop(&quot;`x` and `w` must be the same length&quot;, call. = FALSE) } if (na.rm) { miss &lt;- is.na(x) | is.na(w) x &lt;- x[!miss] w &lt;- w[!miss] } sum(w * x) / sum(w) } Esto es mucho trabajo con poca ganancia adicional. Un compromiso útil es incorporar stopifnot(): esto comprueba que cada argumento sea TRUE, en caso contrario genera un mensaje de error. wt_mean &lt;- function(x, w, na.rm = FALSE) { stopifnot(is.logical(na.rm), length(na.rm) == 1) stopifnot(length(x) == length(w)) if (na.rm) { miss &lt;- is.na(x) | is.na(w) x &lt;- x[!miss] w &lt;- w[!miss] } sum(w * x) / sum(w) } wt_mean(1:6, 6:1, na.rm = &quot;foo&quot;) #&gt; Error in wt_mean(1:6, 6:1, na.rm = &quot;foo&quot;): is.logical(na.rm) is not TRUE Ten en cuenta que al usar stopifnot() afirmas lo que debería ser cierto en lugar de verificar lo que podría estar mal. 8.5.3 Punto-punto-punto (…) Muchas funciones en R tienen un número arbitrario de entrada: sum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) #&gt; [1] 55 stringr::str_c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) #&gt; [1] &quot;abcdef&quot; ¿Cómo funcionan estas funciones? Ellos confían en un argumento especial: ... (Pronunciando punto-punto-punto). Este argumento especial captura cualquier número de arguemntos que no están de otra forma contempladas. Es útil porque puedes enviar estos ... a otra función. Esto es útil si su función envuelve principalmente otra función. Por ejemplo, creo estas funciones de ayuda alrededor de str_c(): commas &lt;- function(...) stringr::str_c(..., collapse = &quot;, &quot;) commas(letters[1:10]) #&gt; [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 cat(title, &quot; &quot;, stringr::str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } rule(&quot;Important output&quot;) #&gt; Important output ------------------------------------------------------ Aquí ... permite remitirse a cualquier argumento que no quiera tratar con str_c(). Esto es muy conveniente. Pero esto viene con un precio: cualquier argumento mal escrito no presentará un error. Esto hace fácil para typos que no se notan: x &lt;- c(1, 2) sum(x, na.mr = TRUE) #&gt; [1] 4 Si quieres captura los valores de ..., utiliza list(...). 8.5.4 Evaluación diferida Los argumentos en R se evalúan con holgazanería: no se computan hasta que se necesitan. Esto significa que si nunca se lo usa, nunca es nombrado. Hay una propiedad importante de R como lenguaje de programación, pero generalmente no es fundamental cuando uno escribe sus propias funciones para el análisis de datos. Puedes leer más acerca de la evaluación diferida http://adv-r.had.co.nz/Functions.html#lazy-evaluation. 8.5.5 Ejercicios ¿Qué realizan las commas(letters, collapse = &quot;-&quot;)? ¿Por qué? Sería bueno si se pudiera suplantar múltiples caracteres al argumento pad, ej., rule(&quot;Title&quot;, pad = &quot;-+&quot;). ¿Por qué esto actualmente no funciona? ¿Cómo podrías solucionarlo? ¿Qué realiza el argumento trim a la función mean()? ¿Cuándo podrías utilizarla? El valor de defecto para el argumento method a cor() es c(&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;). ¿Qué significa esto? ¿Qué valor se utiliza por defecto? 8.6 Valores de salida Darse cuenta qué es lo que tu función debería devolver suele ser bastante directo: ¡es el porqué de crear la función en primer lugar! Hay dos cosas que debes considerar al devolver un valor: ¿Devolver un valor antes, hace que tu función sea más fácil de leer? ¿Puedes hacer tu función apta para un pipe? 8.6.1 Sentencias explícitas de salida El valor de salida de una función suele ser la última sentencia que esta evalúa, pero puedes optar por devolver algo anticipadamente haciendo uso de la función return() ( del inglés devolución). Yo creo que es mejor reservar el uso de la función return() para los casos en donde es posible devolver anticipadamente una solución más simple. Una razón común para hacer esto es por ejemplo que los argumentos estén vacíos: complicated_function &lt;- function(x, y, z) { if (length(x) == 0 || length(y) == 0) { return(0) } # Código complicado aquí } Otra razón puede ser porque tienes una sentencia if con un bloque complicado y uno sencillo. Por ejemplo, podrías escribir una sentencia if de esta manera: f &lt;- function() { if (x) { # Haz # algo # que # tome # muchas # lineas # para # expresar } else { # retorna algo corto } } Pero si el primer bloque es muy largo, para cuando llegas al else, ya te has olvidado la condition. Una forma de reescribir esto es usar una salida anticipada para el caso sencillo: f &lt;- function() { if (!x) { return(something_short) } # Haz # algo # que # tome # muchas # lineas # para # expresar } Esto permite hacer el código más fácil de entender, ya que no necesitas tanto contexto para interpretarlo. 8.6.2 Escribir funciones aptas para un pipe Si quieres escribir tus propias funciones pipeables, es importante que pienses sobre los valores de salida. Conocer el tipo de objeto de tu valor de salida significará que tu línea pipeable funciona. Por ejemplo, en dplyr y tidyr el tipo de objeto es un data frame. Hay dos tipos básicos de funciones pipeables: transformaciones y de efectos secundarios. En las transformaciones, se ingresa un objeto como primer argumento y luego se devuelve una versión modificada del mismo. En el caso de efectos_secundarios, el objeto ingresado no es modificado sino que la función actúa sobre el objeto. Un ejemplo sería dibujar un gráfico o guardar un archivo. Las funciones de efectos secundarios deben “invisibilizar” el primer argumento, de forma que mientras no sean impresos puedan seguir siendo usados en una línea pipeable. Por ejemplo, esta función imprime el número de valores faltantes en un data frame: show_missings &lt;- function(df) { n &lt;- sum(is.na(df)) cat(&quot;Missing values: &quot;, n, &quot;\\n&quot;, sep = &quot;&quot;) invisible(df) } Si la llamamos de manera interactiva, el comando invisible() significa que el valor de entrada df no se imprime: show_missings(mtcars) #&gt; Missing values: 0 Pero sigue estando ahí, solamente que no se imprime por defecto: x &lt;- show_missings(mtcars) #&gt; Missing values: 0 class(x) #&gt; [1] &quot;data.frame&quot; dim(x) #&gt; [1] 32 11 Y todavía podemos usarlo en un pipe: mtcars %&gt;% show_missings() %&gt;% mutate(mpg = ifelse(mpg &lt; 20, NA, mpg)) %&gt;% show_missings() #&gt; Missing values: 0 #&gt; Missing values: 18 8.7 Entorno El último componente de una función es su entorno. Esto no es algo que debes entender profundamente cuando tu empieas a escribir funciones. Sin embargo, es importante saber un poco acerca de entornos porque es crucial para que algunas funciones trabajen. El entorno de una función controal como R encuentra el valor asociado con un nombre. Por ejemplo, la siguiente función: f &lt;- function(x) { x + y } En muchos lenguajes de programación, esto sería un error, porque y no está definido dentro de la función. En R, esto es un código válido ya que R usa reglas llamadas lexical scoping para encontrar el valor asociado a un nombre. Como y no está definido dentro de la función, R mirará dentro del entorno donde la función fue definida: y &lt;- 100 f(10) #&gt; [1] 110 y &lt;- 1000 f(10) #&gt; [1] 1010 Este comportamiento parece una receta para errores, y de hecho debes evitar crear funciones como esta deliberadamente, pero en general no causa demasiados problemas (especialmente si reinicias regularmente R para llegar a un borrón y cuenta nueva). La ventaja de este comportamiento es que, desde el punto de vista del lenguaje, permite que R sea muy consistente. Cada nombre es buscado usando el mismo conjunto de reglas. Para f()incluye el comportamiento de dos cosas que podrías no esperar: { y +. Esto permite hacer cosas tortuosas como la siguiente: `+` &lt;- function(x, y) { if (runif(1) &lt; 0.1) { sum(x, y) } else { sum(x, y) * 1.1 } } table(replicate(1000, 1 + 2)) #&gt; #&gt; 3 3.3 #&gt; 100 900 rm(`+`) Hay un fenómeno común en R. R pone algunos límites a tu poder. Puedes hacer cosas que no podrías en otro lenguaje de programación. Puedes hacer cosas que el 99% de las veces son extremadamente desacertadas (¡como ignorar como funciona la adición!). Pero esta flexibilidad es lo que hace que herramientas como ggplot2 y dplyr sea posible. Aprender de como hacer el mejor uso de esta flexibilidad está mas allá del alcance de este libro, pero puedes leer al respecto en Advanced R. "],
["hierarchy.html", "9 Hierarchical data 9.1 Introduction 9.2 Initial exploration 9.3 Extracting deeply nested elements 9.4 Removing a level of hierarchy 9.5 Switching levels in the hierarchy 9.6 Turning lists into data frames", " 9 Hierarchical data 9.1 Introduction This chapter belongs in wrangle: it will give you a set of tools for working with hierarchical data, such as the deeply nested lists you often get when working with JSON. However, you can only learn it now because working with hierarchical structures requires some programming skills, particularly an understanding of data structures, functions, and iteration. Now you have those tools under your belt, you can learn how to work with hierarchical data. The As well as tools to simplify iteration, purrr provides tools for handling deeply nested lists. There are three common sources of such data: JSON and XML The map functions apply a function to every element in a list. They are the most commonly used part of purrr, but not the only part. Since lists are often used to represent complex hierarchies, purrr also provides tools to work with hierarchy: You can extract deeply nested elements in a single call by supplying a character vector to the map functions. You can remove a level of the hierarchy with the flatten functions. You can flip levels of the hierarchy with the transpose function. 9.1.1 Prerequisites This chapter focusses mostly on purrr. As well as the tools for iteration that you’ve already learned about, purrr also provides a number of tools specifically designed to manipulate hierarchical data. library(purrr) 9.2 Initial exploration Sometimes you get data structures that are very deeply nested. A common source of such data is JSON from a web API. I’ve previously downloaded a list of GitHub issues related to this book and saved it as issues.json. Now I’m going to load it into a list with jsonlite. By default fromJSON() tries to be helpful and simplifies the structure a little for you. Here I’m going to show you how to do it with purrr, so I set simplifyVector = FALSE: # From https://api.github.com/repos/hadley/r4ds/issues issues &lt;- jsonlite::fromJSON(&quot;issues.json&quot;, simplifyVector = FALSE) You might be tempted to use str() on this data. Unfortunately, however, str() is not designed for lists that are both deep and wide, and you’ll tend to get overwhelmed by the output. A better strategy is to pull the list apart piece by piece. First, figure out how many elements are in the list, take a look at one, and then check they’re all the same structure. In this case there are eight elements, and the first element is another list. length(issues) #&gt; [1] 8 str(issues[[1]]) #&gt; List of 20 #&gt; $ url : chr &quot;https://api.github.com/repos/hadley/r4ds/issues/11&quot; #&gt; $ labels_url : chr &quot;https://api.github.com/repos/hadley/r4ds/issues/11/labels{/name}&quot; #&gt; $ comments_url: chr &quot;https://api.github.com/repos/hadley/r4ds/issues/11/comments&quot; #&gt; $ events_url : chr &quot;https://api.github.com/repos/hadley/r4ds/issues/11/events&quot; #&gt; $ html_url : chr &quot;https://github.com/hadley/r4ds/pull/11&quot; #&gt; $ id : int 117521642 #&gt; $ number : int 11 #&gt; $ title : chr &quot;Typo correction in file expressing-yourself.Rmd&quot; #&gt; $ user :List of 17 #&gt; ..$ login : chr &quot;shoili&quot; #&gt; ..$ id : int 8914139 #&gt; ..$ avatar_url : chr &quot;https://avatars.githubusercontent.com/u/8914139?v=3&quot; #&gt; ..$ gravatar_id : chr &quot;&quot; #&gt; ..$ url : chr &quot;https://api.github.com/users/shoili&quot; #&gt; ..$ html_url : chr &quot;https://github.com/shoili&quot; #&gt; ..$ followers_url : chr &quot;https://api.github.com/users/shoili/followers&quot; #&gt; ..$ following_url : chr &quot;https://api.github.com/users/shoili/following{/other_user}&quot; #&gt; ..$ gists_url : chr &quot;https://api.github.com/users/shoili/gists{/gist_id}&quot; #&gt; ..$ starred_url : chr &quot;https://api.github.com/users/shoili/starred{/owner}{/repo}&quot; #&gt; ..$ subscriptions_url : chr &quot;https://api.github.com/users/shoili/subscriptions&quot; #&gt; ..$ organizations_url : chr &quot;https://api.github.com/users/shoili/orgs&quot; #&gt; ..$ repos_url : chr &quot;https://api.github.com/users/shoili/repos&quot; #&gt; ..$ events_url : chr &quot;https://api.github.com/users/shoili/events{/privacy}&quot; #&gt; ..$ received_events_url: chr &quot;https://api.github.com/users/shoili/received_events&quot; #&gt; ..$ type : chr &quot;User&quot; #&gt; ..$ site_admin : logi FALSE #&gt; $ labels : list() #&gt; $ state : chr &quot;open&quot; #&gt; $ locked : logi FALSE #&gt; $ assignee : NULL #&gt; $ milestone : NULL #&gt; $ comments : int 0 #&gt; $ created_at : chr &quot;2015-11-18T06:26:09Z&quot; #&gt; $ updated_at : chr &quot;2015-11-18T06:26:09Z&quot; #&gt; $ closed_at : NULL #&gt; $ pull_request:List of 4 #&gt; ..$ url : chr &quot;https://api.github.com/repos/hadley/r4ds/pulls/11&quot; #&gt; ..$ html_url : chr &quot;https://github.com/hadley/r4ds/pull/11&quot; #&gt; ..$ diff_url : chr &quot;https://github.com/hadley/r4ds/pull/11.diff&quot; #&gt; ..$ patch_url: chr &quot;https://github.com/hadley/r4ds/pull/11.patch&quot; #&gt; $ body : chr &quot;The discussion of the code in lines 236-243 was a little confusing with x and y so I proposed changing it to a &quot;| __truncated__ (In this case we got lucky and the structure is (just) simple enough to print out with str(). If you’re unlucky, you may need to repeat this procedure.) tibble::tibble( i = seq_along(issues), names = issues %&gt;% map(names) ) %&gt;% tidyr::unnest(names) %&gt;% table() %&gt;% t() #&gt; i #&gt; names 1 2 3 4 5 6 7 8 #&gt; assignee 1 1 1 1 1 1 1 1 #&gt; body 1 1 1 1 1 1 1 1 #&gt; closed_at 1 1 1 1 1 1 1 1 #&gt; comments 1 1 1 1 1 1 1 1 #&gt; comments_url 1 1 1 1 1 1 1 1 #&gt; created_at 1 1 1 1 1 1 1 1 #&gt; events_url 1 1 1 1 1 1 1 1 #&gt; html_url 1 1 1 1 1 1 1 1 #&gt; id 1 1 1 1 1 1 1 1 #&gt; labels 1 1 1 1 1 1 1 1 #&gt; labels_url 1 1 1 1 1 1 1 1 #&gt; locked 1 1 1 1 1 1 1 1 #&gt; milestone 1 1 1 1 1 1 1 1 #&gt; number 1 1 1 1 1 1 1 1 #&gt; pull_request 1 1 1 0 0 0 0 0 #&gt; state 1 1 1 1 1 1 1 1 #&gt; title 1 1 1 1 1 1 1 1 #&gt; updated_at 1 1 1 1 1 1 1 1 #&gt; url 1 1 1 1 1 1 1 1 #&gt; user 1 1 1 1 1 1 1 1 Another alternative is the listviewer package, https://github.com/timelyportfolio/listviewer. 9.3 Extracting deeply nested elements To work with this sort of data, you typically want to turn it into a data frame by extracting the related vectors that you’re most interested in: issues %&gt;% map_int(&quot;id&quot;) #&gt; [1] 117521642 110795521 109680972 107925580 107506216 99430051 99430007 #&gt; [8] 99429843 issues %&gt;% map_lgl(&quot;locked&quot;) #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE issues %&gt;% map_chr(&quot;state&quot;) #&gt; [1] &quot;open&quot; &quot;open&quot; &quot;open&quot; &quot;open&quot; &quot;open&quot; &quot;open&quot; &quot;open&quot; &quot;open&quot; You can use the same technique to extract more deeply nested structure. For example, imagine you want to extract the name and id of the user. You could do that in two steps: users &lt;- issues %&gt;% map(&quot;user&quot;) users %&gt;% map_chr(&quot;login&quot;) #&gt; [1] &quot;shoili&quot; &quot;benmarwick&quot; &quot;datalove&quot; &quot;hadley&quot; &quot;hadley&quot; #&gt; [6] &quot;hadley&quot; &quot;hadley&quot; &quot;hadley&quot; users %&gt;% map_int(&quot;id&quot;) #&gt; [1] 8914139 1262179 222907 4196 4196 4196 4196 4196 But by supplying a character vector to map_*, you can do it in one: issues %&gt;% map_chr(c(&quot;user&quot;, &quot;login&quot;)) #&gt; [1] &quot;shoili&quot; &quot;benmarwick&quot; &quot;datalove&quot; &quot;hadley&quot; &quot;hadley&quot; #&gt; [6] &quot;hadley&quot; &quot;hadley&quot; &quot;hadley&quot; issues %&gt;% map_int(c(&quot;user&quot;, &quot;id&quot;)) #&gt; [1] 8914139 1262179 222907 4196 4196 4196 4196 4196 What happens if that path is missing in some of the elements? For example, lets try and extract the HTML url to the pull request: issues %&gt;% map_chr(c(&quot;pull_request&quot;, &quot;html_url&quot;)) #&gt; Result 4 must be a single string, not NULL of length 0 Unfortunately that doesn’t work. Whenever you see an error from purrr complaining about the “type” of the result, it’s because it’s trying to shove it into a simple vector (here a character). You can diagnose the problem more easily if you use map(): issues %&gt;% map(c(&quot;pull_request&quot;, &quot;html_url&quot;)) #&gt; [[1]] #&gt; [1] &quot;https://github.com/hadley/r4ds/pull/11&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;https://github.com/hadley/r4ds/pull/7&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;https://github.com/hadley/r4ds/pull/6&quot; #&gt; #&gt; [[4]] #&gt; NULL #&gt; #&gt; [[5]] #&gt; NULL #&gt; #&gt; [[6]] #&gt; NULL #&gt; #&gt; [[7]] #&gt; NULL #&gt; #&gt; [[8]] #&gt; NULL To get the results into a character vector, we need to tell purrr what it should change NULL to. You can do that with the .null argument. The most common value to use is NA: issues %&gt;% map_chr(c(&quot;pull_request&quot;, &quot;html_url&quot;), .null = NA) #&gt; [1] &quot;https://github.com/hadley/r4ds/pull/11&quot; #&gt; [2] &quot;https://github.com/hadley/r4ds/pull/7&quot; #&gt; [3] &quot;https://github.com/hadley/r4ds/pull/6&quot; #&gt; [4] NA #&gt; [5] NA #&gt; [6] NA #&gt; [7] NA #&gt; [8] NA (You might wonder why that isn’t the default value since it’s so useful. Well, if it was the default, you’d never get an error message if you had a typo in the names. You’d just get a vector of missing values. That would be annoying to debug because it’s a silent failure.) It’s possible to mix position and named indexing by using a list issues %&gt;% map_chr(list(&quot;pull_request&quot;, 1), .null = NA) #&gt; [1] &quot;https://api.github.com/repos/hadley/r4ds/pulls/11&quot; #&gt; [2] &quot;https://api.github.com/repos/hadley/r4ds/pulls/7&quot; #&gt; [3] &quot;https://api.github.com/repos/hadley/r4ds/pulls/6&quot; #&gt; [4] NA #&gt; [5] NA #&gt; [6] NA #&gt; [7] NA #&gt; [8] NA 9.4 Removing a level of hierarchy As well as indexing deeply into hierarchy, it’s sometimes useful to flatten it. That’s the job of the flatten family of functions: flatten(), flatten_lgl(), flatten_int(), flatten_dbl(), and flatten_chr(). In the code below we take a list of lists of double vectors, then flatten it to a list of double vectors, then to a double vector. x &lt;- list(list(a = 1, b = 2), list(c = 3, d = 4)) str(x) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ a: num 1 #&gt; ..$ b: num 2 #&gt; $ :List of 2 #&gt; ..$ c: num 3 #&gt; ..$ d: num 4 y &lt;- flatten(x) str(y) #&gt; List of 4 #&gt; $ a: num 1 #&gt; $ b: num 2 #&gt; $ c: num 3 #&gt; $ d: num 4 flatten_dbl(y) #&gt; [1] 1 2 3 4 Graphically, that sequence of operations looks like: Whenever I get confused about a sequence of flattening operations, I’ll often draw a diagram like this to help me understand what’s going on. Base R has unlist(), but I recommend avoiding it for the same reason I recommend avoiding sapply(): it always succeeds. Even if your data structure accidentally changes, unlist() will continue to work silently the wrong type of output. This tends to create problems that are frustrating to debug. 9.5 Switching levels in the hierarchy Other times the hierarchy feels “inside out”. You can use transpose() to flip the first and second levels of a list: x &lt;- list( x = list(a = 1, b = 3, c = 5), y = list(a = 2, b = 4, c = 6) ) x %&gt;% str() #&gt; List of 2 #&gt; $ x:List of 3 #&gt; ..$ a: num 1 #&gt; ..$ b: num 3 #&gt; ..$ c: num 5 #&gt; $ y:List of 3 #&gt; ..$ a: num 2 #&gt; ..$ b: num 4 #&gt; ..$ c: num 6 x %&gt;% transpose() %&gt;% str() #&gt; List of 3 #&gt; $ a:List of 2 #&gt; ..$ x: num 1 #&gt; ..$ y: num 2 #&gt; $ b:List of 2 #&gt; ..$ x: num 3 #&gt; ..$ y: num 4 #&gt; $ c:List of 2 #&gt; ..$ x: num 5 #&gt; ..$ y: num 6 Graphically, this looks like: You’ll see an example of this in the next section, as transpose() is particularly useful in conjunction with adverbs like safely() and quietly(). It’s called transpose by analogy to matrices. When you subset a transposed matrix, you switch indices: x[i, j] is the same as t(x)[j, i]. It’s the same idea when transposing a list, but the subsetting looks a little different: x[[i]][[j]] is equivalent to transpose(x)[[j]][[i]]. Similarly, a transpose is its own inverse so transpose(transpose(x)) is equal to x. Transpose is also useful when working with JSON APIs. Many JSON APIs represent data frames in a row-based format, rather than R’s column-based format. transpose() makes it easy to switch between the two: df &lt;- tibble::tibble(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) df %&gt;% transpose() %&gt;% str() #&gt; List of 3 #&gt; $ :List of 2 #&gt; ..$ x: int 1 #&gt; ..$ y: chr &quot;a&quot; #&gt; $ :List of 2 #&gt; ..$ x: int 2 #&gt; ..$ y: chr &quot;b&quot; #&gt; $ :List of 2 #&gt; ..$ x: int 3 #&gt; ..$ y: chr &quot;c&quot; 9.6 Turning lists into data frames Have a deeply nested list with missing pieces Need a tidy data frame so you can visualise, transform, model etc. What do you do? By hand with purrr, talk about fromJSON and tidyJSON tidyjson 9.6.1 Exercises Challenge: read all the CSV files in a directory. Which ones failed and why? files &lt;- dir(&quot;data&quot;, pattern = &quot;\\\\.csv$&quot;) files %&gt;% set_names(., basename(.)) %&gt;% map_df(safely(readr::read_csv), .id = &quot;filename&quot;) %&gt;% "],
["data-import.html", "10 Data import 10.1 Introduction 10.2 Getting started 10.3 Parsing a vector 10.4 Parsing a file 10.5 Writing to a file 10.6 Other types of data", " 10 Data import 10.1 Introduction Working with data provided by R packages is a great way to learn the tools of data science, but at some point you want to stop learning and start working with your own data. In this chapter, you’ll learn how to read plain-text rectangular files into R. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data. We’ll finish with a few pointers to packages that are useful for other types of data. 10.1.1 Prerequisites In this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse. library(tidyverse) 10.2 Getting started Most of readr’s functions are concerned with turning flat files into data frames: read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter. read_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space. read_log() reads Apache style log files. (But also check out webreadr which is built on top of read_log() and provides many more helpful tools.) These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand read_csv(), you can easily apply your knowledge to all the other functions in readr. The first argument to read_csv() is the most important: it’s the path to the file to read. heights &lt;- read_csv(&quot;data/heights.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; earn = col_double(), #&gt; height = col_double(), #&gt; sex = col_character(), #&gt; ed = col_double(), #&gt; age = col_double(), #&gt; race = col_character() #&gt; ) When you run read_csv() it prints out a column specification that gives the name and type of each column. That’s an important part of readr, which we’ll come back to in parsing a file. You can also supply an inline csv file. This is useful for experimenting with readr and for creating reproducible examples to share with others: read_csv(&quot;a,b,c 1,2,3 4,5,6&quot;) #&gt; # A tibble: 2 x 3 #&gt; a b c #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 In both cases read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behaviour: Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = &quot;#&quot; to drop all lines that start with (e.g.) #. read_csv(&quot;The first line of metadata The second line of metadata x,y,z 1,2,3&quot;, skip = 2) #&gt; # A tibble: 1 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 read_csv(&quot;# A comment I want to skip x,y,z 1,2,3&quot;, comment = &quot;#&quot;) #&gt; # A tibble: 1 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 The data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings, and instead label them sequentially from X1 to Xn: read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = FALSE) #&gt; # A tibble: 2 x 3 #&gt; X1 X2 X3 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 (&quot;\\n&quot; is a convenient shortcut for adding a new line. You’ll learn more about it and other types of string escape in string basics.) Alternatively you can pass col_names a character vector which will be used as the column names: read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;)) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 Another option that commonly needs tweaking is na: this specifies the value (or values) that are used to represent missing values in your file: read_csv(&quot;a,b,c\\n1,2,.&quot;, na = &quot;.&quot;) #&gt; # A tibble: 1 x 3 #&gt; a b c #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 1 2 NA This is all you need to know to read ~75% of CSV files that you’ll encounter in practice. You can also easily adapt what you’ve learned to read tab separated files with read_tsv() and fixed width files with read_fwf(). To read in more challenging files, you’ll need to learn more about how readr parses each column, turning them into R vectors. 10.2.1 Compared to base R If you’ve used R before, you might wonder why we’re not using read.csv(). There are a few good reasons to favour readr functions over the base equivalents: They are typically much faster (~10x) than their base equivalents. Long running jobs have a progress bar, so you can see what’s happening. If you’re looking for raw speed, try data.table::fread(). It doesn’t fit quite so well into the tidyverse, but it can be quite a bit faster. They produce tibbles, they don’t convert character vectors to factors, use row names, or munge the column names. These are common sources of frustration with the base R functions. They are more reproducible. Base R functions inherit some behaviour from your operating system and environment variables, so import code that works on your computer might not work on someone else’s. 10.2.2 Exercises What function would you use to read a file where fields were separated with “|”? Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common? What are the most important arguments to read_fwf()? Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like &quot; or '. By convention, read_csv() assumes that the quoting character will be &quot;, and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame? &quot;x,y\\n1,&#39;a,b&#39;&quot; Identify what is wrong with each of the following inline CSV files. What happens when you run the code? read_csv(&quot;a,b\\n1,2,3\\n4,5,6&quot;) read_csv(&quot;a,b,c\\n1,2\\n1,2,3,4&quot;) read_csv(&quot;a,b\\n\\&quot;1&quot;) read_csv(&quot;a,b\\n1,2\\na,b&quot;) read_csv(&quot;a;b\\n1;3&quot;) 10.3 Parsing a vector Before we get into the details of how readr reads files from disk, we need to take a little detour to talk about the parse_*() functions. These functions take a character vector and return a more specialised vector like a logical, integer, or date: str(parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;NA&quot;))) #&gt; logi [1:3] TRUE FALSE NA str(parse_integer(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) #&gt; int [1:3] 1 2 3 str(parse_date(c(&quot;2010-01-01&quot;, &quot;1979-10-14&quot;))) #&gt; Date[1:2], format: &quot;2010-01-01&quot; &quot;1979-10-14&quot; These functions are useful in their own right, but are also an important building block for readr. Once you’ve learned how the individual parsers work in this section, we’ll circle back and see how they fit together to parse a complete file in the next section. Like all functions in the tidyverse, the parse_*() functions are uniform: the first argument is a character vector to parse, and the na argument specifies which strings should be treated as missing: parse_integer(c(&quot;1&quot;, &quot;231&quot;, &quot;.&quot;, &quot;456&quot;), na = &quot;.&quot;) #&gt; [1] 1 231 NA 456 If parsing fails, you’ll get a warning: x &lt;- parse_integer(c(&quot;123&quot;, &quot;345&quot;, &quot;abc&quot;, &quot;123.45&quot;)) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual #&gt; 3 -- an integer abc #&gt; 4 -- no trailing characters .45 And the failures will be missing in the output: x #&gt; [1] 123 345 NA NA #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 If there are many parsing failures, you’ll need to use problems() to get the complete set. This returns a tibble, which you can then manipulate with dplyr. problems(x) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 Using parsers is mostly a matter of understanding what’s available and how they deal with different types of input. There are eight particularly important parsers: parse_logical() and parse_integer() parse logicals and integers respectively. There’s basically nothing that can go wrong with these parsers so I won’t describe them here further. parse_double() is a strict numeric parser, and parse_number() is a flexible numeric parser. These are more complicated than you might expect because different parts of the world write numbers in different ways. parse_character() seems so simple that it shouldn’t be necessary. But one complication makes it quite important: character encodings. parse_factor() create factors, the data structure that R uses to represent categorical variables with fixed and known values. parse_datetime(), parse_date(), and parse_time() allow you to parse various date &amp; time specifications. These are the most complicated because there are so many different ways of writing dates. The following sections describe these parsers in more detail. 10.3.1 Numbers It seems like it should be straightforward to parse a number, but three problems make it tricky: People write numbers differently in different parts of the world. For example, some countries use . in between the integer and fractional parts of a real number, while others use ,. Numbers are often surrounded by other characters that provide some context, like “$1000” or “10%”. Numbers often contain “grouping” characters to make them easier to read, like “1,000,000”, and these grouping characters vary around the world. To address the first problem, readr has the notion of a “locale”, an object that specifies parsing options that differ from place to place. When parsing numbers, the most important option is the character you use for the decimal mark. You can override the default value of . by creating a new locale and setting the decimal_mark argument: parse_double(&quot;1.23&quot;) #&gt; [1] 1.23 parse_double(&quot;1,23&quot;, locale = locale(decimal_mark = &quot;,&quot;)) #&gt; [1] 1.23 readr’s default locale is US-centric, because generally R is US-centric (i.e. the documentation of base R is written in American English). An alternative approach would be to try and guess the defaults from your operating system. This is hard to do well, and, more importantly, makes your code fragile: even if it works on your computer, it might fail when you email it to a colleague in another country. parse_number() addresses the second problem: it ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text. parse_number(&quot;$100&quot;) #&gt; [1] 100 parse_number(&quot;20%&quot;) #&gt; [1] 20 parse_number(&quot;It cost $123.45&quot;) #&gt; [1] 123 The final problem is addressed by the combination of parse_number() and the locale as parse_number() will ignore the “grouping mark”: # Used in America parse_number(&quot;$123,456,789&quot;) #&gt; [1] 1.23e+08 # Used in many parts of Europe parse_number(&quot;123.456.789&quot;, locale = locale(grouping_mark = &quot;.&quot;)) #&gt; [1] 1.23e+08 # Used in Switzerland parse_number(&quot;123&#39;456&#39;789&quot;, locale = locale(grouping_mark = &quot;&#39;&quot;)) #&gt; [1] 1.23e+08 10.3.2 Strings It seems like parse_character() should be really simple — it could just return its input. Unfortunately life isn’t so simple, as there are multiple ways to represent the same string. To understand what’s going on, we need to dive into the details of how computers represent strings. In R, we can get at the underlying representation of a string using charToRaw(): charToRaw(&quot;Hadley&quot;) #&gt; [1] 48 61 64 6c 65 79 Each hexadecimal number represents a byte of information: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case the encoding is called ASCII. ASCII does a great job of representing English characters, because it’s the American Standard Code for Information Interchange. Things get more complicated for languages other than English. In the early days of computing there were many competing standards for encoding non-English characters, and to correctly interpret a string you needed to know both the values and the encoding. For example, two common encodings are Latin1 (aka ISO-8859-1, used for Western European languages) and Latin2 (aka ISO-8859-2, used for Eastern European languages). In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today, as well as many extra symbols (like emoji!). readr uses UTF-8 everywhere: it assumes your data is UTF-8 encoded when you read it, and always uses it when writing. This is a good default, but will fail for data produced by older systems that don’t understand UTF-8. If this happens to you, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times you’ll get complete gibberish. For example: x1 &lt;- &quot;El Ni\\xf1o was particularly bad this year&quot; x2 &lt;- &quot;\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd&quot; x1 #&gt; [1] &quot;El Ni\\xf1o was particularly bad this year&quot; x2 #&gt; [1] &quot;\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd&quot; To fix the problem you need to specify the encoding in parse_character(): parse_character(x1, locale = locale(encoding = &quot;Latin1&quot;)) #&gt; [1] &quot;El Niño was particularly bad this year&quot; parse_character(x2, locale = locale(encoding = &quot;Shift-JIS&quot;)) #&gt; [1] &quot;こんにちは&quot; How do you find the correct encoding? If you’re lucky, it’ll be included somewhere in the data documentation. Unfortunately, that’s rarely the case, so readr provides guess_encoding() to help you figure it out. It’s not foolproof, and it works better when you have lots of text (unlike here), but it’s a reasonable place to start. Expect to try a few different encodings before you find the right one. guess_encoding(charToRaw(x1)) #&gt; # A tibble: 2 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ISO-8859-1 0.46 #&gt; 2 ISO-8859-9 0.23 guess_encoding(charToRaw(x2)) #&gt; # A tibble: 1 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 KOI8-R 0.42 The first argument to guess_encoding() can either be a path to a file, or, as in this case, a raw vector (useful if the strings are already in R). Encodings are a rich and complex topic, and I’ve only scratched the surface here. If you’d like to learn more I’d recommend reading the detailed explanation at http://kunststube.net/encoding/. 10.3.3 Factors R uses factors to represent categorical variables that have a known set of possible values. Give parse_factor() a vector of known levels to generate a warning whenever an unexpected value is present: fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;) parse_factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;bananana&quot;), levels = fruit) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 3 -- value in level set bananana #&gt; [1] apple banana &lt;NA&gt; #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 1 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA value in level set bananana #&gt; Levels: apple banana But if you have many problematic entries, it’s often easier to leave as character vectors and then use the tools you’ll learn about in strings and factors to clean them up. 10.3.4 Dates, date-times, and times You pick between three parsers depending on whether you want a date (the number of days since 1970-01-01), a date-time (the number of seconds since midnight 1970-01-01), or a time (the number of seconds since midnight). When called without any additional arguments: parse_datetime() expects an ISO8601 date-time. ISO8601 is an international standard in which the components of a date are organised from biggest to smallest: year, month, day, hour, minute, second. parse_datetime(&quot;2010-10-01T2010&quot;) #&gt; [1] &quot;2010-10-01 20:10:00 UTC&quot; # If time is omitted, it will be set to midnight parse_datetime(&quot;20101010&quot;) #&gt; [1] &quot;2010-10-10 UTC&quot; This is the most important date/time standard, and if you work with dates and times frequently, I recommend reading https://en.wikipedia.org/wiki/ISO_8601 parse_date() expects a four digit year, a - or /, the month, a - or /, then the day: parse_date(&quot;2010-10-01&quot;) #&gt; [1] &quot;2010-10-01&quot; parse_time() expects the hour, :, minutes, optionally : and seconds, and an optional am/pm specifier: library(hms) parse_time(&quot;01:10 am&quot;) #&gt; 01:10:00 parse_time(&quot;20:10:01&quot;) #&gt; 20:10:01 Base R doesn’t have a great built in class for time data, so we use the one provided in the hms package. If these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces: Year %Y (4 digits). %y (2 digits); 00-69 -&gt; 2000-2069, 70-99 -&gt; 1970-1999. Month %m (2 digits). %b (abbreviated name, like “Jan”). %B (full name, “January”). Day %d (2 digits). %e (optional leading space). Time %H 0-23 hour. %I 0-12, must be used with %p. %p AM/PM indicator. %M minutes. %S integer seconds. %OS real seconds. %Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this [time zones]. %z (as offset from UTC, e.g. +0800). Non-digits %. skips one non-digit character. %* skips any number of non-digits. The best way to figure out the correct format is to create a few examples in a character vector, and test with one of the parsing functions. For example: parse_date(&quot;01/02/15&quot;, &quot;%m/%d/%y&quot;) #&gt; [1] &quot;2015-01-02&quot; parse_date(&quot;01/02/15&quot;, &quot;%d/%m/%y&quot;) #&gt; [1] &quot;2015-02-01&quot; parse_date(&quot;01/02/15&quot;, &quot;%y/%m/%d&quot;) #&gt; [1] &quot;2001-02-15&quot; If you’re using %b or %B with non-English month names, you’ll need to set the lang argument to locale(). See the list of built-in languages in date_names_langs(), or if your language is not already included, create your own with date_names(). parse_date(&quot;1 janvier 2015&quot;, &quot;%d %B %Y&quot;, locale = locale(&quot;fr&quot;)) #&gt; [1] &quot;2015-01-01&quot; 10.3.5 Exercises What are the most important arguments to locale()? What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”? I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful. If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly. What’s the difference between read_csv() and read_csv2()? What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. Generate the correct format string to parse each of the following dates and times: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 t1 &lt;- &quot;1705&quot; t2 &lt;- &quot;11:15:10.12 PM&quot; 10.4 Parsing a file Now that you’ve learned how to parse an individual vector, it’s time to return to the beginning and explore how readr parses a file. There are two new things that you’ll learn about in this section: How readr automatically guesses the type of each column. How to override the default specification. 10.4.1 Strategy readr uses a heuristic to figure out the type of each column: it reads the first 1000 rows and uses some (moderately conservative) heuristics to figure out the type of each column. You can emulate this process with a character vector using guess_parser(), which returns readr’s best guess, and parse_guess() which uses that guess to parse the column: guess_parser(&quot;2010-10-01&quot;) #&gt; [1] &quot;date&quot; guess_parser(&quot;15:01&quot;) #&gt; [1] &quot;time&quot; guess_parser(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) #&gt; [1] &quot;logical&quot; guess_parser(c(&quot;1&quot;, &quot;5&quot;, &quot;9&quot;)) #&gt; [1] &quot;double&quot; guess_parser(c(&quot;12,352,561&quot;)) #&gt; [1] &quot;number&quot; str(parse_guess(&quot;2010-10-10&quot;)) #&gt; Date[1:1], format: &quot;2010-10-10&quot; The heuristic tries each of the following types, stopping when it finds a match: logical: contains only “F”, “T”, “FALSE”, or “TRUE”. integer: contains only numeric characters (and -). double: contains only valid doubles (including numbers like 4.5e-5). number: contains valid doubles with the grouping mark inside. time: matches the default time_format. date: matches the default date_format. date-time: any ISO8601 date. If none of these rules apply, then the column will stay as a vector of strings. 10.4.2 Problems These defaults don’t always work for larger files. There are two basic problems: The first thousand rows might be a special case, and readr guesses a type that is not sufficiently general. For example, you might have a column of doubles that only contains integers in the first 1000 rows. The column might contain a lot of missing values. If the first 1000 rows contain only NAs, readr will guess that it’s a character vector, whereas you probably want to parse it as something more specific. readr contains a challenging CSV that illustrates both of these problems: challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) #&gt; Warning: 1000 parsing failures. #&gt; row col expected actual file #&gt; 1001 y 1/0/T/F/TRUE/FALSE 2015-01-16 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1002 y 1/0/T/F/TRUE/FALSE 2018-05-18 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1003 y 1/0/T/F/TRUE/FALSE 2015-09-05 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1004 y 1/0/T/F/TRUE/FALSE 2012-11-28 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1005 y 1/0/T/F/TRUE/FALSE 2020-01-13 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; .... ... .................. .......... .................................................... #&gt; See problems(...) for more details. (Note the use of readr_example() which finds the path to one of the files included with the package) There are two printed outputs: the column specification generated by looking at the first 1000 rows, and the first five parsing failures. It’s always a good idea to explicitly pull out the problems(), so you can explore them in more depth: problems(challenge) #&gt; # A tibble: 1,000 x 5 #&gt; row col expected actual file #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1001 y 1/0/T/F/TRUE/F… 2015-01-… &#39;/home/travis/R/Library/readr/extd… #&gt; 2 1002 y 1/0/T/F/TRUE/F… 2018-05-… &#39;/home/travis/R/Library/readr/extd… #&gt; 3 1003 y 1/0/T/F/TRUE/F… 2015-09-… &#39;/home/travis/R/Library/readr/extd… #&gt; 4 1004 y 1/0/T/F/TRUE/F… 2012-11-… &#39;/home/travis/R/Library/readr/extd… #&gt; 5 1005 y 1/0/T/F/TRUE/F… 2020-01-… &#39;/home/travis/R/Library/readr/extd… #&gt; 6 1006 y 1/0/T/F/TRUE/F… 2016-04-… &#39;/home/travis/R/Library/readr/extd… #&gt; # … with 994 more rows A good strategy is to work column by column until there are no problems remaining. Here we can see that there are a lot of parsing problems with the x column - there are trailing characters after the integer value. That suggests we need to use a double parser instead. To fix the call, start by copying and pasting the column specification into your original call: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_integer(), y = col_character() ) ) Then you can tweak the type of the x column: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_character() ) ) That fixes the first problem, but if we look at the last few rows, you’ll see that they’re dates stored in a character vector: tail(challenge) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.805 2019-11-21 #&gt; 2 0.164 2018-03-29 #&gt; 3 0.472 2014-08-04 #&gt; 4 0.718 2015-08-16 #&gt; 5 0.270 2020-02-04 #&gt; 6 0.608 2019-01-06 You can fix that by specifying that y is a date column: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_date() ) ) tail(challenge) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 0.805 2019-11-21 #&gt; 2 0.164 2018-03-29 #&gt; 3 0.472 2014-08-04 #&gt; 4 0.718 2015-08-16 #&gt; 5 0.270 2020-02-04 #&gt; 6 0.608 2019-01-06 Every parse_xyz() function has a corresponding col_xyz() function. You use parse_xyz() when the data is in a character vector in R already; you use col_xyz() when you want to tell readr how to load the data. I highly recommend always supplying col_types, building up from the print-out provided by readr. This ensures that you have a consistent and reproducible data import script. If you rely on the default guesses and your data changes, readr will continue to read it in. If you want to be really strict, use stop_for_problems(): that will throw an error and stop your script if there are any parsing problems. 10.4.3 Other strategies There are a few other general strategies to help you parse files: In the previous example, we just got unlucky: if we look at just one more row than the default, we can correctly parse in one shot: challenge2 &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), guess_max = 1001) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_date(format = &quot;&quot;) #&gt; ) challenge2 #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors: challenge2 &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types = cols(.default = col_character()) ) This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame. df &lt;- tribble( ~x, ~y, &quot;1&quot;, &quot;1.21&quot;, &quot;2&quot;, &quot;2.32&quot;, &quot;3&quot;, &quot;4.56&quot; ) df #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 1.21 #&gt; 2 2 2.32 #&gt; 3 3 4.56 # Note the column types type_convert(df) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_double() #&gt; ) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.21 #&gt; 2 2 2.32 #&gt; 3 3 4.56 If you’re reading a very large file, you might want to set n_max to a smallish number like 10,000 or 100,000. That will accelerate your iterations while you eliminate common problems. If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats. 10.5 Writing to a file readr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). Both functions increase the chances of the output file being read back in correctly by: Always encoding strings in UTF-8. Saving dates and date-times in ISO8601 format so they are easily parsed elsewhere. If you want to export a csv file to Excel, use write_excel_csv() — this writes a special character (a “byte order mark”) at the start of the file which tells Excel that you’re using the UTF-8 encoding. The most important arguments are x (the data frame to save), and path (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file. write_csv(challenge, &quot;challenge.csv&quot;) Note that the type information is lost when you save to csv: challenge #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows write_csv(challenge, &quot;challenge-2.csv&quot;) read_csv(&quot;challenge-2.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows This makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two alternatives: write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS: write_rds(challenge, &quot;challenge.rds&quot;) read_rds(&quot;challenge.rds&quot;) #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows The feather package implements a fast binary file format that can be shared across programming languages: library(feather) write_feather(challenge, &quot;challenge.feather&quot;) read_feather(&quot;challenge.feather&quot;) # &gt; # A tibble: 2,000 x 2 # &gt; x y # &gt; &lt;dbl&gt; &lt;date&gt; # &gt; 1 404 &lt;NA&gt; # &gt; 2 4172 &lt;NA&gt; # &gt; 3 3004 &lt;NA&gt; # &gt; 4 787 &lt;NA&gt; # &gt; 5 37 &lt;NA&gt; # &gt; 6 2332 &lt;NA&gt; # &gt; # ... with 1,994 more rows Feather tends to be faster than RDS and is usable outside of R. RDS supports list-columns (which you’ll learn about in [many models]); feather currently does not. 10.6 Other types of data To get other types of data into R, we recommend starting with the tidyverse packages listed below. They’re certainly not perfect, but they are a good place to start. For rectangular data: haven reads SPSS, Stata, and SAS files. readxl reads excel files (both .xls and .xlsx). DBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame. For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. Jenny Bryan has some excellent worked examples at https://jennybc.github.io/purrr-tutorial/. For other file types, try the R data import/export manual and the rio package. "],
["introduction-3.html", "11 Introduction 11.1 What you will learn 11.2 How this book is organised 11.3 What you won’t learn 11.4 Prerequisites 11.5 Running R code 11.6 Getting help and learning more 11.7 Acknowledgements 11.8 Colophon", " 11 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of R. 11.1 What you will learn Data science is a huge field, and there’s no way you can master it by reading a single book. The goal of this book is to give you a solid foundation in the most important tools. Our model of the tools needed in a typical data science project looks something like this: First you must import your data into R. This typically means that you take data stored in a file, database, or web API, and load it into a data frame in R. If you can’t get your data into R, you can’t do data science on it! Once you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing velocity from speed and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight! Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times. Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease. You’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book we’ll point you to resources where you can learn more. 11.2 How this book is organised The previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them: Starting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth it. Some topics are best explained with other tools. For example, we believe that it’s easier to understand how models work if you already know about visualisation, tidy data, and programming. Programming tools are not necessarily interesting in their own right, but do allow you to tackle considerably more challenging problems. We’ll give you a selection of programming tools in the middle of the book, and then you’ll see how they can combine with the data science tools to tackle interesting modelling problems. Within each chapter, we try and stick to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. While it’s tempting to skip the exercises, there’s no better way to learn than practicing on real problems. 11.3 What you won’t learn There are some important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic. 11.3.1 Big data This book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you’re working with large data, the performance payoff is worth the extra effort required to learn it. If your data is bigger than this, carefully consider if your big data problem might actually be a small data problem in disguise. While the complete data might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration. Another possibility is that your big data problem is actually a large number of small data problems. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. That would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer the question for a single subset using the tools described in this book, you learn new tools like sparklyr, rhipe, and ddr to solve it for the full dataset. 11.3.2 Python, Julia, and friends In this book, you won’t learn anything about Python, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing. We think R is a great place to start your data science journey because it is an environment designed from the ground up to support data science. R is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, R is a much more flexible language than many of its peers. This flexibility comes with its downsides, but the big upside is how easy it is to evolve tailored grammars for specific parts of the data science process. These mini languages help you think about problems as a data scientist, while supporting fluent interaction between your brain and the computer. 11.3.3 Non-rectangular data This book focuses exclusively on rectangular data: collections of values that are each associated with a variable and an observation. There are lots of datasets that do not naturally fit in this paradigm: including images, sounds, trees, and text. But rectangular data frames are extremely common in science and industry, and we believe that they are a great place to start your data science journey. 11.3.4 Hypothesis confirmation It’s possible to divide data analysis into two camps: hypothesis generation and hypothesis confirmation (sometimes called confirmatory analysis). The focus of this book is unabashedly on hypothesis generation, or data exploration. Here you’ll look deeply at the data and, in combination with your subject knowledge, generate many interesting hypotheses to help explain why the data behaves the way it does. You evaluate the hypotheses informally, using your scepticism to challenge the data in multiple ways. The complement of hypothesis generation is hypothesis confirmation. Hypothesis confirmation is hard for two reasons: You need a precise mathematical model in order to generate falsifiable predictions. This often requires considerable statistical sophistication. You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you’re back to doing exploratory analysis. This means to do hypothesis confirmation you need to “preregister” (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. We’ll talk a little about some strategies you can use to make this easier in modelling. It’s common to think about modelling as a tool for hypothesis confirmation, and visualisation as a tool for hypothesis generation. But that’s a false dichotomy: models are often used for exploration, and with a little care you can use visualisation for confirmation. The key difference is how often do you look at each observation: if you look only once, it’s confirmation; if you look more than once, it’s exploration. 11.4 Prerequisites We’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. If you’ve never programmed before, you might find Hands on Programming with R by Garrett to be a useful adjunct to this book. There are four things you need to run the code in this book: R, RStudio, a collection of R packages called the tidyverse, and a handful of other packages. Packages are the fundamental units of reproducible R code. They include reusable functions, the documentation that describes how to use them, and sample data. 11.4.1 R To download R, go to CRAN, the comprehensive R archive network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to reinstall all your packages, but putting it off only makes it worse. 11.4.2 RStudio RStudio is an integrated development environment, or IDE, for R programming. Download and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features. For this book, make sure you have RStudio 1.0.0. When you start RStudio, you’ll see two key regions in the interface: For now, all you need to know is that you type R code in the console pane, and press enter to run it. You’ll learn more as we go along! 11.4.3 The tidyverse You’ll also need to install some R packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this book are part of the so-called tidyverse. The packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally. You can install the complete tidyverse with a single line of code: install.packages(&quot;tidyverse&quot;) On your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy. You will not be able to use the functions, objects, and help files in a package until you load it with library(). Once you have installed a package, you can load it with the library() function: library(tidyverse) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() This tells you that tidyverse is loading the ggplot2, tibble, tidyr, readr, purrr, and dplyr packages. These are considered to be the core of the tidyverse because you’ll use them in almost every analysis. Packages in the tidyverse change fairly frequently. You can see if updates are available, and optionally install them, by running tidyverse_update(). 11.4.4 Other packages There are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesn’t make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages. As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data. In this book we’ll use three data packages from outside the tidyverse: install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) These packages provide data on airline flights, world development, and baseball that we’ll use to illustrate key data science ideas. 11.5 Running R code The previous section showed you a couple of examples of running R code. Code in the book looks like this: 1 + 2 #&gt; [1] 3 # &gt; [1] 3 If you run the same code in your local console, it will look like this: &gt; 1 + 2 [1] 3 There are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, output is commented out with #&gt;; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the console. Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like flights or x. If we want to make it clear what package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate(), or nycflights13::flights. This is also valid R code. 11.6 Getting help and learning more This book is not an island; there is no single resource that will allow you to master R. As you start to apply the techniques described in this book to your own data you will soon find questions that I do not answer. This section describes a few tips on how to get help, and to help you keep learning. If you get stuck, start with Google. Typically adding “R” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any R-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = &quot;en&quot;) and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer, including [R] to restrict your search to questions and answers that use R. If you don’t find anything useful, prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are three things you need to include to make your example reproducible: required packages, data, and code. Packages should be loaded at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed the package. For packages in the tidyverse, the easiest way to check is to run tidyverse_update(). The easiest way to include data in a question is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I’d perform the following steps: Run dput(mtcars) in R Copy the output In my reproducible script, type mtcars &lt;- then paste. Try and find the smallest subset of your data that still reveals the problem. Spend a little bit of time ensuring that your code is easy for others to read: Make sure you’ve used spaces and your variable names are concise, yet informative. Use comments to indicate where your problem lies. Do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand, and the easier it is to fix. Finish by checking that you have actually made a reproducible example by starting a fresh R session and copying and pasting your script in. You should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning R each day will pay off handsomely in the long run. One way is to follow what Hadley, Garrett, and everyone else at RStudio are doing on the RStudio blog. This is where we post announcements about new packages, new IDE features, and in-person courses. You might also want to follow Hadley (@hadleywickham) or Garrett (@statgarrett) on Twitter, or follow @rstudiotips to keep up with new features in the IDE. To keep up with the R community more broadly, we recommend reading http://www.r-bloggers.com: it aggregates over 500 blogs about R from around the world. If you’re an active Twitter user, follow the #rstats hashtag. Twitter is one of the key tools that Hadley uses to keep up with new developments in the community. 11.7 Acknowledgements This book isn’t just the product of Hadley and Garrett, but is the result of many conversations (in person and online) that we’ve had with the many people in the R community. There are a few people we’d like to thank in particular, because they have spent many hours answering our dumb questions and helping us to better think about data science: Jenny Bryan and Lionel Henry for many helpful discussions around working with lists and list-columns. The three chapters on workflow were adapted (with permission), from http://stat545.com/block002_hello-r-workspace-wd-project.html by Jenny Bryan. Genevera Allen for discussions about models, modelling, the statistical learning perspective, and the difference between hypothesis generation and hypothesis confirmation. Yihui Xie for his work on the bookdown package, and for tirelessly responding to my feature requests. Bill Behrman for his thoughtful reading of the entire book, and for trying it out with his data science class at Stanford. The #rstats twitter community who reviewed all of the draft chapters and provided tons of useful feedback. Tal Galili for augmenting his dendextend package to support a section on clustering that did not make it into the final draft. This book was written in the open, and many people contributed pull requests to fix minor problems. Special thanks goes to everyone who contributed via GitHub: Thanks go to all contributers in alphabetical order: adi pradhan, Ahmed ElGabbas, Ajay Deonarine, @Alex, Andrew Landgraf, bahadir cankardes, @batpigandme, @behrman, Ben Marwick, Bill Behrman, Brandon Greenwell, Brett Klamer, Christian G. Warden, Christian Mongeau, Colin Gillespie, Cooper Morris, Curtis Alexander, Daniel Gromer, David Clark, Derwin McGeary, Devin Pastoor, Dylan Cashman, Earl Brown, Eric Watt, Etienne B. Racine, Flemming Villalona, Gregory Jefferis, @harrismcgehee, Hengni Cai, Ian Lyttle, Ian Sealy, Jakub Nowosad, Jennifer (Jenny) Bryan, @jennybc, Jeroen Janssens, Jim Hester, @jjchern, Joanne Jang, John Sears, Jon Calder, Jonathan Page, @jonathanflint, Jose Roberto Ayala Solares, Julia Stewart Lowndes, Julian During, Justinas Petuchovas, Kara Woo, @kdpsingh, Kenny Darrell, Kirill Sevastyanenko, @koalabearski, @KyleHumphrey, Lawrence Wu, Matthew Sedaghatfar, Mine Cetinkaya-Rundel, @MJMarshall, Mustafa Ascha, @nate-d-olson, Nelson Areal, Nick Clark, @nickelas, Nirmal Patel, @nwaff, @OaCantona, Patrick Kennedy, @Paul, Peter Hurford, Rademeyer Vermaak, Radu Grosu, @rlzijdeman, Robert Schuessler, @robinlovelace, @robinsones, S’busiso Mkhondwane, @seamus-mckinsey, @seanpwilliams, Shannon Ellis, @shoili, @sibusiso16, @spirgel, Steve Mortimer, @svenski, Terence Teo, Thomas Klebel, TJ Mahr, Tom Prior, Will Beasley, @yahwes, Yihui Xie, @zeal626. 11.8 Colophon An online version of this book is available at http://r4ds.had.co.nz. It will continue to evolve in between reprints of the physical book. The source of the book is available at https://github.com/hadley/r4ds. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: devtools::session_info(c(&quot;tidyverse&quot;)) #&gt; ─ Session info ────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.6.0 (2017-01-27) #&gt; os Ubuntu 14.04.5 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz UTC #&gt; date 2019-05-14 #&gt; #&gt; ─ Packages ────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; askpass 1.1 2019-01-13 [1] CRAN (R 3.6.0) #&gt; assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) #&gt; backports 1.1.4 2019-04-10 [1] CRAN (R 3.6.0) #&gt; base64enc 0.1-3 2015-07-28 [1] CRAN (R 3.6.0) #&gt; BH 1.69.0-1 2019-01-07 [1] CRAN (R 3.6.0) #&gt; broom 0.5.2 2019-04-07 [1] CRAN (R 3.6.0) #&gt; callr 3.2.0 2019-03-15 [1] CRAN (R 3.6.0) #&gt; cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) #&gt; cli 1.1.0 2019-03-19 [1] CRAN (R 3.6.0) #&gt; clipr 0.6.0 2019-04-15 [1] CRAN (R 3.6.0) #&gt; colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) #&gt; crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) #&gt; curl 3.3 2019-01-10 [1] CRAN (R 3.6.0) #&gt; DBI 1.0.0 2018-05-02 [1] CRAN (R 3.6.0) #&gt; dbplyr 1.4.0 2019-04-23 [1] CRAN (R 3.6.0) #&gt; digest 0.6.18 2018-10-10 [1] CRAN (R 3.6.0) #&gt; dplyr * 0.8.1 2019-05-14 [1] CRAN (R 3.6.0) #&gt; ellipsis 0.1.0 2019-02-19 [1] CRAN (R 3.6.0) #&gt; evaluate 0.13 2019-02-12 [1] CRAN (R 3.6.0) #&gt; fansi 0.4.0 2018-10-05 [1] CRAN (R 3.6.0) #&gt; forcats * 0.4.0 2019-02-17 [1] CRAN (R 3.6.0) #&gt; fs 1.3.1 2019-05-06 [1] CRAN (R 3.6.0) #&gt; generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) #&gt; ggplot2 * 3.1.1 2019-04-07 [1] CRAN (R 3.6.0) #&gt; glue 1.3.1 2019-03-12 [1] CRAN (R 3.6.0) #&gt; gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) #&gt; haven 2.1.0 2019-02-19 [1] CRAN (R 3.6.0) #&gt; highr 0.8 2019-03-20 [1] CRAN (R 3.6.0) #&gt; hms 0.4.2 2018-03-10 [1] CRAN (R 3.6.0) #&gt; htmltools 0.3.6 2017-04-28 [1] CRAN (R 3.6.0) #&gt; httr 1.4.0 2018-12-11 [1] CRAN (R 3.6.0) #&gt; jsonlite 1.6 2018-12-07 [1] CRAN (R 3.6.0) #&gt; knitr 1.22 2019-03-08 [1] CRAN (R 3.6.0) #&gt; labeling 0.3 2014-08-23 [1] CRAN (R 3.6.0) #&gt; lattice 0.20-38 2018-11-04 [3] CRAN (R 3.6.0) #&gt; lazyeval 0.2.2 2019-03-15 [1] CRAN (R 3.6.0) #&gt; lubridate 1.7.4 2018-04-11 [1] CRAN (R 3.6.0) #&gt; magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) #&gt; markdown 0.9 2018-12-07 [1] CRAN (R 3.6.0) #&gt; MASS 7.3-51.4 2019-03-31 [3] CRAN (R 3.6.0) #&gt; Matrix 1.2-17 2019-03-22 [3] CRAN (R 3.6.0) #&gt; mgcv 1.8-28 2019-03-21 [3] CRAN (R 3.6.0) #&gt; mime 0.6 2018-10-05 [1] CRAN (R 3.6.0) #&gt; modelr 0.1.4 2019-02-18 [1] CRAN (R 3.6.0) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) #&gt; nlme 3.1-139 2019-04-09 [3] CRAN (R 3.6.0) #&gt; openssl 1.3 2019-03-22 [1] CRAN (R 3.6.0) #&gt; pillar 1.4.0 2019-05-11 [1] CRAN (R 3.6.0) #&gt; pkgconfig 2.0.2 2018-08-16 [1] CRAN (R 3.6.0) #&gt; plogr 0.2.0 2018-03-25 [1] CRAN (R 3.6.0) #&gt; plyr 1.8.4 2016-06-08 [1] CRAN (R 3.6.0) #&gt; prettyunits 1.0.2 2015-07-13 [1] CRAN (R 3.6.0) #&gt; processx 3.3.1 2019-05-08 [1] CRAN (R 3.6.0) #&gt; progress 1.2.1 2019-05-13 [1] CRAN (R 3.6.0) #&gt; ps 1.3.0 2018-12-21 [1] CRAN (R 3.6.0) #&gt; purrr * 0.3.2 2019-03-15 [1] CRAN (R 3.6.0) #&gt; R6 2.4.0 2019-02-14 [1] CRAN (R 3.6.0) #&gt; RColorBrewer 1.1-2 2014-12-07 [1] CRAN (R 3.6.0) #&gt; Rcpp 1.0.1 2019-03-17 [1] CRAN (R 3.6.0) #&gt; readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) #&gt; readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) #&gt; rematch 1.0.1 2016-04-21 [1] CRAN (R 3.6.0) #&gt; reprex 0.2.1 2018-09-16 [1] CRAN (R 3.6.0) #&gt; reshape2 1.4.3 2017-12-11 [1] CRAN (R 3.6.0) #&gt; rlang 0.3.4 2019-04-07 [1] CRAN (R 3.6.0) #&gt; rmarkdown 1.12 2019-03-14 [1] CRAN (R 3.6.0) #&gt; rstudioapi 0.10 2019-03-19 [1] CRAN (R 3.6.0) #&gt; rvest 0.3.3 2019-04-11 [1] CRAN (R 3.6.0) #&gt; scales 1.0.0 2018-08-09 [1] CRAN (R 3.6.0) #&gt; selectr 0.4-1 2018-04-06 [1] CRAN (R 3.6.0) #&gt; stringi 1.4.3 2019-03-12 [1] CRAN (R 3.6.0) #&gt; stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) #&gt; sys 3.2 2019-04-23 [1] CRAN (R 3.6.0) #&gt; tibble * 2.1.1 2019-03-16 [1] CRAN (R 3.6.0) #&gt; tidyr * 0.8.3 2019-03-01 [1] CRAN (R 3.6.0) #&gt; tidyselect 0.2.5 2018-10-11 [1] CRAN (R 3.6.0) #&gt; tidyverse * 1.2.1 2017-11-14 [1] CRAN (R 3.6.0) #&gt; tinytex 0.13 2019-05-14 [1] CRAN (R 3.6.0) #&gt; utf8 1.1.4 2018-05-24 [1] CRAN (R 3.6.0) #&gt; vctrs 0.1.0 2018-11-29 [1] CRAN (R 3.6.0) #&gt; viridisLite 0.3.0 2018-02-01 [1] CRAN (R 3.6.0) #&gt; whisker 0.3-2 2013-04-28 [1] CRAN (R 3.6.0) #&gt; withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) #&gt; xfun 0.7 2019-05-14 [1] CRAN (R 3.6.0) #&gt; xml2 1.2.0 2018-01-24 [1] CRAN (R 3.6.0) #&gt; yaml 2.2.0 2018-07-25 [1] CRAN (R 3.6.0) #&gt; zeallot 0.1.0 2018-01-28 [1] CRAN (R 3.6.0) #&gt; #&gt; [1] /home/travis/R/Library #&gt; [2] /usr/local/lib/R/site-library #&gt; [3] /home/travis/R-bin/lib/R/library "],
["iteration.html", "12 Iteration 12.1 Introduction 12.2 For loops 12.3 For loop variations 12.4 For loops vs. functionals 12.5 The map functions 12.6 Dealing with failure 12.7 Mapping over multiple arguments 12.8 Walk 12.9 Other patterns of for loops", " 12 Iteration 12.1 Introduction In [functions], we talked about how important it is to reduce duplication in your code by creating functions instead of copying-and-pasting. Reducing code duplication has three main benefits: It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in more places. One tool for reducing duplication is functions, which reduce duplication by identifying repeated patterns of code and extract them out into independent pieces that can be easily reused and updated. Another tool for reducing duplication is iteration, which helps you when you need to do the same thing to multiple inputs: repeating the same operation on different columns, or on different datasets. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors. 12.1.1 Prerequisites Once you’ve mastered the for loops provided by base R, you’ll learn some of the powerful programming tools provided by purrr, one of the tidyverse core packages. library(tidyverse) 12.2 For loops Imagine we have this simple tibble: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) We want to compute the median of each column. You could do with copy-and-paste: median(df$a) #&gt; [1] -0.246 median(df$b) #&gt; [1] -0.287 median(df$c) #&gt; [1] -0.0567 median(df$d) #&gt; [1] 0.144 But that breaks our rule of thumb: never copy and paste more than twice. Instead, we could use a for loop: output &lt;- vector(&quot;double&quot;, ncol(df)) # 1. output for (i in seq_along(df)) { # 2. sequence output[[i]] &lt;- median(df[[i]]) # 3. body } output #&gt; [1] -0.2458 -0.2873 -0.0567 0.1443 Every for loop has three components: The output: output &lt;- vector(&quot;double&quot;, length(x)). Before you start the loop, you must always allocate sufficient space for the output. This is very important for efficiency: if you grow the for loop at each iteration using c() (for example), your for loop will be very slow. A general way of creating an empty vector of given length is the vector() function. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc) and the length of the vector. The sequence: i in seq_along(df). This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”. You might not have seen seq_along() before. It’s a safe version of the familiar 1:length(l), with an important difference: if you have a zero-length vector, seq_along() does the right thing: y &lt;- vector(&quot;double&quot;, 0) seq_along(y) #&gt; integer(0) 1:length(y) #&gt; [1] 1 0 You probably won’t create a zero-length vector deliberately, but it’s easy to create them accidentally. If you use 1:length(x) instead of seq_along(x), you’re likely to get a confusing error message. The body: output[[i]] &lt;- median(df[[i]]). This is the code that does the work. It’s run repeatedly, each time with a different value for i. The first iteration will run output[[1]] &lt;- median(df[[1]]), the second will run output[[2]] &lt;- median(df[[2]]), and so on. That’s all there is to the for loop! Now is a good time to practice creating some basic (and not so basic) for loops using the exercises below. Then we’ll move on some variations of the for loop that help you solve other problems that will crop up in practice. 12.2.1 Exercises Write for loops to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). Think about the output, sequence, and body before you start writing the loop. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors: out &lt;- &quot;&quot; for (x in letters) { out &lt;- stringr::str_c(out, x) } x &lt;- sample(100) sd &lt;- 0 for (i in seq_along(x)) { sd &lt;- sd + (x[i] - mean(x))^2 } sd &lt;- sqrt(sd / (length(x) - 1)) x &lt;- runif(100) out &lt;- vector(&quot;numeric&quot;, length(x)) out[1] &lt;- x[1] for (i in 2:length(x)) { out[i] &lt;- out[i - 1] + x[i] } Combine your function writing and for loop skills: Write a for loop that prints() the lyrics to the children’s song “Alice the camel”. Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure. Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface. It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step: output &lt;- vector(&quot;integer&quot;, 0) for (i in seq_along(x)) { output &lt;- c(output, lengths(x[[i]])) } output How does this affect performance? Design and execute an experiment. 12.3 For loop variations Once you have the basic for loop under your belt, there are some variations that you should be aware of. These variations are important regardless of how you do iteration, so don’t forget about them once you’ve mastered the FP techniques you’ll learn about in the next section. There are four variations on the basic theme of the for loop: Modifying an existing object, instead of creating a new object. Looping over names or values, instead of indices. Handling outputs of unknown length. Handling sequences of unknown length. 12.3.1 Modifying an existing object Sometimes you want to use a for loop to modify an existing object. For example, remember our challenge from [functions]. We wanted to rescale every column in a data frame: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } df$a &lt;- rescale01(df$a) df$b &lt;- rescale01(df$b) df$c &lt;- rescale01(df$c) df$d &lt;- rescale01(df$d) To solve this with a for loop we again think about the three components: Output: we already have the output — it’s the same as the input! Sequence: we can think about a data frame as a list of columns, so we can iterate over each column with seq_along(df). Body: apply rescale01(). This gives us: for (i in seq_along(df)) { df[[i]] &lt;- rescale01(df[[i]]) } Typically you’ll be modifying a list or data frame with this sort of loop, so remember to use [[, not [. You might have spotted that I used [[ in all my for loops: I think it’s better to use [[ even for atomic vectors because it makes it clear that I want to work with a single element. 12.3.2 Looping patterns There are three basic ways to loop over a vector. So far I’ve shown you the most general: looping over the numeric indices with for (i in seq_along(xs)), and extracting the value with x[[i]]. There are two other forms: Loop over the elements: for (x in xs). This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently. Loop over the names: for (nm in names(xs)). This gives you name, which you can use to access the value with x[[nm]]. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so: results &lt;- vector(&quot;list&quot;, length(x)) names(results) &lt;- names(x) Iteration over the numeric indices is the most general form, because given the position you can extract both the name and the value: for (i in seq_along(x)) { name &lt;- names(x)[[i]] value &lt;- x[[i]] } 12.3.3 Unknown output length Sometimes you might not know how long the output will be. For example, imagine you want to simulate some random vectors of random lengths. You might be tempted to solve this problem by progressively growing the vector: means &lt;- c(0, 1, 2) output &lt;- double() for (i in seq_along(means)) { n &lt;- sample(100, 1) output &lt;- c(output, rnorm(n, means[[i]])) } str(output) #&gt; num [1:138] 0.912 0.205 2.584 -0.789 0.588 ... But this is not very efficient because in each iteration, R has to copy all the data from the previous iterations. In technical terms you get “quadratic” (\\(O(n^2)\\)) behaviour which means that a loop with three times as many elements would take nine (\\(3^2\\)) times as long to run. A better solution to save the results in a list, and then combine into a single vector after the loop is done: out &lt;- vector(&quot;list&quot;, length(means)) for (i in seq_along(means)) { n &lt;- sample(100, 1) out[[i]] &lt;- rnorm(n, means[[i]]) } str(out) #&gt; List of 3 #&gt; $ : num [1:76] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ... #&gt; $ : num [1:17] -0.11 1.149 0.614 0.77 1.392 ... #&gt; $ : num [1:41] 1.88 2.46 2.62 1.82 1.88 ... str(unlist(out)) #&gt; num [1:134] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ... Here I’ve used unlist() to flatten a list of vectors into a single vector. A stricter option is to use purrr::flatten_dbl() — it will throw an error if the input isn’t a list of doubles. This pattern occurs in other places too: You might be generating a long string. Instead of paste()ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with paste(output, collapse = &quot;&quot;). You might be generating a big data frame. Instead of sequentially rbind()ing in each iteration, save the output in a list, then use dplyr::bind_rows(output) to combine the output into a single data frame. Watch out for this pattern. Whenever you see it, switch to a more complex result object, and then combine in one step at the end. 12.3.4 Unknown sequence length Sometimes you don’t even know how long the input sequence should run for. This is common when doing simulations. For example, you might want to loop until you get three heads in a row. You can’t do that sort of iteration with the for loop. Instead, you can use a while loop. A while loop is simpler than for loop because it only has two components, a condition and a body: while (condition) { # body } A while loop is also more general than a for loop, because you can rewrite any for loop as a while loop, but you can’t rewrite every while loop as a for loop: for (i in seq_along(x)) { # body } # Equivalent to i &lt;- 1 while (i &lt;= length(x)) { # body i &lt;- i + 1 } Here’s how we could use a while loop to find how many tries it takes to get three heads in a row: flip &lt;- function() sample(c(&quot;T&quot;, &quot;H&quot;), 1) flips &lt;- 0 nheads &lt;- 0 while (nheads &lt; 3) { if (flip() == &quot;H&quot;) { nheads &lt;- nheads + 1 } else { nheads &lt;- 0 } flips &lt;- flips + 1 } flips #&gt; [1] 21 I mention while loops only briefly, because I hardly ever use them. They’re most often used for simulation, which is outside the scope of this book. However, it is good to know they exist so that you’re prepared for problems where the number of iterations is not known in advance. 12.3.5 Exercises Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files &lt;- dir(&quot;data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame. What happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique? Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(iris) would print: show_mean(iris) # &gt; Sepal.Length: 5.84 # &gt; Sepal.Width: 3.06 # &gt; Petal.Length: 3.76 # &gt; Petal.Width: 1.20 (Extra challenge: what function did I use to make sure that the numbers lined up nicely, even though the variable names had different lengths?) What does this code do? How does it work? trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) { factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) } ) for (var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } 12.4 For loops vs. functionals For loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. To see why this is important, consider (again) this simple data frame: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) Imagine you want to compute the mean of every column. You could do that with a for loop: output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[[i]] &lt;- mean(df[[i]]) } output #&gt; [1] -0.326 0.136 0.429 -0.250 You realise that you’re going to want to compute the means of every column pretty frequently, so you extract it out into a function: col_mean &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- mean(df[[i]]) } output } But then you think it’d also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your col_mean() function and replace the mean() with median() and sd(): col_median &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- median(df[[i]]) } output } col_sd &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- sd(df[[i]]) } output } Uh oh! You’ve copied-and-pasted this code twice, so it’s time to think about how to generalise it. Notice that most of this code is for-loop boilerplate and it’s hard to see the one thing (mean(), median(), sd()) that is different between the functions. What would you do if you saw a set of functions like this: f1 &lt;- function(x) abs(x - mean(x))^1 f2 &lt;- function(x) abs(x - mean(x))^2 f3 &lt;- function(x) abs(x - mean(x))^3 Hopefully, you’d notice that there’s a lot of duplication, and extract it out into an additional argument: f &lt;- function(x, i) abs(x - mean(x))^i You’ve reduced the chance of bugs (because you now have 1/3 less code), and made it easy to generalise to new situations. We can do exactly the same thing with col_mean(), col_median() and col_sd() by adding an argument that supplies the function to apply to each column: col_summary &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { out[i] &lt;- fun(df[[i]]) } out } col_summary(df, median) #&gt; [1] -0.5185 0.0278 0.1730 -0.6116 col_summary(df, mean) #&gt; [1] -0.326 0.136 0.429 -0.250 The idea of passing a function to another function is extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. It might take you a while to wrap your head around the idea, but it’s worth the investment. In the rest of the chapter, you’ll learn about and use the purrr package, which provides functions that eliminate the need for many common for loops. The apply family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but purrr is more consistent and thus is easier to learn. The goal of using purrr functions instead of for loops is to allow you break common list manipulation challenges into independent pieces: How can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list. If you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe. This structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code. 12.4.1 Exercises Read the documentation for apply(). In the 2d case, what two for loops does it generalise? Adapt col_summary() so that it only applies to numeric columns You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column. 12.5 The map functions The pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output: map() makes a list. map_lgl() makes a logical vector. map_int() makes an integer vector. map_dbl() makes a double vector. map_chr() makes a character vector. Each function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function. Once you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!). Some people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years). The chief benefits of using functions like map() is not speed, but clarity: they make your code easier to write and to read. We can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use map_dbl(): map_dbl(df, mean) #&gt; a b c d #&gt; -0.326 0.136 0.429 -0.250 map_dbl(df, median) #&gt; a b c d #&gt; -0.5185 0.0278 0.1730 -0.6116 map_dbl(df, sd) #&gt; a b c d #&gt; 0.921 0.485 0.982 1.156 Compared to using a for loop, focus is on the operation being performed (i.e. mean(), median(), sd()), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe: df %&gt;% map_dbl(mean) #&gt; a b c d #&gt; -0.326 0.136 0.429 -0.250 df %&gt;% map_dbl(median) #&gt; a b c d #&gt; -0.5185 0.0278 0.1730 -0.6116 df %&gt;% map_dbl(sd) #&gt; a b c d #&gt; 0.921 0.485 0.982 1.156 There are a few differences between map_*() and col_summary(): All purrr functions are implemented in C. This makes them a little faster at the expense of readability. The second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section. map_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called: map_dbl(df, mean, trim = 0.5) #&gt; a b c d #&gt; -0.5185 0.0278 0.1730 -0.6116 The map functions also preserve names: z &lt;- list(x = 1:3, y = 4:5) map_int(z, length) #&gt; x y #&gt; 3 2 12.5.1 Shortcuts There are a few shortcuts that you can use with .f in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits the up the mtcars dataset in to three pieces (one for each value of cylinder) and fits the same linear model to each piece: models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(function(df) lm(mpg ~ wt, data = df)) The syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula. models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .)) Here I’ve used . as a pronoun: it refers to the current list element (in the same way that i referred to the current index in the for loop). When you’re looking at many models, you might want to extract a summary statistic like the \\(R^2\\). To do that we need to first run summary() and then extract the component called r.squared. We could do that using the shorthand for anonymous functions: models %&gt;% map(summary) %&gt;% map_dbl(~ .$r.squared) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 But extracting named components is a common operation, so purrr provides an even shorter shortcut: you can use a string. models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 You can also use an integer to select elements by position: x &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9)) x %&gt;% map_dbl(2) #&gt; [1] 2 5 8 12.5.2 Base R If you’re familiar with the apply family of functions in base R, you might have noticed some similarities with the purrr functions: lapply() is basically identical to map(), except that map() is consistent with all the other functions in purrr, and you can use the shortcuts for .f. Base sapply() is a wrapper around lapply() that automatically simplifies the output. This is useful for interactive work but is problematic in a function because you never know what sort of output you’ll get: x1 &lt;- list( c(0.27, 0.37, 0.57, 0.91, 0.20), c(0.90, 0.94, 0.66, 0.63, 0.06), c(0.21, 0.18, 0.69, 0.38, 0.77) ) x2 &lt;- list( c(0.50, 0.72, 0.99, 0.38, 0.78), c(0.93, 0.21, 0.65, 0.13, 0.27), c(0.39, 0.01, 0.38, 0.87, 0.34) ) threshold &lt;- function(x, cutoff = 0.8) x[x &gt; cutoff] x1 %&gt;% sapply(threshold) %&gt;% str() #&gt; List of 3 #&gt; $ : num 0.91 #&gt; $ : num [1:2] 0.9 0.94 #&gt; $ : num(0) x2 %&gt;% sapply(threshold) %&gt;% str() #&gt; num [1:3] 0.99 0.93 0.87 vapply() is a safe alternative to sapply() because you supply an additional argument that defines the type. The only problem with vapply() is that it’s a lot of typing: vapply(df, is.numeric, logical(1)) is equivalent to map_lgl(df, is.numeric). One advantage of vapply() over purrr’s map functions is that it can also produce matrices — the map functions only ever produce vectors. I focus on purrr functions here because they have more consistent names and arguments, helpful shortcuts, and in the future will provide easy parallelism and progress bars. 12.5.3 Exercises Write code that uses one of the map functions to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? What does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why? Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function. 12.6 Dealing with failure When you use the map functions to repeat many operations, the chances are much higher that one of those operations will fail. When this happens, you’ll get an error message, and no output. This is annoying: why does one failure prevent you from accessing all the other successes? How do you ensure that one bad apple doesn’t ruin the whole barrel? In this section you’ll learn how to deal this situation with a new function: safely(). safely() is an adverb: it takes a function (a verb) and returns a modified version. In this case, the modified function will never throw an error. Instead, it always returns a list with two elements: result is the original result. If there was an error, this will be NULL. error is an error object. If the operation was successful, this will be NULL. (You might be familiar with the try() function in base R. It’s similar, but because it sometimes returns the original result and it sometimes returns an error object it’s more difficult to work with.) Let’s illustrate this with a simple example: log(): safe_log &lt;- safely(log) str(safe_log(10)) #&gt; List of 2 #&gt; $ result: num 2.3 #&gt; $ error : NULL str(safe_log(&quot;a&quot;)) #&gt; List of 2 #&gt; $ result: NULL #&gt; $ error :List of 2 #&gt; ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; When the function succeeds, the result element contains the result and the error element is NULL. When the function fails, the result element is NULL and the error element contains an error object. safely() is designed to work with map: x &lt;- list(1, 10, &quot;a&quot;) y &lt;- x %&gt;% map(safely(log)) str(y) #&gt; List of 3 #&gt; $ :List of 2 #&gt; ..$ result: num 0 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: num 2.3 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: NULL #&gt; ..$ error :List of 2 #&gt; .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; This would be easier to work with if we had two lists: one of all the errors and one of all the output. That’s easy to get with purrr::transpose(): y &lt;- y %&gt;% transpose() str(y) #&gt; List of 2 #&gt; $ result:List of 3 #&gt; ..$ : num 0 #&gt; ..$ : num 2.3 #&gt; ..$ : NULL #&gt; $ error :List of 3 #&gt; ..$ : NULL #&gt; ..$ : NULL #&gt; ..$ :List of 2 #&gt; .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; It’s up to you how to deal with the errors, but typically you’ll either look at the values of x where y is an error, or work with the values of y that are ok: is_ok &lt;- y$error %&gt;% map_lgl(is_null) x[!is_ok] #&gt; [[1]] #&gt; [1] &quot;a&quot; y$result[is_ok] %&gt;% flatten_dbl() #&gt; [1] 0.0 2.3 Purrr provides two other useful adverbs: Like safely(), possibly() always succeeds. It’s simpler than safely(), because you give it a default value to return when there is an error. x &lt;- list(1, 10, &quot;a&quot;) x %&gt;% map_dbl(possibly(log, NA_real_)) #&gt; [1] 0.0 2.3 NA quietly() performs a similar role to safely(), but instead of capturing errors, it captures printed output, messages, and warnings: x &lt;- list(1, -1) x %&gt;% map(quietly(log)) %&gt;% str() #&gt; List of 2 #&gt; $ :List of 4 #&gt; ..$ result : num 0 #&gt; ..$ output : chr &quot;&quot; #&gt; ..$ warnings: chr(0) #&gt; ..$ messages: chr(0) #&gt; $ :List of 4 #&gt; ..$ result : num NaN #&gt; ..$ output : chr &quot;&quot; #&gt; ..$ warnings: chr &quot;NaNs produced&quot; #&gt; ..$ messages: chr(0) 12.7 Mapping over multiple arguments So far we’ve mapped along a single input. But often you have multiple related inputs that you need iterate along in parallel. That’s the job of the map2() and pmap() functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with map(): mu &lt;- list(5, 10, -3) mu %&gt;% map(rnorm, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 5.63 7.1 4.39 3.37 4.99 #&gt; $ : num [1:5] 9.34 9.33 9.52 11.32 10.64 #&gt; $ : num [1:5] -2.49 -4.75 -2.11 -2.78 -2.42 What if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds: sigma &lt;- list(1, 5, 10) seq_along(mu) %&gt;% map(~ rnorm(5, mu[[.]], sigma[[.]])) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 4.82 5.74 4 2.06 5.72 #&gt; $ : num [1:5] 6.51 0.529 10.381 14.377 12.269 #&gt; $ : num [1:5] -11.51 2.66 8.52 -10.56 -7.89 But that obfuscates the intent of the code. Instead we could use map2() which iterates over two vectors in parallel: map2(mu, sigma, rnorm, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 3.83 4.52 5.12 3.23 3.59 #&gt; $ : num [1:5] 13.55 3.8 8.16 12.31 8.39 #&gt; $ : num [1:5] -15.872 -13.3 12.141 0.469 14.794 map2() generates this series of function calls: Note that the arguments that vary for each call come before the function; arguments that are the same for every call come after. Like map(), map2() is just a wrapper around a for loop: map2 &lt;- function(x, y, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], y[[i]], ...) } out } You could also imagine map3(), map4(), map5(), map6() etc, but that would get tedious quickly. Instead, purrr provides pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples: n &lt;- list(1, 3, 5) args1 &lt;- list(n, mu, sigma) args1 %&gt;% pmap(rnorm) %&gt;% str() #&gt; List of 3 #&gt; $ : num 5.39 #&gt; $ : num [1:3] 5.41 2.08 9.58 #&gt; $ : num [1:5] -23.85 -2.96 -6.56 8.46 -5.21 That looks like: If you don’t name the elements of list, pmap() will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments: args2 &lt;- list(mean = mu, sd = sigma, n = n) args2 %&gt;% pmap(rnorm) %&gt;% str() That generates longer, but safer, calls: Since the arguments are all the same length, it makes sense to store them in a data frame: params &lt;- tribble( ~mean, ~sd, ~n, 5, 1, 1, 10, 5, 3, -3, 10, 5 ) params %&gt;% pmap(rnorm) #&gt; [[1]] #&gt; [1] 6.02 #&gt; #&gt; [[2]] #&gt; [1] 8.68 18.29 6.13 #&gt; #&gt; [[3]] #&gt; [1] -12.24 -5.76 -8.93 -4.22 8.80 As soon as your code gets complicated, I think a data frame is a good approach because it ensures that each column has a name and is the same length as all the other columns. 12.7.1 Invoking different functions There’s one more step up in complexity - as well as varying the arguments to the function you might also vary the function itself: f &lt;- c(&quot;runif&quot;, &quot;rnorm&quot;, &quot;rpois&quot;) param &lt;- list( list(min = -1, max = 1), list(sd = 5), list(lambda = 10) ) To handle this case, you can use invoke_map(): invoke_map(f, param, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 0.479 0.439 -0.471 0.348 -0.581 #&gt; $ : num [1:5] 2.48 3.9 7.54 -9.12 3.94 #&gt; $ : int [1:5] 6 11 5 8 9 The first argument is a list of functions or character vector of function names. The second argument is a list of lists giving the arguments that vary for each function. The subsequent arguments are passed on to every function. And again, you can use tribble() to make creating these matching pairs a little easier: sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sim = invoke_map(f, params, n = 10)) 12.8 Walk Walk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk - the important thing is the action, not the return value. Here’s a very simple example: x &lt;- list(1, &quot;a&quot;, 3) x %&gt;% walk(print) #&gt; [1] 1 #&gt; [1] &quot;a&quot; #&gt; [1] 3 walk() is generally not that useful compared to walk2() or pwalk(). For example, if you had a list of plots and a vector of file names, you could use pwalk() to save each file to the corresponding location on disk: library(ggplot2) plots &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~ ggplot(., aes(mpg, wt)) + geom_point()) paths &lt;- stringr::str_c(names(plots), &quot;.pdf&quot;) pwalk(list(paths, plots), ggsave, path = tempdir()) walk(), walk2() and pwalk() all invisibly return .x, the first argument. This makes them suitable for use in the middle of pipelines. 12.9 Other patterns of for loops Purrr provides a number of other functions that abstract over other types of for loops. You’ll use them less frequently than the map functions, but they’re useful to know about. The goal here is to briefly illustrate each function, so hopefully it will come to mind if you see a similar problem in the future. Then you can go look up the documentation for more details. 12.9.1 Predicate functions A number of functions work with predicate functions that return either a single TRUE or FALSE. keep() and discard() keep elements of the input where the predicate is TRUE or FALSE respectively: iris %&gt;% keep(is.factor) %&gt;% str() #&gt; &#39;data.frame&#39;: 150 obs. of 1 variable: #&gt; $ Species: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris %&gt;% discard(is.factor) %&gt;% str() #&gt; &#39;data.frame&#39;: 150 obs. of 4 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... some() and every() determine if the predicate is true for any or for all of the elements. x &lt;- list(1:5, letters, list(10)) x %&gt;% some(is_character) #&gt; [1] TRUE x %&gt;% every(is_vector) #&gt; [1] TRUE detect() finds the first element where the predicate is true; detect_index() returns its position. x &lt;- sample(10) x #&gt; [1] 10 6 1 3 2 4 5 8 9 7 x %&gt;% detect(~ . &gt; 5) #&gt; [1] 10 x %&gt;% detect_index(~ . &gt; 5) #&gt; [1] 1 head_while() and tail_while() take elements from the start or end of a vector while a predicate is true: x %&gt;% head_while(~ . &gt; 5) #&gt; [1] 10 6 x %&gt;% tail_while(~ . &gt; 5) #&gt; [1] 8 9 7 12.9.2 Reduce and accumulate Sometimes you have a complex list that you want to reduce to a simple list by repeatedly applying a function that reduces a pair to a singleton. This is useful if you want to apply a two-table dplyr verb to multiple tables. For example, you might have a list of data frames, and you want to reduce to a single data frame by joining the elements together: dfs &lt;- list( age = tibble(name = &quot;John&quot;, age = 30), sex = tibble(name = c(&quot;John&quot;, &quot;Mary&quot;), sex = c(&quot;M&quot;, &quot;F&quot;)), trt = tibble(name = &quot;Mary&quot;, treatment = &quot;A&quot;) ) dfs %&gt;% reduce(full_join) #&gt; Joining, by = &quot;name&quot; #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 2 x 4 #&gt; name age sex treatment #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John 30 M &lt;NA&gt; #&gt; 2 Mary NA F A Or maybe you have a list of vectors, and want to find the intersection: vs &lt;- list( c(1, 3, 5, 6, 10), c(1, 2, 3, 7, 8, 10), c(1, 2, 3, 4, 8, 9, 10) ) vs %&gt;% reduce(intersect) #&gt; [1] 1 3 10 The reduce function takes a “binary” function (i.e. a function with two primary inputs), and applies it repeatedly to a list until there is only a single element left. Accumulate is similar but it keeps all the interim results. You could use it to implement a cumulative sum: x &lt;- sample(10) x #&gt; [1] 7 5 10 9 8 3 1 4 2 6 x %&gt;% accumulate(`+`) #&gt; [1] 7 12 22 31 39 42 43 47 49 55 12.9.3 Exercises Implement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t? Create an enhanced col_sum() that applies a summary function to every numeric column in a data frame. A possible base R equivalent of col_sum() is: col_sum3 &lt;- function(df, f) { is_num &lt;- sapply(df, is.numeric) df_num &lt;- df[, is_num] sapply(df_num, f) } But it has a number of bugs as illustrated with the following inputs: df &lt;- tibble( x = 1:3, y = 3:1, z = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) # OK col_sum3(df, mean) # Has problems: don&#39;t always return numeric vector col_sum3(df[1:2], mean) col_sum3(df[1], mean) col_sum3(df[0], mean) What causes the bugs? "],
["conceptos-basicos.html", "13 Conceptos básicos 13.1 Introducción 13.2 Un modelo simple 13.3 Visualizando modelos 13.4 Fórmulas y familias de modelos 13.5 Valores faltantes 13.6 Otras familias de modelos", " 13 Conceptos básicos 13.1 Introducción El objetivo de un modelo es proveer un resumen de baja dimensión de un conjunto de datos (o dataset, en inglés). En el contexto de este libro vamos a usar modelos para particionar los datos en patrones y residuos. Patrones fuertes esconderán tendencias sutiles, por lo que usaremos modelos para remover capas de estructuras mientras exploramos el conjunto de datos. Sin embargo, antes de que podamos comenzar a usar modelos en conjuntos de datos interesantes y reales, necesitarás los conceptos básicos de cómo funcionan los modelos. Por dicha razón, este capítulo es único porque usa solamente datos simulados. Estos conjunto de datos son muy simples y no muy interesantes, pero nos ayudarán a entender la esencia del modelado antes de que apliques las mismas técnicas con datos reales en el próximo capítulo. Hay dos partes en un modelo: Primero, defines una familia de modelos que expresa un patrón que quieras capturar. El patrón debe ser preciso, pero también genérico. Por ejemplo, el patrón podría ser una línea recta, o una curva cuadrática. Expresarás la familia de modelos con una ecuación como y = a_1 * x + a_2 o y = a_1 * x ^ a_2. Aquí, x e y son variables conocidas de tus datos, y a_1 y a_2 son parámetros que pueden variar al capturar diferentes patrones. Luego, generas un modelo ajustado al encontrar un modelo de la familia que sea lo más cercano a tus datos. Esto toma la familia de modelos genérica y la vuelve específica, como y = 3 * x + 7 o y = 9 * x ^ 2. Es importante enteder que el modelo ajustado es solamente el modelo más cercano a la familia de modelos. Esto implica que tu tienes el “mejor” modelo (de acuerdo a cierto criterio); no implica que tu tienes un buen modelo y ciertamente no implica que ese modelo es “verdadero”. George Box lo explica muy bien en su famoso aforismo: Todos los modelos están mal, algunos son útiles. Vale la pena leer el contexto más completo de la cita: Ahora sería muy notable si cualquier sistema existente en el mundo real pudiera ser representado exactamente por algún modelo simple. Sin embargo, modelos simples astutamente escogidos a menudo proporcionan aproximaciones notablemente útiles. Por ejemplo, la ley PV = RT que relaciona la presión P, el volumen V y la temperatura T de un gas “ideal” a través de una constante R no es exactamente verdadera para cualquier gas real, pero frecuentmente provee una aproximación útil y, además, su estructura es informativa ya que proviene de un punto de vista físico del comportamiento de las moléculas de un gas. Para tal modelo, no hay necesidad de preguntarse “¿Es el modelo verdadero?”. Si la “verdad” debe ser la “verdad completa”, la respuesta debe ser “No”. La única pregunta de interés es “¿Es el modelo esclarecedor y útil?”. El objetivo de un modelo no es descubrir la verdad, sino descubrir una aproximación simple que sea útil. 13.1.1 Prerrequisitos En este capítulo usaremos el paquete modelr que encapsula (del inglés wrapper) las funciones de modelado de R base para que funcionen naturalmente en un pipe. library(tidyverse) library(modelr) options(na.action = na.warn) 13.2 Un modelo simple Miremos el conjunto de datos simulado sim1, incluído dentro del paquete modelr. Este contiene dos variables continuas, x e y. Grafiquémoslas para ver como están relacionadas: ggplot(sim1, aes(x, y)) + geom_point() Puedes ver un fuerte patrón en los datos. Usemos un modelo para capturar dicho patrón y hacerlo explícito. Es nuestro trabajo proporcionar la forma básica del modelo. En este caso, la relación parece ser lineal, es decir: y = a_0 + a_1 * x. Comencemos por tener una idea de cómo son los modelos de esa familia generando aleatoriamente unos pocos y superponiéndolos sobre los datos. Para este caso simple, podemos usar geom_abline() que toma una pendiente e intercepto (u ordenada al origen) como parámetros. Más adelante, aprenderemos técnicas más generales que funcionan con cualquier modelo. modelos &lt;- tibble( a1 = runif(250, -20, 40), a2 = runif(250, -5, 5) ) ggplot(sim1, aes(x, y)) + geom_abline(aes(intercept = a1, slope = a2), data = modelos, alpha = 1 / 4) + geom_point() Hay 250 modelos en el gráfico, ¡pero muchos son realmente malos! Necesitamos encontrar los modelos buenos especificando nuestra intuición de que un buen modelo está “cerca” de los datos. Necesitamos una manera de cuantificar la distancia entre los datos y un modelo. Entonces podemos ajustar el modelo encontrando el valor de a_0 ya_1 que genera el modelo con la menor distancia a estos datos. Un lugar fácil para comenzar es encontrar la distancia vertical entre cada punto y el modelo, como lo muestra el siguiente diagrama. (Nota que he cambiado ligeramente los valores x para que puedas ver las distancias individuales.) La distancia es solo la diferencia entre el valor dado por el modelo (la predicción), y el valor real y en los datos (la respuesta). Para calcular esta distancia, primero transformamos nuestra familia de modelos en una función de R. Esta función toma los parámetros del modelo y los datos como inputs, y retorna el valor predicho por el modelo como output: model1 &lt;- function(a, data) { a[1] + data$x * a[2] } model1(c(7, 1.5), sim1) #&gt; [1] 8.5 8.5 8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 #&gt; [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 #&gt; [29] 22.0 22.0 Luego, necesitaremos calcular la distancia entre lo predicho y los valores reales. En otras palabras, el siguiente gráfico muestra 30 distancias: ¿Cómo las colapsamos en un único número? Una forma habitual de hacer esto en estadística es usar la “raíz del error cuadrático medio” (del inglés root-mean-squared deviation). Calculamos la diferencia entre los valores reales y los predichos, los elevamos al cuadrado, luego se promedian y tomamos la raíz cuadrada. Esta distancia cuenta con propiedades matemáticas interesantes, pero no nos referiremos a ellas en este capítulo. ¡Tendrás que creer en mi palabra! measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff^2)) } measure_distance(c(7, 1.5), sim1) #&gt; [1] 2.67 Ahora podemos usar purrr para calcular la distancia de todos los modelos definidos anteriormente. Necesitamos una función auxiliar debido a que nuestra función de distancia espera que el modelo sea un vector numérico de longitud 2. sim1_dist &lt;- function(a1, a2) { measure_distance(c(a1, a2), sim1) } modelos &lt;- modelos %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) modelos #&gt; # A tibble: 250 x 3 #&gt; a1 a2 dist #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -15.2 0.0889 30.8 #&gt; 2 30.1 -0.827 13.2 #&gt; 3 16.0 2.27 13.2 #&gt; 4 -10.6 1.38 18.7 #&gt; 5 -19.6 -1.04 41.8 #&gt; 6 7.98 4.59 19.3 #&gt; # … with 244 more rows A continuación, vamos a superponer los mejores 10 modelos en los datos. He coloreado los modelos usando -dist: esto es una forma fácil de asegurarse de que los mejores modelos (es decir, aquellos con la menor distancia) tengan los colores más brillantes. ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(modelos, rank(dist) &lt;= 10) ) También podemos pensar estos modelos como observaciones y visualizar un diagrama de dispersión (o scatterplot, en inglés) de a1 versus a2, nuevamente coloreado usando -dist. No podremos ver directamente cómo el modelo contrasta con los datos, pero podemos ver muchos modelos a la vez. Nuevamente, he destacado los mejores 10 modelos, esta vez dibujando círculos rojos bajo ellos. ggplot(modelos, aes(a1, a2)) + geom_point(data = filter(modelos, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) En lugar de probar con múltples modelos aleatorios, se puede sistematizar y generar una cuadrícula de puntos igualmente espaciados (esto se llama búsqueda en cuadrícula). He seleccionado los parámetros de la cuadrícula por aproximación, mirando donde se ubican los mejores modelos en el gráfico anterior. grid &lt;- expand.grid( a1 = seq(-5, 20, length = 25), a2 = seq(1, 3, length = 25) ) %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) grid %&gt;% ggplot(aes(a1, a2)) + geom_point(data = filter(grid, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) Cuando superpones los mejores 10 modelos en los datos originales, se ven bastante bien: ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(grid, rank(dist) &lt;= 10) ) Podrás imaginarte que de forma iterativa puedo hacer la cuadrícula más y más fina hasta reducir los resultados al mejor modelo. Existe una forma mejor de resolver el problema: una herramienta de minimización llamada búsqueda de Newton-Raphson. La intuición detrás de Newton-Raphson es bastante simple: tomas un punto de partida y buscas la pendiente más fuerte en torno a ese punto. Puedes bajar por esa pendiente un poco, para luego repetir el proceso varias veces, hasta que no se puede descender más. En R, esto se puede hacer con la función optim(): best &lt;- optim(c(0, 0), measure_distance, data = sim1) best$par #&gt; [1] 4.22 2.05 ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline(intercept = best$par[1], slope = best$par[2]) No te preocupes demasiado acerca de los detalles de cómo funciona optim(). La intuición es lo importante en esta parte. Si tienes una función que define la mínima distancia entre un modelo y un conjunto de datos, un algoritmo que pueda minimizar la distancia modificando los parámetros del modelo te permitirá encontrar el mejor modelo. Lo interesante de este enfoque es que funciona con cualquier familia de modelos respecto de la cual se pueda escribir una ecuación que los describa. Existe otro enfoque que podemos usar para este modelo, debido a que es un caso especial de una familia más amplia: los modelos lineales. Un modelo lineal es de la forma y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n+1). Este modelo simple es equivalente a un modelo lineal generalizado en el que n tiene valor 2 y x_1 es x. R cuenta con una herramienta diseñada especialmente para ajustar modelos lineales llamada lm(). lm() tiene un modo especial de especificar la familia del modelo: las fórmulas. Las fórmulas son similares a y ~ x, que lm() traducirá a una función de la forma y = a_1 + a_2 * x. Podemos ajustar el modelo y mirar la salida: sim1_mod &lt;- lm(y ~ x, data = sim1) coef(sim1_mod) #&gt; (Intercept) x #&gt; 4.22 2.05 ¡Estos son exactamente los mismos valores obtenidos con optim()! Detrás del escenario lm() no usa optim(), sin embargo saca ventaja de la estructura matemática de los modelos lineales. Usando algunas conexiones entre geometría, cálculo y álgebra lineal, lm() encuentra directamente el mejor modelo en un paso, usando un algoritmo sofisticado. Este enfoque es a la vez raṕido y garantiza que existe un mínimo global. 13.2.1 Ejercicios Una desventaja del modelo lineal es ser sensible a valores inusuales debido a que la distancia incorpora un término al cuadrado. Ajusta un modelo a los datos simulados que se presentan a continuación y visualiza los resultados. Corre el modelo varias veces para generar diferentes conjuntos de datos simulados. ¿Qué puedes observar respecto del modelo? sim1a &lt;- tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ) Una forma de obtener un modelo lineal más robusto es usar una métrica distinta para la distancia. Por ejemplo, en lugar de la raíz de la distancia media cuadrática (del inglés root-mean-squared distance) se podría usar la media de la distancia absoluta: measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) mean(abs(diff)) } Usa optim() para ajustar este modelo a los datos simulados anteriormente y compara el resultado con el modelo lineal. Un desafío al realizar optimización numérica es que únicamente garantiza encontrar un óptimo local. ¿Qué problema se presenta al optimizar un modelo de tres parámetros como el que se presenta a continuación? model1 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } 13.3 Visualizando modelos Para modelos simples, como el presentado anteriormente, puedes descubrir el patrón que captura el modelo si inspeccionas cuidadosamente la familia del modelo y los coeficientes ajustados. Si alguna vez tomaste un curso de modelado estadístico, te será habitual gastar mucho tiempo en esa tarea. Aquí, sin embargo, tomaremos otro camino. Vamos a enfocarnos en entender un modelo mirando las predicciones que genera. Esto tiene una gran ventaja: cada tipo de modelo predictivo realiza predicciones (¿qué otra podría realizar?) de modo que podemos usar el mismo conjunto de técnicas para entender cualquier tipo de modelo predictivo. También es útil observar lo que el modelo no captura, los llamados residuos que se obtienen restando las predicciones a los datos. Los residuos son poderosos porque nos permiten usar modelos para quitar patrones fuertes y así observar tendencias sutiles que se mantengan luego de quitar los patrones más evidentes. 13.3.1 Predicciones Para visualizar las predicciones de un modelo, podemos partir por generar una grilla de valores equidistantes que cubra la región donde se encuentran los datos. La forma más fácil de hacerlo es usando modelr::data_grid(). El primer argumento es un data frame y, por cada argumento adicional, encuentra las variables únicas y luego genera todas las combinaciones: grid &lt;- sim1 %&gt;% data_grid(x) grid #&gt; # A tibble: 10 x 1 #&gt; x #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 #&gt; 4 4 #&gt; 5 5 #&gt; 6 6 #&gt; # … with 4 more rows (Esto será más interesante cuando se agreguen más variables al modelo.) Luego agregamos las predicciones. Usaremos modelr::add_predictions() que toma un data frame y un modelo. Esto agrega las predicciones del modelo en una nueva columna en el data frame: grid &lt;- grid %&gt;% add_predictions(sim1_mod) grid #&gt; # A tibble: 10 x 2 #&gt; x pred #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 6.27 #&gt; 2 2 8.32 #&gt; 3 3 10.4 #&gt; 4 4 12.4 #&gt; 5 5 14.5 #&gt; 6 6 16.5 #&gt; # … with 4 more rows (También puedes usar esta función para agregar prediccionees al conjunto de datos original.) A continuación, graficamos las predicciones. Te preguntarás acerca de todo este trabajo adicional en comparación a usar geom_abline(). Pero la ventaja de este enfoque es que funciona con cualquier modelo en R, desde los más simples a los más complejos. La única limitante son tus habilidades de visualización. Para más ideas respecto de como visualizar modelos complejos, puedes consultar http://vita.had.co.nz/papers/model-vis.html. ggplot(sim1, aes(x)) + geom_point(aes(y = y)) + geom_line(aes(y = pred), data = grid, colour = &quot;red&quot;, size = 1) 13.3.2 Residuos La otra cara de las predicciones son los residuos. Las predicciones te informan de los patrones que el modelo captura y los residuos te dicen lo que el modelo ignora. Los residuos son las distancias entre lo observado y los valores predichos que calculamos anteriormente. Agregamos los residuos a los datos con add_residuals(), que funciona de manera similar a add_predictions(). Nota, sin embargo, que usamos el data frame original y no no una grilla manufacturada. Esto es porque para calcular los residuos se necesitan los valores de “y”. sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod) sim1 #&gt; # A tibble: 30 x 3 #&gt; x y resid #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 4.20 -2.07 #&gt; 2 1 7.51 1.24 #&gt; 3 1 2.13 -4.15 #&gt; 4 2 8.99 0.665 #&gt; 5 2 10.2 1.92 #&gt; 6 2 11.3 2.97 #&gt; # … with 24 more rows Existen diferentes formas de entender qué nos dicen los residuos respecto del modelo. Una forma es dibujar un polígono de frecuencia que nos ayude a entender cómo se propagan los residuos: ggplot(sim1, aes(resid)) + geom_freqpoly(binwidth = 0.5) Esto ayuda a calibrar la calidad del modelo: ¿qué tan lejos se encuentran las predicciones de los valores observados? Nota que el promedio del residuo es siempre cero. A menudo vas a querer crear gráficos usando los residuos en lugar del predictor original. Verás mucho de eso en el capítulo siguiente: ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0) + geom_point() Esto parece ser ruido aleatorio, lo que sugiere que el modelo ha hecho un buen trabajo capturando los patrones del conjunto de datos. 13.3.3 Ejercicios En lugar de usar lm() para ajustar una línea recta, puedes usar loess() para ajustar una curva suave. Repite el proceso de ajustar el modelo, generar la cuadrícula, predicciones y visualización con sim1 usando loess() en vez de lm(). ¿Cómo se compara el resultado a geom_smooth(). add_predictions() está pareada con gather_predictions() y spread_predictions(). ¿Cómo difieren estas tres funciones? ¿Qué hace geom_ref_line()? ¿De qué paquete proviene? ¿Por qué es útil e importante incluir una línea de referencia en los gráficos que muestran residuos? ¿Por qué quisieras mirar un polígono de frecuencias con los residuos absolutos? ¿Cuáles son las ventajas y desventajas de los residuos crudos? 13.4 Fórmulas y familias de modelos Ya habrás visto fórmulas anteriormente cuando usamos facet_wrap() y facet_grid(). En R, las fórmulas proveen un modo general de obtener “comportamientos especiales”. En lugar de evaluar los valores de las variables directamente, se capturan los valores para que sean interpretados por una función. La mayoría de los las funciones de modelado en R usan una conversión estándar para las fórmulas y las funciones. Ya habrás visto una conversión simple y ~ x que se convierte en y = a_1 + a_2 * x. Si quieres ver lo que hace R, puedes usar la función model_matrix(). Esta toma un data frame y una fórmula para entregar un tibble que define la ecuación del modelo: cada columna en la salida está asociada con un coeficiente del modelo, la función es siempre y = a_1 * salida_1 + a_2 * salida_2. Para el caso simple y ~ x1 esto nos muestra algo interesante: df &lt;- tribble( ~y, ~x1, ~x2, 4, 2, 5, 5, 1, 6 ) model_matrix(df, y ~ x1) #&gt; # A tibble: 2 x 2 #&gt; `(Intercept)` x1 #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 #&gt; 2 1 1 La forma en que R agrega el intercepto (u ordenada al origen) al modelo es mediante una columna de unos. Por defecto, R siempre agregará esta columna. Si no quieres esto, necesitas excluirla explícitamente usando -1: model_matrix(df, y ~ x1 - 1) #&gt; # A tibble: 2 x 1 #&gt; x1 #&gt; &lt;dbl&gt; #&gt; 1 2 #&gt; 2 1 La matriz del modelo crece de manera nada sorprendente si incluyes más variables al modelo: model_matrix(df, y ~ x1 + x2) #&gt; # A tibble: 2 x 3 #&gt; `(Intercept)` x1 x2 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 5 #&gt; 2 1 1 6 Esta notación para las fórmulas a veces se le llama “notación de Wilkinson-Rogers”, la cual fue descrita inicialmente en Symbolic Description of Factorial Models for Analysis of Variance, escrito por G. N. Wilkinson y C. E. Rogers https://www.jstor.org/stable/2346786. Es conveniente excavar un poco y leer el artículo original si quieres entender los detalles del álgebra de modelado. Las siguientes secciones detallan cómo esta notación de fórmulas funciona con variables categóricas, interacciones y transformaciones. 13.4.1 Variables categóricas Generar una función a partir de una fórmula es directo cuando el predictor es una variable continua, pero las cosas son más complicadas cuando el predictor es una variable categórica. Imagina que tienes una fórmula como y ~ sexo, donde el sexo puede ser hombre o mujer. No tiene sentido convertir a una fórmula del tipo y = x_0 + x_1 * sexo debido a que sexo no es un número - ¡no se puede multiplicar! En su lugar, lo que R hace es convertir a y = x_0 + x_1 * sexo_hombre donde sexo_hombre tiene valor 1 si sexo corresponde a hombre y cero a mujer: df &lt;- tribble( ~sex, ~response, &quot;male&quot;, 1, &quot;female&quot;, 2, &quot;male&quot;, 1 ) model_matrix(df, response ~ sex) #&gt; # A tibble: 3 x 2 #&gt; `(Intercept)` sexmale #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 0 #&gt; 3 1 1 Quizá te preguntes por qué R no crea la columna sexo_mujer. El problema es que eso crearía una columna perfectamente predecible a partir de las otras columnas (es decir, sexo_mujer = 1 - sexo_hombre). Desafortunadamete los detalles exactos de por qué esto es un problema van más allá del alcance del libro, pero básicamente crea una familia de modelos que es muy flexible y genera infinitos modelos igualmente cercanos a los datos. Afortunadamente, sin embargo, si te enfocas en visualizar las predicciones no necesitas preocuparte de la parametrización exacta. Veamos algunos datos y modelos para hacer algo concreto. Aquí está el dataset sim2 de modelr: ggplot(sim2) + geom_point(aes(x, y)) Podemos ajustar un modelo a esto y generar predicciones: mod2 &lt;- lm(y ~ x, data = sim2) grid &lt;- sim2 %&gt;% data_grid(x) %&gt;% add_predictions(mod2) grid #&gt; # A tibble: 4 x 2 #&gt; x pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1.15 #&gt; 2 b 8.12 #&gt; 3 c 6.13 #&gt; 4 d 1.91 Efectivamente, un modelo con una variable x categórica va a predecir el valor medio para cada categoría. (¿Por qué? porque la media minimiza la raíz de la distancia media cuadrática.) Es fácil de ver si superponemos la predicción sobre los datos originales: ggplot(sim2, aes(x)) + geom_point(aes(y = y)) + geom_point(data = grid, aes(y = pred), colour = &quot;red&quot;, size = 4) No es posible hacer predicciones sobre niveles no observados. A veces harás esto por accidente, por lo que es bueno reconocer el siguiente mensaje de error: tibble(x = &quot;e&quot;) %&gt;% add_predictions(mod2) #&gt; Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor x has new level e 13.4.2 Interacciones (continuas y categóricas) ¿Qué ocurre si combinas una variable continua y una categórica? sim3 contiene un predictor categórico y otro predictor continuo. Podemos visualizarlos con un gráfico simple: ggplot(sim3, aes(x1, y)) + geom_point(aes(colour = x2)) Existen dos posibles modelos que se pueden ajustar a estos datos: mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) Cuando agregas variables con + el modelo va a estimar cada efecto independientemente de los demás. Es posible agregar al ajuste lo que se conoce como interacción usando *. Por ejemplo, y ~ x1 * x2 se traduce en y = a_0 + a_1 * x_1 + a_2 * x_2 + a_12 * x_1 * x_2. Observa que si usas *, tanto el efecto interacción como los efectos individuales se incluyen en el modelo. Para visualizar estos modelos necesitamos dos nuevos trucos: Tenemos dos predictores, por lo que necesitamos pasar ambas variables a data_grid(). Esto encontrará todos los valores únicos de x1 y x2 y luego generará todas las combinaciones, Para generar predicciones de ambos modelos simultáneamente, podemos usar gather_predictions() que incorpora cada predicción como una fila. El complemento de gather_predictions() es spread_predictions() que incluye cada predicción en una nueva columna. Esto combinado nos da: grid &lt;- sim3 %&gt;% data_grid(x1, x2) %&gt;% gather_predictions(mod1, mod2) grid #&gt; # A tibble: 80 x 4 #&gt; model x1 x2 pred #&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 mod1 1 a 1.67 #&gt; 2 mod1 1 b 4.56 #&gt; 3 mod1 1 c 6.48 #&gt; 4 mod1 1 d 4.03 #&gt; 5 mod1 2 a 1.48 #&gt; 6 mod1 2 b 4.37 #&gt; # … with 74 more rows Podemos visualizar los resultados de ambos modelos en un gráfico usando separando en facetas: ggplot(sim3, aes(x1, y, colour = x2)) + geom_point() + geom_line(data = grid, aes(y = pred)) + facet_wrap(~model) Observa que el modelo que usa + tiene la misma pendiente para cada recta, pero distintos interceptos (u ordenadas al origen). El modelo que usa * tiene distinta pendiente e intercepto. ¿Qué modelo es el más adecuado para los datos? Podemos mirar los residuos. Aquí hemos separado facetas por modelo y por x2ya que facilita ver el patrón dentro de cada grupo. sim3 &lt;- sim3 %&gt;% gather_residuals(mod1, mod2) ggplot(sim3, aes(x1, resid, colour = x2)) + geom_point() + facet_grid(model ~ x2) Existe un patrón poco obvio en los residuos de mod2. Los residuos de mod1 muestran que el modelo tiene algunos patrones ignorados en b, y un poco menos ignorados en c y d. Quizá te preguntas si existe una forma precisa de determinar si acaso mod1 o mod2 es mejor. Existe, pero requiere un respaldo matemático fuerte y no nos preocuparemos de eso. Lo que aquí interesa es evaluar cualitativamente si el modelo ha capturado los patrones que nos interesan. 13.4.3 Interacciones (dos variables continuas) Demos un vistazo al modelo equivalente para dos variables continuas. Para comenzar, se procede de igual modo que el ejemplo anterior: mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) grid &lt;- sim4 %&gt;% data_grid( x1 = seq_range(x1, 5), x2 = seq_range(x2, 5) ) %&gt;% gather_predictions(mod1, mod2) grid #&gt; # A tibble: 50 x 4 #&gt; model x1 x2 pred #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mod1 -1 -1 0.996 #&gt; 2 mod1 -1 -0.5 -0.395 #&gt; 3 mod1 -1 0 -1.79 #&gt; 4 mod1 -1 0.5 -3.18 #&gt; 5 mod1 -1 1 -4.57 #&gt; 6 mod1 -0.5 -1 1.91 #&gt; # … with 44 more rows Observa el uso de seq_range() dentro de data_grid(). En lugar de usar cada valor único de x, usamos una cuadrícula uniformemente espaciada de cinco valores entre los números mínimo y máximo. Quizá no es lo más importante aquí, pero es una técnica útil en general. Existen otros dos argumentos en seq_range(): pretty = TRUE generará una secuencia “bonita”, por ejemplo algo agradable al ojo humano. Esto es útil si quieres generar tablas a partir del output: seq_range(c(0.0123, 0.923423), n = 5) #&gt; [1] 0.0123 0.2401 0.4679 0.6956 0.9234 seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE) #&gt; [1] 0.0 0.2 0.4 0.6 0.8 1.0 trim = 0.1 eliminará el 10% de los valores en el extremo de la cola. Esto es útil si las variables tienen una distribución con una cola larga y te quieres enfocar en generar valores cerca del centro: x1 &lt;- rcauchy(100) seq_range(x1, n = 5) #&gt; [1] -115.9 -83.5 -51.2 -18.8 13.5 seq_range(x1, n = 5, trim = 0.10) #&gt; [1] -13.84 -8.71 -3.58 1.55 6.68 seq_range(x1, n = 5, trim = 0.25) #&gt; [1] -2.1735 -1.0594 0.0547 1.1687 2.2828 seq_range(x1, n = 5, trim = 0.50) #&gt; [1] -0.725 -0.268 0.189 0.647 1.104 expand = 0.1 es en cierta medida el opuesto de trim() ya que expande el rango en un 10%. x2 &lt;- c(0, 1) seq_range(x2, n = 5) #&gt; [1] 0.00 0.25 0.50 0.75 1.00 seq_range(x2, n = 5, expand = 0.10) #&gt; [1] -0.050 0.225 0.500 0.775 1.050 seq_range(x2, n = 5, expand = 0.25) #&gt; [1] -0.125 0.188 0.500 0.812 1.125 seq_range(x2, n = 5, expand = 0.50) #&gt; [1] -0.250 0.125 0.500 0.875 1.250 A continuación intentemos visualizar el modelo. Tenemos dos predictores continuos, por lo que te imaginarás el modelo como una superficie 3d. Podemos mostrar esto usando geom_tile(): ggplot(grid, aes(x1, x2)) + geom_tile(aes(fill = pred)) + facet_wrap(~model) ¡Esto no sugiere que los modelos sean muy distintos! Pero eso es en parte una ilusión: nuestros ojos y cerebros no son muy buenos en comparar sombras de color de forma adecuada. En lugar de mirar la superficie desde arriba, podríamos mirarla desde los costados, mostrando múltiples cortes: ggplot(grid, aes(x1, pred, colour = x2, group = x2)) + geom_line() + facet_wrap(~model) ggplot(grid, aes(x2, pred, colour = x1, group = x1)) + geom_line() + facet_wrap(~model) Esto muestra la interacción entre dos variables continuas que básicamente opera del mismo modo que una variable continua y una categórica. Una interacción dice que no existe un resultado fijo: necesitas considerar los valores de x1 y x2 simultáneamente para predecir y. Podrás ver que con apenas dos variables continuas, obtener un buen resultado de visualización es difícil. Pero esto es razonable: ¡no deberías esperar que con tres o más variables sea más fácil entender la interacción! Nuevamente, estamos parcialmente a salvo porque estamos usando modelos para la exploración y crearás tus propios modelos en el tiempo. El modelo no debe ser perfecto, tiene que ayudar a revelar información acerca de los datos. Pasé un tiempo mirando los residuos para ver si acaso mod2 es mejor que mod1. Creo que lo es, pero es algo sutil. Tendrás la oportunidad de explorar esto en los ejercicios. 13.4.4 Transformaciones Puedes hacer transformaciones dentro de la fórmula del modelo. Por ejemplo log(y) ~ sqrt(x1) + x2 se transforma en log(y) = a_1 + a_2 * sqrt(x1) + a_3 * x2. Si la transformación involucra +, *, ^ o -, necesitas dejar eso dentro de I() para que R no lo tome como parte de la especificación del modelo. Por ejemplo, y ~ x + I(x ^ 2) se traduce en y = a_1 + a_2 * x + a_3 * x^2. Si olvidas incluir I() y especificas y ~ x ^ 2 + x, R va a calcular y ~ x * x + x. x * x lo que resulta en la interacción de x consigo misma, lo que se reduce a x. R automáticamente elimina las variables redundantes, por lo que x + x se convierte en x, lo que significa que y ~ x ^ 2 + x especifica la función y = a_1 + a_2 * x. ¡Eso probablemente no es lo que querías! Nuevamente, si te confunde lo que el modelo hace, puedes usar model_matrix() para ver exactamente lo que la ecuación lm() está ajustando: df &lt;- tribble( ~y, ~x, 1, 1, 2, 2, 3, 3 ) model_matrix(df, y ~ x^2 + x) #&gt; # A tibble: 3 x 2 #&gt; `(Intercept)` x #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 2 #&gt; 3 1 3 model_matrix(df, y ~ I(x^2) + x) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `I(x^2)` x #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 1 #&gt; 2 1 4 2 #&gt; 3 1 9 3 Las transformaciones son útiles porque puedes aproximar funciones no lineales. Si tuviste clases de cálculo, habrás escuchado acerca del teorema de Taylor que dice que puedes aproximar una función suave como la suma de infinitos polinomios. Esto significa que puedes usar una función polinomial para acercarte a una distancia arbitrariamente pequeña de una función suave ajustando una ecuación como y = a_1 + a_2 * x + a_3 * x^2 + a_4 * x^3. Escribir esta secuencia a mano es tedioso, pero R provee la función auxiliar poly(): model_matrix(df, y ~ poly(x, 2)) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `poly(x, 2)1` `poly(x, 2)2` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 -7.07e- 1 0.408 #&gt; 2 1 -7.85e-17 -0.816 #&gt; 3 1 7.07e- 1 0.408 Sin embargo, existe un problema mayor al usar poly(): fuera del rango de los datos, los polinomios rápidamente se disparan a infinito positivo o negativo. Una alternativa más segura es usar spline natural, splines::ns(). library(splines) model_matrix(df, y ~ ns(x, 2)) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `ns(x, 2)1` `ns(x, 2)2` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0 0 #&gt; 2 1 0.566 -0.211 #&gt; 3 1 0.344 0.771 Veamos esto cuando intentamos aproximar una función no lineal: sim5 &lt;- tibble( x = seq(0, 3.5 * pi, length = 50), y = 4 * sin(x) + rnorm(length(x)) ) ggplot(sim5, aes(x, y)) + geom_point() Voy a ajustar cinco modelos a los datos. mod1 &lt;- lm(y ~ splines::ns(x, 1), data = sim5) mod2 &lt;- lm(y ~ splines::ns(x, 2), data = sim5) mod3 &lt;- lm(y ~ splines::ns(x, 3), data = sim5) mod4 &lt;- lm(y ~ splines::ns(x, 4), data = sim5) mod5 &lt;- lm(y ~ splines::ns(x, 5), data = sim5) grid &lt;- sim5 %&gt;% data_grid(x = seq_range(x, n = 50, expand = 0.1)) %&gt;% gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = &quot;y&quot;) ggplot(sim5, aes(x, y)) + geom_point() + geom_line(data = grid, colour = &quot;red&quot;) + facet_wrap(~model) Observa que la extrapolación fuera del rango de los datos es claramente mala. Esta es la desventaja de aproximar una función mediante un polinomio. Pero este es un problema real con cualquier modelo: el modelo nunca te dirá si el comportamiento es verdadero cuando extrapolas fuera del rango de los datos que has observado. Deberás apoyarte en la teoría y la ciencia. 13.4.5 Ejercicios ¿Qué pasa si repites el análisis de sim2 usando un modelo sin intercepto? ¿Qué ocurre con la ecuación del modelo? ¿Qué ocurre con las predicciones? Usa model_matrix() para explorar las ecuaciones generadas por los modelos ajustados a sim3 y sim4. ¿Por qué * es un atajo para la interacción? Usando los principios básicos, convierte las fórmulas de los siguientes modelos en funciones. (Sugerencia: comienza por convertir las variables categóricas en ceros y unos.) mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) Para sim4, ¿Es mejor mod1 o mod2? Yo creo que mod2 es ligeramente mejor removiendo las tendencias, pero es bastante sutil. ¿Puedes generar un gráfico que de sustento a esta hipótesis? 13.5 Valores faltantes Los valores faltantes obviamente no proporcionan información respecto de la relación entre las variables, por lo que modelar funciones va a eliminar todas las filas con valores faltantes. R por defecto lo hace de forma silenciosa, pero options(na.action = na.warn) (ejecutado en los prerrequisitos) asegura que la salida incluya una advertencia. df &lt;- tribble( ~x, ~y, 1, 2.2, 2, NA, 3, 3.5, 4, 8.3, NA, 10 ) mod &lt;- lm(y ~ x, data = df) #&gt; Warning: Dropping 2 rows with missing values Para suprimir los mensajes de advertencia, incluye na.action = na.exclude: mod &lt;- lm(y ~ x, data = df, na.action = na.exclude) Siempre puedes consultar cuántas observaciones se usaron con nobs(): nobs(mod) #&gt; [1] 3 13.6 Otras familias de modelos Este capítulo se centró de forma exclusiva en la familia de modelos lineales, la cual asume una relación de la forma y = a_1 * x1 + a_2 * x2 + ... + a_n * xn. Además, los modelos lineales asumen que los residuos siguen una distribución normal, algo de lo que no hemos hablado. Existe un amplio conjunto de familias de modelos que extienden la familia de modelos lineales de varias formas interesantes. Algunos son: Modelos lineales generalizados, es decir, stats::glm(). Los modelos lineales asumen que la respuesta es una variable continua y que el error sigue una distribución normal. Los modelos lineales generalizados extienden los modelos lineales para incluir respuestas no continuas (es decir, datos binarios o conteos). Definen una distancia métrica basada en la idea estadística de verosimilitud. Modelos generalizados aditivos, es decir, mgcv::gam(), extienden los modelos lineales generalizados para incorporar funciones suaves arbitrarias. Esto significa que puedes escribir una fórmula del tipo y ~ s(x) que se transforma en una ecuación de la forma y = f(x) y dejar que gam() estime la función (sujeto a algunas restricciones de suavidad para que el problema sea manejable). Modelos lineales penalizados, es decir, glmnet::glmnet(), incorporan un término de penalización a la distancia y así penalizan modelos complejos (definidos por la distancia entre el vector de parámetros y el origen). Esto tiende a entregar modelos que generalizan mejor respecto de nuevos conjuntos de datos para la misma población. Modelos lineales robustos, es decir, MASS:rlm(), modifican la distancia para restar importancia a los puntos que quedan muy alejados. Esto resulta en modelos menos sensibles a valores extremos, con el inconveniente de que no son muy buenos cuando no hay valores extremos. Árboles, es decir, rpart::rpart(), atacan un problema de un modo totalmente distinto a los modelos lineales. Ajustan un modelo constante por partes, dividiendo los datos en partes progresivamente más y más pequeñas. Los árboles no son tremendamente efectivos por sí solos, pero son muy poderosos cuando se usan en modelos agregados como bosques aleatorios, del inglés random forests), (es decir, randomForest::randomForest()) o máquinas aceleradoras de gradiente, del inglés gradient boosting machines (es decir, xgboost::xgboost.). Estos modelos son todos similares desde una perspectiva de programación. Una vez que hayas manejado los modelos lineales, te resultará sencillo entender la mecánica de otras clases de modelos. Ser un modelador hábil consiste en tener buenos principios generales y una gran caja de herramientas técnicas. Ahora que has aprendido algunas herramientas y algunas clases de modelos, puedes continuar aprendiendo sobre otras clases en otras fuentes. "],
["model-building.html", "14 Model building 14.1 Introduction 14.2 Why are low quality diamonds more expensive? 14.3 What affects the number of daily flights? 14.4 Learning more about models", " 14 Model building 14.1 Introduction In the previous chapter you learned how linear models work, and learned some basic tools for understanding what a model is telling you about your data. The previous chapter focussed on simulated datasets. This chapter will focus on real data, showing you how you can progressively build up a model to aid your understanding of the data. We will take advantage of the fact that you can think about a model partitioning your data into pattern and residuals. We’ll find patterns with visualisation, then make them concrete and precise with a model. We’ll then repeat the process, but replace the old response variable with the residuals from the model. The goal is to transition from implicit knowledge in the data and your head to explicit knowledge in a quantitative model. This makes it easier to apply to new domains, and easier for others to use. For very large and complex datasets this will be a lot of work. There are certainly alternative approaches - a more machine learning approach is simply to focus on the predictive ability of the model. These approaches tend to produce black boxes: the model does a really good job at generating predictions, but you don’t know why. This is a totally reasonable approach, but it does make it hard to apply your real world knowledge to the model. That, in turn, makes it difficult to assess whether or not the model will continue to work in the long-term, as fundamentals change. For most real models, I’d expect you to use some combination of this approach and a more classic automated approach. It’s a challenge to know when to stop. You need to figure out when your model is good enough, and when additional investment is unlikely to pay off. I particularly like this quote from reddit user Broseidon241: A long time ago in art class, my teacher told me “An artist needs to know when a piece is done. You can’t tweak something into perfection - wrap it up. If you don’t like it, do it over again. Otherwise begin something new”. Later in life, I heard “A poor seamstress makes many mistakes. A good seamstress works hard to correct those mistakes. A great seamstress isn’t afraid to throw out the garment and start over.” – Broseidon241, https://www.reddit.com/r/datascience/comments/4irajq 14.1.1 Prerequisites We’ll use the same tools as in the previous chapter, but add in some real datasets: diamonds from ggplot2, and flights from nycflights13. We’ll also need lubridate in order to work with the date/times in flights. library(tidyverse) library(hexbin) library(modelr) options(na.action = na.warn) library(nycflights13) library(lubridate) 14.2 Why are low quality diamonds more expensive? In previous chapters we’ve seen a surprising relationship between the quality of diamonds and their price: low quality diamonds (poor cuts, bad colours, and inferior clarity) have higher prices. ggplot(diamonds, aes(cut, price)) + geom_boxplot() ggplot(diamonds, aes(color, price)) + geom_boxplot() ggplot(diamonds, aes(clarity, price)) + geom_boxplot() Note that the worst diamond color is J (slightly yellow), and the worst clarity is I1 (inclusions visible to the naked eye). 14.2.1 Price and carat It looks like lower quality diamonds have higher prices because there is an important confounding variable: the weight (carat) of the diamond. The weight of the diamond is the single most important factor for determining the price of the diamond, and lower quality diamonds tend to be larger. ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 50) We can make it easier to see how the other attributes of a diamond affect its relative price by fitting a model to separate out the effect of carat. But first, lets make a couple of tweaks to the diamonds dataset to make it easier to work with: Focus on diamonds smaller than 2.5 carats (99.7% of the data) Log-transform the carat and price variables. diamonds2 &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% mutate(lprice = log2(price), lcarat = log2(carat)) Together, these changes make it easier to see the relationship between carat and price: ggplot(diamonds2, aes(lcarat, lprice)) + geom_hex(bins = 50) The log-transformation is particularly useful here because it makes the pattern linear, and linear patterns are the easiest to work with. Let’s take the next step and remove that strong linear pattern. We first make the pattern explicit by fitting a model: mod_diamond &lt;- lm(lprice ~ lcarat, data = diamonds2) Then we look at what the model tells us about the data. Note that I back transform the predictions, undoing the log transformation, so I can overlay the predictions on the raw data: grid &lt;- diamonds2 %&gt;% data_grid(carat = seq_range(carat, 20)) %&gt;% mutate(lcarat = log2(carat)) %&gt;% add_predictions(mod_diamond, &quot;lprice&quot;) %&gt;% mutate(price = 2^lprice) ggplot(diamonds2, aes(carat, price)) + geom_hex(bins = 50) + geom_line(data = grid, colour = &quot;red&quot;, size = 1) That tells us something interesting about our data. If we believe our model, then the large diamonds are much cheaper than expected. This is probably because no diamond in this dataset costs more than $19,000. Now we can look at the residuals, which verifies that we’ve successfully removed the strong linear pattern: diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond, &quot;lresid&quot;) ggplot(diamonds2, aes(lcarat, lresid)) + geom_hex(bins = 50) Importantly, we can now re-do our motivating plots using those residuals instead of price. ggplot(diamonds2, aes(cut, lresid)) + geom_boxplot() ggplot(diamonds2, aes(color, lresid)) + geom_boxplot() ggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot() Now we see the relationship we expect: as the quality of the diamond increases, so too does its relative price. To interpret the y axis, we need to think about what the residuals are telling us, and what scale they are on. A residual of -1 indicates that lprice was 1 unit lower than a prediction based solely on its weight. \\(2^{-1}\\) is 1/2, points with a value of -1 are half the expected price, and residuals with value 1 are twice the predicted price. 14.2.2 A more complicated model If we wanted to, we could continue to build up our model, moving the effects we’ve observed into the model to make them explicit. For example, we could include color, cut, and clarity into the model so that we also make explicit the effect of these three categorical variables: mod_diamond2 &lt;- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2) This model now includes four predictors, so it’s getting harder to visualise. Fortunately, they’re currently all independent which means that we can plot them individually in four plots. To make the process a little easier, we’re going to use the .model argument to data_grid: grid &lt;- diamonds2 %&gt;% data_grid(cut, .model = mod_diamond2) %&gt;% add_predictions(mod_diamond2) grid #&gt; # A tibble: 5 x 5 #&gt; cut lcarat color clarity pred #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Fair -0.515 G VS2 11.2 #&gt; 2 Good -0.515 G VS2 11.3 #&gt; 3 Very Good -0.515 G VS2 11.4 #&gt; 4 Premium -0.515 G VS2 11.4 #&gt; 5 Ideal -0.515 G VS2 11.4 ggplot(grid, aes(cut, pred)) + geom_point() If the model needs variables that you haven’t explicitly supplied, data_grid() will automatically fill them in with “typical” value. For continuous variables, it uses the median, and categorical variables it uses the most common value (or values, if there’s a tie). diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond2, &quot;lresid2&quot;) ggplot(diamonds2, aes(lcarat, lresid2)) + geom_hex(bins = 50) This plot indicates that there are some diamonds with quite large residuals - remember a residual of 2 indicates that the diamond is 4x the price that we expected. It’s often useful to look at unusual values individually: diamonds2 %&gt;% filter(abs(lresid2) &gt; 1) %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(pred = round(2^pred)) %&gt;% select(price, pred, carat:table, x:z) %&gt;% arrange(price) #&gt; # A tibble: 16 x 11 #&gt; price pred carat cut color clarity depth table x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1013 264 0.25 Fair F SI2 54.4 64 4.3 4.23 2.32 #&gt; 2 1186 284 0.25 Premium G SI2 59 60 5.33 5.28 3.12 #&gt; 3 1186 284 0.25 Premium G SI2 58.8 60 5.33 5.28 3.12 #&gt; 4 1262 2644 1.03 Fair E I1 78.2 54 5.72 5.59 4.42 #&gt; 5 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; 6 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; # … with 10 more rows Nothing really jumps out at me here, but it’s probably worth spending time considering if this indicates a problem with our model, or if there are errors in the data. If there are mistakes in the data, this could be an opportunity to buy diamonds that have been priced low incorrectly. 14.2.3 Exercises In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent? If log(price) = a_0 + a_1 * log(carat), what does that say about the relationship between price and carat? Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors? Does the final model, mod_diamond2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond? 14.3 What affects the number of daily flights? Let’s work through a similar process for a dataset that seems even simpler at first glance: the number of flights that leave NYC per day. This is a really small dataset — only 365 rows and 2 columns — and we’re not going to end up with a fully realised model, but as you’ll see, the steps along the way will help us better understand the data. Let’s get started by counting the number of flights per day and visualising it with ggplot2. daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% group_by(date) %&gt;% summarise(n = n()) daily #&gt; # A tibble: 365 x 2 #&gt; date n #&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 2013-01-01 842 #&gt; 2 2013-01-02 943 #&gt; 3 2013-01-03 914 #&gt; 4 2013-01-04 915 #&gt; 5 2013-01-05 720 #&gt; 6 2013-01-06 832 #&gt; # … with 359 more rows ggplot(daily, aes(date, n)) + geom_line() 14.3.1 Day of week Understanding the long-term trend is challenging because there’s a very strong day-of-week effect that dominates the subtler patterns. Let’s start by looking at the distribution of flight numbers by day-of-week: daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) ggplot(daily, aes(wday, n)) + geom_boxplot() There are fewer flights on weekends because most travel is for business. The effect is particularly pronounced on Saturday: you might sometimes leave on Sunday for a Monday morning meeting, but it’s very rare that you’d leave on Saturday as you’d much rather be at home with your family. One way to remove this strong pattern is to use a model. First, we fit the model, and display its predictions overlaid on the original data: mod &lt;- lm(n ~ wday, data = daily) grid &lt;- daily %&gt;% data_grid(wday) %&gt;% add_predictions(mod, &quot;n&quot;) ggplot(daily, aes(wday, n)) + geom_boxplot() + geom_point(data = grid, colour = &quot;red&quot;, size = 4) Next we compute and visualise the residuals: daily &lt;- daily %&gt;% add_residuals(mod) daily %&gt;% ggplot(aes(date, resid)) + geom_ref_line(h = 0) + geom_line() Note the change in the y-axis: now we are seeing the deviation from the expected number of flights, given the day of week. This plot is useful because now that we’ve removed much of the large day-of-week effect, we can see some of the subtler patterns that remain: Our model seems to fail starting in June: you can still see a strong regular pattern that our model hasn’t captured. Drawing a plot with one line for each day of the week makes the cause easier to see: ggplot(daily, aes(date, resid, colour = wday)) + geom_ref_line(h = 0) + geom_line() Our model fails to accurately predict the number of flights on Saturday: during summer there are more flights than we expect, and during Fall there are fewer. We’ll see how we can do better to capture this pattern in the next section. There are some days with far fewer flights than expected: daily %&gt;% filter(resid &lt; -100) #&gt; # A tibble: 11 x 4 #&gt; date n wday resid #&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; #&gt; 1 2013-01-01 842 Tue -109. #&gt; 2 2013-01-20 786 Sun -105. #&gt; 3 2013-05-26 729 Sun -162. #&gt; 4 2013-07-04 737 Thu -229. #&gt; 5 2013-07-05 822 Fri -145. #&gt; 6 2013-09-01 718 Sun -173. #&gt; # … with 5 more rows If you’re familiar with American public holidays, you might spot New Year’s day, July 4th, Thanksgiving and Christmas. There are some others that don’t seem to correspond to public holidays. You’ll work on those in one of the exercises. There seems to be some smoother long term trend over the course of a year. We can highlight that trend with geom_smooth(): daily %&gt;% ggplot(aes(date, resid)) + geom_ref_line(h = 0) + geom_line(colour = &quot;grey50&quot;) + geom_smooth(se = FALSE, span = 0.20) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; There are fewer flights in January (and December), and more in summer (May-Sep). We can’t do much with this pattern quantitatively, because we only have a single year of data. But we can use our domain knowledge to brainstorm potential explanations. 14.3.2 Seasonal Saturday effect Let’s first tackle our failure to accurately predict the number of flights on Saturday. A good place to start is to go back to the raw numbers, focussing on Saturdays: daily %&gt;% filter(wday == &quot;Sat&quot;) %&gt;% ggplot(aes(date, n)) + geom_point() + geom_line() + scale_x_date(NULL, date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) (I’ve used both points and lines to make it more clear what is data and what is interpolation.) I suspect this pattern is caused by summer holidays: many people go on holiday in the summer, and people don’t mind travelling on Saturdays for vacation. Looking at this plot, we might guess that summer holidays are from early June to late August. That seems to line up fairly well with the state’s school terms: summer break in 2013 was Jun 26–Sep 9. Why are there more Saturday flights in the Spring than the Fall? I asked some American friends and they suggested that it’s less common to plan family vacations during the Fall because of the big Thanksgiving and Christmas holidays. We don’t have the data to know for sure, but it seems like a plausible working hypothesis. Lets create a “term” variable that roughly captures the three school terms, and check our work with a plot: term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) daily %&gt;% filter(wday == &quot;Sat&quot;) %&gt;% ggplot(aes(date, n, colour = term)) + geom_point(alpha = 1 / 3) + geom_line() + scale_x_date(NULL, date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) (I manually tweaked the dates to get nice breaks in the plot. Using a visualisation to help you understand what your function is doing is a really powerful and general technique.) It’s useful to see how this new variable affects the other days of the week: daily %&gt;% ggplot(aes(wday, n, colour = term)) + geom_boxplot() It looks like there is significant variation across the terms, so fitting a separate day of week effect for each term is reasonable. This improves our model, but not as much as we might hope: mod1 &lt;- lm(n ~ wday, data = daily) mod2 &lt;- lm(n ~ wday * term, data = daily) daily %&gt;% gather_residuals(without_term = mod1, with_term = mod2) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) We can see the problem by overlaying the predictions from the model on to the raw data: grid &lt;- daily %&gt;% data_grid(wday, term) %&gt;% add_predictions(mod2, &quot;n&quot;) ggplot(daily, aes(wday, n)) + geom_boxplot() + geom_point(data = grid, colour = &quot;red&quot;) + facet_wrap(~term) Our model is finding the mean effect, but we have a lot of big outliers, so mean tends to be far away from the typical value. We can alleviate this problem by using a model that is robust to the effect of outliers: MASS::rlm(). This greatly reduces the impact of the outliers on our estimates, and gives a model that does a good job of removing the day of week pattern: mod3 &lt;- MASS::rlm(n ~ wday * term, data = daily) daily %&gt;% add_residuals(mod3, &quot;resid&quot;) %&gt;% ggplot(aes(date, resid)) + geom_hline(yintercept = 0, size = 2, colour = &quot;white&quot;) + geom_line() It’s now much easier to see the long-term trend, and the positive and negative outliers. 14.3.3 Computed variables If you’re experimenting with many models and many visualisations, it’s a good idea to bundle the creation of variables up into a function so there’s no chance of accidentally applying a different transformation in different places. For example, we could write: compute_vars &lt;- function(data) { data %&gt;% mutate( term = term(date), wday = wday(date, label = TRUE) ) } Another option is to put the transformations directly in the model formula: wday2 &lt;- function(x) wday(x, label = TRUE) mod3 &lt;- lm(n ~ wday2(date) * term(date), data = daily) Either approach is reasonable. Making the transformed variable explicit is useful if you want to check your work, or use them in a visualisation. But you can’t easily use transformations (like splines) that return multiple columns. Including the transformations in the model function makes life a little easier when you’re working with many different datasets because the model is self contained. 14.3.4 Time of year: an alternative approach In the previous section we used our domain knowledge (how the US school term affects travel) to improve the model. An alternative to using our knowledge explicitly in the model is to give the data more room to speak. We could use a more flexible model and allow that to capture the pattern we’re interested in. A simple linear trend isn’t adequate, so we could try using a natural spline to fit a smooth curve across the year: library(splines) mod &lt;- MASS::rlm(n ~ wday * ns(date, 5), data = daily) daily %&gt;% data_grid(wday, date = seq_range(date, n = 13)) %&gt;% add_predictions(mod) %&gt;% ggplot(aes(date, pred, colour = wday)) + geom_line() + geom_point() We see a strong pattern in the numbers of Saturday flights. This is reassuring, because we also saw that pattern in the raw data. It’s a good sign when you get the same signal from different approaches. 14.3.5 Exercises Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year? What do the three days with high positive residuals represent? How would these days generalise to another year? daily %&gt;% top_n(3, resid) #&gt; # A tibble: 3 x 5 #&gt; date n wday resid term #&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 2013-11-30 857 Sat 112. fall #&gt; 2 2013-12-01 987 Sun 95.5 fall #&gt; 3 2013-12-28 814 Sat 69.4 fall Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term? Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like? What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful? What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective? We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday. 14.4 Learning more about models We have only scratched the absolute surface of modelling, but you have hopefully gained some simple, but general-purpose tools that you can use to improve your own data analyses. It’s OK to start simple! As you’ve seen, even very simple models can make a dramatic difference in your ability to tease out interactions between variables. These modelling chapters are even more opinionated than the rest of the book. I approach modelling from a somewhat different perspective to most others, and there is relatively little space devoted to it. Modelling really deserves a book on its own, so I’d highly recommend that you read at least one of these three books: Statistical Modeling: A Fresh Approach by Danny Kaplan, http://www.mosaic-web.org/go/StatisticalModeling/. This book provides a gentle introduction to modelling, where you build your intuition, mathematical tools, and R skills in parallel. The book replaces a traditional “introduction to statistics” course, providing a curriculum that is up-to-date and relevant to data science. An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, http://www-bcf.usc.edu/~gareth/ISL/ (available online for free). This book presents a family of modern modelling techniques collectively known as statistical learning. For an even deeper understanding of the math behind the models, read the classic Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, http://statweb.stanford.edu/~tibs/ElemStatLearn/ (also available online for free). Applied Predictive Modeling by Max Kuhn and Kjell Johnson, http://appliedpredictivemodeling.com. This book is a companion to the caret package and provides practical tools for dealing with real-life predictive modelling challenges. "],
["muchos-modelos.html", "15 Muchos modelos 15.1 Introducción 15.2 gapminder 15.3 Columnas-lista 15.4 Creando columnas-lista 15.5 Simplificando columnas-lista 15.6 Haciendo datos ordenados con broom", " 15 Muchos modelos 15.1 Introducción En este capítulo vas a aprender tres ideas poderosas que te van a ayudar a trabajar fácilmente con un gran número de modelos: Usar muchos modelos simples para entender mejor conjuntos de datos complejos. Usar columnas-lista (list-columns) para almacenar estructuras de datos arbitrarias en un data frame. Ésto, por ejemplo, te permitirá tener una columna que contenga modelos lineales. Usar el paquete broom, de David Robinson, para transformar modelos en datos ordenados. Ésta es una técnica poderosa para trabajar con un gran número de modelos porque una vez que tienes datos ordenados, puedes aplicar todas las técnicas que has aprendido anteriormente en el libro. Empezaremos entrando de lleno en un ejemplo motivador usando datos sobre la esperanza de vida alrededor del mundo. Es un conjunto de datos pequeño pero que ilustra cuán importante puede ser modelar para mejorar tus visualizaciones. Utilizaremos un número grande de modelos simples para extraer algunas de las señales más fuertes y que podamos ver las sutiles que permanecen. También veremos cómo las medidas de resumen de los modelos nos pueden ayudar a encontrar datos atípicos y tendencias inusuales. Las siguientes secciones ahondarán en más detalles acerca de las técnicas: En columnas-lista, aprenderás más acerca de la estructura de datos columna-lista, y por qué es válido poner listas en data frames. En creando columnas-lista, aprenderás las tres maneras principales en las que crearás columnas-lista. En simplificando columnas-lista aprenderás cómo convertir columnas-lista de vuelta a vectores atómicos regulares (o conjuntos de vectores atómicos) para que puedas trabajar con ellos más fácilmente. En haciendo datos ordenados con broom, aprenderás sobre el conjunto de herramientas completo provisto por broom (del inglés escoba), y verás cómo puede ser aplicado a otros tipos de estructuras de datos. Este capítulo es aspiracional en cierta medida: si este libro es tu primera introducción a R, este capítulo es probable que sea una lucha para ti. Requiere que tengas profundamente internalizadas ideas acerca de modelamiento, estructuras de datos, e iteración. Así que no te preocupes si no lo entiendes — solo aparta este capítulo por un par de meses, y vuelve cuando quieras ejercitar tu cerebro. 15.1.1 Prerrequisitos Trabajar con muchos modelos requiere muchos de los paquetes del tidyverse (para exploración de datos, doma y programación) y modelr para facilitar el modelamiento. library(modelr) library(tidyverse) 15.2 gapminder Para motivar el poder de muchos modelos simples, vamos a mirar los datos de “gapminder”. Estos datos fueron popularizados por Hans Rosling, un doctor y estadístico sueco. Si nunca has escuchado de él, para de leer este capítulo ahora mismo y ve a mirar uno de sus videos! Él es un presentador de datos fantástico e ilustra cómo puedes usar datos para presentar una historia convincente. Un buen lugar para empezar es este video corto filmado en conjunto con la BBC: https://www.youtube.com/watch?v=jbkSRLYSojo. Los datos de gapminder resumen la progresión de países a través del tiempo, mirando estadísticos como esperanza de vida y PIB. Los datos son de fácil acceso en R, gracias a Jenny Bryan que creó el paquete gapminder. Utilizaremos la versión en español contenida en el paquete datos (TODO: ver cómo poner esto): library(datos) paises #&gt; # A tibble: 1,704 x 6 #&gt; pais continente anio esperanza_de_vida poblacion pib_per_capita #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afganistan Asia 1952 28.8 8425333 779. #&gt; 2 Afganistan Asia 1957 30.3 9240934 821. #&gt; 3 Afganistan Asia 1962 32.0 10267083 853. #&gt; 4 Afganistan Asia 1967 34.0 11537966 836. #&gt; 5 Afganistan Asia 1972 36.1 13079460 740. #&gt; 6 Afganistan Asia 1977 38.4 14880372 786. #&gt; # … with 1,698 more rows En este caso de estudio, nos enfocaremos en solo tres variables para responder la pregunta “¿Cómo la esperanza de vida (esperanza_de_vida) cambia a través del tiempo (anio) para cada país (pais)?”. Un buen lugar para empezar es con un gráfico: paises %&gt;% ggplot(aes(anio, esperanza_de_vida, group = pais)) + geom_line(alpha = 1 / 3) Es un conjunto de datos pequeño: solo tiene ~1,700 observaciones y 3 variables. Pero aún así es difícil ver qué está pasando! En general, parece que la esperanza de vida ha estado mejorando en forma constante. Sin embargo, si miras de cerca, puedes notar algunos países que no siguen este patrón. ¿Cómo podemos hacer que esos países se vean más fácilmente? Una forma es usar el mismo enfoque que en el último capítulo: hay una señal fuerte (en general crecimiento lineal) que hace difícil ver tendencias más sutiles. Separaremos estos factores estimando un modelo con una tendencia lineal. El modelo captura el crecimiento estable en el tiempo, y los residuos mostrarán lo que queda fuera. Ya sabes cómo hacer eso si tenemos un solo país: nz &lt;- filter(paises, pais == &quot;Nueva Zelanda&quot;) nz %&gt;% ggplot(aes(anio, esperanza_de_vida)) + geom_line() + ggtitle(&quot;Datos completos = &quot;) nz_mod &lt;- lm(esperanza_de_vida ~ anio, data = nz) nz %&gt;% add_predictions(nz_mod) %&gt;% ggplot(aes(anio, pred)) + geom_line() + ggtitle(&quot;Tendencia lineal + &quot;) nz %&gt;% add_residuals(nz_mod) %&gt;% ggplot(aes(anio, resid)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 3) + geom_line() + ggtitle(&quot;Patrón restante&quot;) ¿Cómo podemos ajustar fácilmente ese modelo para cada país? 15.2.1 Datos anidados Te puedes imaginar copiando y pegando ese código múltiples veces; pero ya has aprendido una mejor forma! Extrae el código en común con una función y repítelo usando una función map (TODO: no me queda claro que tenga sentido poner que el nombre de esta función venga del inglés, y no sé si ponerla en cursiva) de purrr. Este problema se estructura un poco diferente respecto a lo que has visto antes. En lugar de repetir una acción por cada variable, queremos repetirla para cada país, un subconjunto de filas. Para hacer eso, necesitamos una nueva estructura de datos: el data frame anidado (nested data frame). Para crear un data frame anidado empezamos con un data frame agrupado, y lo “anidamos”: por_pais &lt;- paises %&gt;% group_by(pais, continente) %&gt;% nest() por_pais #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistan Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows (Estoy haciendo un poco de trampa agrupando por continente y pais al mismo tiempo. Dado el pais, continente es fijo, así que no agrega ningún grupo más, pero es una forma fácil de llevarnos una variable adicional para el camino.) Ésto crea un data frame que tiene una fila por grupo (por país), y una columna bastante inusual: data. data es una lista de data frames (o tibbles, para ser precisos). Esto parece una idea un poco loca: tenemos un data frame con una columna que es una lista de otros data frames! Explicaré brevemente por qué pienso que es una buena idea. La columna data es un poco difícil de examinar porque es una lista moderadamente complicada, y todavía estamos trabajando para tener buenas herramientas para explorar estos objetos. Desafortunadamente usar str() no es recomendable porque usualmente producirá un output (salida de código) muy extenso. Pero si extraes un solo elemento de la columna data verás que contiene todos los datos para ese país (en este caso, Afganistán). por_pais$data[[1]] #&gt; # A tibble: 12 x 4 #&gt; anio esperanza_de_vida poblacion pib_per_capita #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 8425333 779. #&gt; 2 1957 30.3 9240934 821. #&gt; 3 1962 32.0 10267083 853. #&gt; 4 1967 34.0 11537966 836. #&gt; 5 1972 36.1 13079460 740. #&gt; 6 1977 38.4 14880372 786. #&gt; # … with 6 more rows Nota la diferencia entre un data frame agrupado estándar y un data frame anidado: en un data frame agrupado, cada fila es una observación; en un data frame anidado, cada fila es un grupo. Otra forma de pensar en un conjunto de datos anidado es que ahora tenemos una meta-observación: una fila que representa todo el transcurso de tiempo para un país, en lugar de solo un punto en el tiempo. 15.2.2 Columnas-lista Ahora que tenemos nuestro data frame anidado, estamos en una buena posición para ajustar algunos modelos. Tenemos una función para ajustar modelos: modelo_pais &lt;- function(df) { lm(esperanza_de_vida ~ anio, data = df) } Y queremos aplicarlo a cada data frame. Los data frames están en una lista, así que podemos usar purrr::map() para aplicar modelo_pais a cada elemento: modelos &lt;- map(por_pais$data, modelo_pais) Sin embargo, en lugar de dejar la lista de modelos como un objeto suelto, creo que es mejor almacenarlo como una columna en el data frame por_pais. Almacenar objetos relacionados en columnas es una parte clave del valor de los data frames, y por eso pienso que las columnas-lista son tan buena idea. En el transcurso de nuetro trabajo con estos países, vamos a tener muchas listas donde tenemos un elemento por país. ¿Por qué no almacenarlos todos juntos en un data frame? En otras palabras, en lugar de crear un nuevo objeto en el entorno global, vamos a crear una nueva variable en el data frame por_pais. Ese es un trabajo para dplyr::mutate(): por_pais &lt;- por_pais %&gt;% mutate(modelo = map(data, modelo_pais)) por_pais #&gt; # A tibble: 142 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Afganistan Asia &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; # … with 136 more rows Esto tiene una gran ventaja: como todos los objetos relacionados están almacenados juntos, no necesitas manualmente mantenerlos sincronizados cuando filtras o reordenas. La semántica del data frame se ocupa de esto por ti: por_pais %&gt;% filter(continente == &quot;Europa&quot;) #&gt; # A tibble: 30 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Albania Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 2 Austria Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 3 Bélgica Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 4 Bosnia y Herzegovina Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 5 Bulgaria Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 6 Croacia Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; # … with 24 more rows por_pais %&gt;% arrange(continente, pais) #&gt; # A tibble: 142 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Argelia África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 2 Angola África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 3 Benin África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 4 Botswana África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 5 Burkina Faso África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; 6 Burundi África &lt;tibble [12 × 4]&gt; &lt;lm&gt; #&gt; # … with 136 more rows Si tu lista de data frames y lista de modelos fueran objetos separados, tienes (TODO: acá usaría “tendrías” pero no es lo que usa el original) que acordarte de que cuando reordenas o seleccionas un subconjunto de un vector, necesitas reordenar o seleccionar el subconjunto de todos los demás para mantenerlos sincronizados. Si te olvidas, tu código va a seguir funcionando, pero va a devolver la respuesta equivocada! 15.2.3 Desanidando Previamente calculamos los residuos de un único modelo con un conjunto de datos también único. Ahora tenemos 142 data frames y 142 modelos. Para calcular los residuos, necesitamos llamar a la función add_residuals() (del inglés adicionar residuos) con cada par modelo-datos: por_pais &lt;- por_pais %&gt;% mutate( residuos = map2(data, modelo, add_residuals) ) por_pais #&gt; # A tibble: 142 x 5 #&gt; pais continente data modelo residuos #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Afganistan Asia &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; # … with 136 more rows ¿Pero cómo puedes graficar una lista de data frames? En lugar de luchar para contestar esa pregunta, transformemos la lista de data frames de vuelta en un data frame regular. Previamente usamos nest() (del inglés anidar) para transformar un data frame regular en uno anidado, y ahora desanidamos con unnest(): residuos &lt;- unnest(por_pais, residuos) residuos #&gt; # A tibble: 1,704 x 7 #&gt; pais continente anio esperanza_de_vi… poblacion pib_per_capita resid #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afgan… Asia 1952 28.8 8425333 779. -1.11 #&gt; 2 Afgan… Asia 1957 30.3 9240934 821. -0.952 #&gt; 3 Afgan… Asia 1962 32.0 10267083 853. -0.664 #&gt; 4 Afgan… Asia 1967 34.0 11537966 836. -0.0172 #&gt; 5 Afgan… Asia 1972 36.1 13079460 740. 0.674 #&gt; 6 Afgan… Asia 1977 38.4 14880372 786. 1.65 #&gt; # … with 1,698 more rows Nota que cada columna regular está repetida una vez por cada fila en la columna anidada. Ahora tenemos un data frame regular, podemos graficar los residuos: residuos %&gt;% ggplot(aes(anio, resid)) + geom_line(aes(group = pais), alpha = 1 / 3) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Separar facetas por continente es particularmente revelador: residuos %&gt;% ggplot(aes(anio, resid, group = pais)) + geom_line(alpha = 1 / 3) + facet_wrap(~continente) Parece que hemos perdido algunos patrones suaves. También hay algo interesante pasando en África: vemos algunos residuos muy grandes lo que sugiere que nuestro modelo no está ajustando muy bien. Exploraremos más eso en la próxima sección, atacando el problema desde un ángulo un poco diferente. 15.2.4 Calidad del modelo En lugar de examinar los residuos del modelo, podríamos examinar algunas medidas generales de la calidad del modelo. Aprendiste cómo calcular algunas medidas específicas en el capítulo anterior. Aquí mostraremos un enfoque diferente usando el paquete broom. El paquete broom provee un conjunto de funciones generales para transformar modelos en datos ordenados. Aquí utilizaremos broom::glance() (del inglés vistazo) para extraer algunas métricas de la calidad del modelo. Si lo aplicamos a un modelo, obtenemos un data frame con una única fila: broom::glance(nz_mod) #&gt; # A tibble: 1 x 11 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.954 0.949 0.804 205. 5.41e-8 2 -13.3 32.6 34.1 #&gt; # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Podemos usar mutate() y unnest() para crear un data frame con una fila por cada país: por_pais %&gt;% mutate(glance = map(modelo, broom::glance)) %&gt;% unnest(glance) #&gt; # A tibble: 142 x 16 #&gt; pais continente data modelo residuos r.squared adj.r.squared sigma #&gt; &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afga… Asia &lt;tib… &lt;lm&gt; &lt;tibble… 0.948 0.942 1.22 #&gt; 2 Alba… Europa &lt;tib… &lt;lm&gt; &lt;tibble… 0.911 0.902 1.98 #&gt; 3 Arge… África &lt;tib… &lt;lm&gt; &lt;tibble… 0.985 0.984 1.32 #&gt; 4 Ango… África &lt;tib… &lt;lm&gt; &lt;tibble… 0.888 0.877 1.41 #&gt; 5 Arge… Américas &lt;tib… &lt;lm&gt; &lt;tibble… 0.996 0.995 0.292 #&gt; 6 Aust… Oceanía &lt;tib… &lt;lm&gt; &lt;tibble… 0.980 0.978 0.621 #&gt; # … with 136 more rows, and 8 more variables: statistic &lt;dbl&gt;, #&gt; # p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, #&gt; # deviance &lt;dbl&gt;, df.residual &lt;int&gt; Este no es exactamente el output que queremos, porque aún incluye todas las columnas que son una lista. Éste es el comportamiento por defecto cuando unnest() trabaja sobre data frames con una única fila. Para suprimir esas columnas usamos .drop = TRUE (drop — del inglés descartar): glance &lt;- por_pais %&gt;% mutate(glance = map(modelo, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) glance #&gt; # A tibble: 142 x 13 #&gt; pais continente r.squared adj.r.squared sigma statistic p.value df #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Afga… Asia 0.948 0.942 1.22 181. 9.84e- 8 2 #&gt; 2 Alba… Europa 0.911 0.902 1.98 102. 1.46e- 6 2 #&gt; 3 Arge… África 0.985 0.984 1.32 662. 1.81e-10 2 #&gt; 4 Ango… África 0.888 0.877 1.41 79.1 4.59e- 6 2 #&gt; 5 Arge… Américas 0.996 0.995 0.292 2246. 4.22e-13 2 #&gt; 6 Aust… Oceanía 0.980 0.978 0.621 481. 8.67e-10 2 #&gt; # … with 136 more rows, and 5 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, #&gt; # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; (Presta atención a las variables que no se imprimieron: hay mucha información útil allí.) Con este data frame, podemos empezar a buscar modelos que no se ajustan bien: glance %&gt;% arrange(r.squared) #&gt; # A tibble: 142 x 13 #&gt; pais continente r.squared adj.r.squared sigma statistic p.value df #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Ruan… África 0.0172 -0.0811 6.56 0.175 0.685 2 #&gt; 2 Bots… África 0.0340 -0.0626 6.11 0.352 0.566 2 #&gt; 3 Zimb… África 0.0562 -0.0381 7.21 0.596 0.458 2 #&gt; 4 Zamb… África 0.0598 -0.0342 4.53 0.636 0.444 2 #&gt; 5 Swaz… África 0.0682 -0.0250 6.64 0.732 0.412 2 #&gt; 6 Leso… África 0.0849 -0.00666 5.93 0.927 0.358 2 #&gt; # … with 136 more rows, and 5 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, #&gt; # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; Los peores modelos parecen estar todos en África. Vamos a chequear ésto con un gráfico. Tenemos un número relativamente chico de observaciones y una variable discreta, así que geom_jitter() (TODO: en inglés jitter es temblar o algo similar, pero no sé si estaría bien poner acá eso porque no es exactamente eso…) es efectiva: glance %&gt;% ggplot(aes(continente, r.squared)) + geom_jitter(width = 0.5) Podríamos quitar los países con un \\(R^2\\) particularmente malo y graficar los datos: mal_ajuste &lt;- filter(glance, r.squared &lt; 0.25) paises %&gt;% semi_join(mal_ajuste, by = &quot;pais&quot;) %&gt;% ggplot(aes(anio, esperanza_de_vida, colour = pais)) + geom_line() Vemos dos efectos principales aquí: las tragedias de la epidemia de VIH/SIDA y el genocidio de Ruanda. 15.2.5 Ejercicios Una tendencia lineal parece ser demasiado simple para la tendencia general. ¿Puedes hacerlo mejor con un polinomio cuadrático? ¿Cómo puedes interpretar el coeficiente del término cuadrático? (Pista: puedes querer transformar year para que tenga media cero.) Explora otros métodos para visualizar la distribución del \\(R^2\\) por continente. Puedes querer probar el paquete ggbeeswarm, que provee métodos similares para evitar superposiciones como jitter, pero usa métodos determinísticos. Para crear el último gráfico (mostrando los datos para los países con los peores ajustes del modelo), precisamos dos pasos: creamos un data frame con una fila por país y después hicimos un semi-join (del inglés semi juntar) (TODO: deberíamos aclarar algo?) al conjunto de datos original. Es posible evitar este join si usamos unnest() en lugar de unnest(.drop = TRUE). ¿Cómo? 15.3 Columnas-lista Ahora que has visto un flujo de trabajo básico para manejar muchos modelos, vamos a sumergirnos en algunos detalles. En esta sección, exploraremos en más detalle la estructura de datos columna-lista. Solo recientemente es que he comenzado a apreciar realmente la idea de la columna-lista. Esta estructura está implícita en la definición de data frame: un data frame es una lista nombrada de vectores de igual largo. Una lista es un vector, así que siempre ha sido legítimo usar una lista como una columna de un data frame. Sin embargo, R base no hace las cosas fáciles para crear columnas-lista, y data.frame() trata a la lista como una lista de columnas: data.frame(x = list(1:3, 3:5)) #&gt; x.1.3 x.3.5 #&gt; 1 1 3 #&gt; 2 2 4 #&gt; 3 3 5 Puedes prevenir que data.frame() haga esto con I(), pero el resultado no se imprime particularmente bien: data.frame( x = I(list(1:3, 3:5)), y = c(&quot;1, 2&quot;, &quot;3, 4, 5&quot;) ) #&gt; x y #&gt; 1 1, 2, 3 1, 2 #&gt; 2 3, 4, 5 3, 4, 5 Tibble mitiga este problema siendo más perezoso (TODO: lazier) (tibble() no modifica sus inputs) y proporcionando un mejor método de impresión: tibble( x = list(1:3, 3:5), y = c(&quot;1, 2&quot;, &quot;3, 4, 5&quot;) ) #&gt; # A tibble: 2 x 2 #&gt; x y #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;int [3]&gt; 1, 2 #&gt; 2 &lt;int [3]&gt; 3, 4, 5 Es incluso más fácil con tribble() ya que automáticamente puede interpretar que necesitas una lista: tribble( ~x, ~y, 1:3, &quot;1, 2&quot;, 3:5, &quot;3, 4, 5&quot; ) #&gt; # A tibble: 2 x 2 #&gt; x y #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;int [3]&gt; 1, 2 #&gt; 2 &lt;int [3]&gt; 3, 4, 5 Columnas-lista son usualmente más útiles como estructuras de datos intermedias. Es difícil trabajar con ellas directamente, porque la mayoría de las funciones de R trabajan con vectores atómicos o data frames, pero la ventaja de mantener ítems relacionados juntos en un data frame hace que valga la pena un poco de molestia. Generalmente hay tres partes de un pipeline (del inglés tubería) efectivo de columnas-lista: Creas la columna-lista usando uno entre nest(), summarise() + list(), o mutate() + una función map, como se describió en Creando columnas-lista. Creas otra columna-lista intermedia transformando columnas lista existentes con map(), map2() o pmap(). Por ejemplo, en el caso de estudio de arriba, creamos una columna-lista de modelos transformando una columna-lista de data frames. Simplificas la columna-lista de vuelta en un data frame o vector atómico, como se describió en Simplificando columnas-lista. 15.4 Creando columnas-lista Típicamente, no crearás columnas-lista con tibble(). En cambio, las crearás a partir de columnas regulares, usando uno de tres métodos: Con tidyr::nest() para convertir un data frame agrupado en uno anidado donde tengas columnas-lista de data frames. Con mutate() y funciones vectorizadas que retornan una lista. Con summarise() y funciones de resumen que retornan múltiples resultados. Alternativamente, podrías crearlas a partir de una lista nombrada, usando tibble::enframe(). Generalmente, cuando creas columnas-lista, debes asegurarte de que sean homogéneas: cada elemento debe contener el mismo tipo de cosa. No hay chequeos para asegurarte de que sea así, pero si usas purrr y recuerdas lo que aprendiste sobre funciones de tipo estable (TODO: type-stable functions), encontrarás que eso pasa naturalmente. 15.4.1 Con anidación nest() crea un data frame anidado, que es un data frame con una columna-lista de data frames. En un data frame anidado cada fila es una meta-observación: las otras columnas son variables que definen la observación (como país y continente arriba), y la columna-lista de data frames tiene las observaciones individuales que construyen la meta-observación. Hay dos formas de usar nest(). Hasta ahora has visto cómo usarlo con un data frame agrupado. Cuando se aplica a un data frame agrupado, nest() mantiene las columnas que agrupan tal cual, y envuelve todo lo demás en la columna-lista: paises %&gt;% group_by(pais, continente) %&gt;% nest() #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistan Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows También lo puedes usar en un data frame no agrupado, especificando cuáles columnas quieres anidar: paises %&gt;% nest(anio:pib_per_capita) #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistan Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows 15.4.2 A partir de funciones vectorizadas Algunas funciones útiles toman un vector atómico y retornan una lista. Por ejemplo, en strings (TODO: chequear cómo se llama el capítulo en español) aprendiste stringr::str_split() que toma un vector de caracteres y retorna una lista de vectores de caracteres. Si lo usas dentro de mutate (TODO: no sé si ponerlo en cursiva o dejarlo como el original), obtendrás una columna-lista: df &lt;- tribble( ~x1, &quot;a,b,c&quot;, &quot;d,e,f,g&quot; ) df %&gt;% mutate(x2 = stringr::str_split(x1, &quot;,&quot;)) #&gt; # A tibble: 2 x 2 #&gt; x1 x2 #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a,b,c &lt;chr [3]&gt; #&gt; 2 d,e,f,g &lt;chr [4]&gt; unnest() sabe cómo manejar estas listas de vectores: df %&gt;% mutate(x2 = stringr::str_split(x1, &quot;,&quot;)) %&gt;% unnest() #&gt; # A tibble: 7 x 2 #&gt; x1 x2 #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a,b,c a #&gt; 2 a,b,c b #&gt; 3 a,b,c c #&gt; 4 d,e,f,g d #&gt; 5 d,e,f,g e #&gt; 6 d,e,f,g f #&gt; # … with 1 more row (Si usas mucho este patrón, asegúrate de chequear tidyr::separate_rows() (del inglés separar filas) que es un wrapper (TODO: cuando esté el capítulo de Joshua poner explicación) alrededor de este patrón común). Otro ejemplo de este patrón es usar map(), map2(), pmap() de purrr. Por ejemplo, podríamos tomar el ejemplo final de Invoking different functions (TODO: chequear nombre en español) y reescribirlo usando mutate(): sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sims = invoke_map(f, params, n = 10)) #&gt; # A tibble: 3 x 3 #&gt; f params sims #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 runif &lt;list [2]&gt; &lt;dbl [10]&gt; #&gt; 2 rnorm &lt;list [1]&gt; &lt;dbl [10]&gt; #&gt; 3 rpois &lt;list [1]&gt; &lt;int [10]&gt; Nota que técnicamente sim no es homogénea porque contiene vectores con tipo de datos dobles así como enteros (TODO: both double and integer vectors). Sin embargo, es probable que ésto no cause muchos problemas porque ambos vectores son numéricos. 15.4.3 A partir de medidas de resumen con más de un valor Una restricción de summarise() es que solo funciona con funciones de resumen que retornan un único valor. Eso significa que no puedes usarlo con funciones como quantile() que retorna un vector de largo arbitrario: mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = quantile(millas)) #&gt; Error: Column `q` must be length 1 (a summary value), not 5 Sin embargo, puedes envolver el resultado en una lista! Esto obedece el contrato de summarise(), porque cada resumen ahora es una lista (un vector) de largo 1. mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = list(quantile(millas))) #&gt; # A tibble: 3 x 2 #&gt; cilindros q #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 4 &lt;dbl [5]&gt; #&gt; 2 6 &lt;dbl [5]&gt; #&gt; 3 8 &lt;dbl [5]&gt; Para producir resultados útiles con unnest, también necesitarás capturar las probabilidades: probs &lt;- c(0.01, 0.25, 0.5, 0.75, 0.99) mtautos %&gt;% group_by(cilindros) %&gt;% summarise(p = list(probs), q = list(quantile(millas, probs))) %&gt;% unnest() #&gt; # A tibble: 15 x 3 #&gt; cilindros p q #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 0.01 21.4 #&gt; 2 4 0.25 22.8 #&gt; 3 4 0.5 26 #&gt; 4 4 0.75 30.4 #&gt; 5 4 0.99 33.8 #&gt; 6 6 0.01 17.8 #&gt; # … with 9 more rows 15.4.4 A partir de una lista nombrada Data frames con columnas-lista proveen una solución a un problema común: ¿qué haces si quieres iterar sobre el contenido de una lista y también sobre sus elementos? En lugar de tratar de juntar todo en un único objeto, usualmente es más fácil hacer un data frame: una columna puede contener los elementos y otra columna la lista. Una forma fácil de crear un data frame como éste desde una lista es tibble::enframe(). x &lt;- list( a = 1:5, b = 3:4, c = 5:6 ) df &lt;- enframe(x) df #&gt; # A tibble: 3 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a &lt;int [5]&gt; #&gt; 2 b &lt;int [2]&gt; #&gt; 3 c &lt;int [2]&gt; La ventaja de esta estructura es que se generaliza de una forma relativamente sencilla - los nombres son útiles si tienes como metadata vectores de caracteres, pero no ayudan para otros tipos de datos o para múltiples vectores. Ahora, si quieres iterar sobre los nombres y valores en paralelo, puedes usar map2(): df %&gt;% mutate( smry = map2_chr(name, value, ~ stringr::str_c(.x, &quot;: &quot;, .y[1])) ) #&gt; # A tibble: 3 x 3 #&gt; name value smry #&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 a &lt;int [5]&gt; a: 1 #&gt; 2 b &lt;int [2]&gt; b: 3 #&gt; 3 c &lt;int [2]&gt; c: 5 15.4.5 Ejercicios Lista todas las funciones en las que puedas pensar que tomen como input un vector atómico y retornen una lista. Piensa en funciones de resumen útiles que, como quantile(), retornen múltiples valores. ¿Qué es lo que falta en el siguiente data frame? ¿Cómo quantile() retorna eso que falta? ¿Por qué eso no es tan útil aquí? mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = list(quantile(millas))) %&gt;% unnest() #&gt; # A tibble: 15 x 2 #&gt; cilindros q #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 21.4 #&gt; 2 4 22.8 #&gt; 3 4 26 #&gt; 4 4 30.4 #&gt; 5 4 33.9 #&gt; 6 6 17.8 #&gt; # … with 9 more rows ¿Qué hace este código? ¿Por qué podría ser útil? mtautos %&gt;% group_by(cilindros) %&gt;% summarise_each(funs(list)) 15.5 Simplificando columnas-lista Para aplicar las técnicas de manipulación de datos y visualización que has aprendido en este libro, necesitarás simplificar la columna-lista de vuelta a una columna regular (un vector atómico), o conjunto de columnas. La técnica que usarás para volver a una estructura más sencilla depende de si quieres un único valor por elemento, o múltiples valores. Si quieres un único valor, usa mutate() con map_lgl(), map_int(), map_dbl(), y map_chr() para crear un vector atómico. Si quieres varios valores, usa unnest() para convertir columnas-lista de vuelta a columnas regulares, repitiendo las filas tantas veces como sea necesario. Estas técnicas están descritas con más detalle abajo. 15.5.1 Lista a vector Si puedes reducir tu columna lista a un vector atómico entonces será una columna regular. Por ejemplo, siempre puedes resumir un objeto con su tipo y largo, por lo que este código funcionará sin importar cuál tipo de columna-lista tengas: df &lt;- tribble( ~x, letters[1:5], 1:3, runif(5) ) df %&gt;% mutate( tipo = map_chr(x, typeof), largo = map_int(x, length) ) #&gt; # A tibble: 3 x 3 #&gt; x tipo largo #&gt; &lt;list&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;chr [5]&gt; character 5 #&gt; 2 &lt;int [3]&gt; integer 3 #&gt; 3 &lt;dbl [5]&gt; double 5 Ésta es la misma información básica que obtienes del método por defecto de imprimir tbl (TODO: esto se traduce?), pero ahora lo puedes usar para filtrar. Es una técnica útil si tienes listas heterogéneas, y quieres remover las partes que no te sirven. No te olvides de los atajos de map_*() - puedes usar map_chr(x, &quot;manzana&quot;) para extraer la cadena de caracteres almacenada en manzana para cada elemento de x. Ésto es útil para separar listas anidadas en columnas regulares. Usa el argumento .null para proveer un valor para usar si el elemento es un valor perdido (missing) (TODO: traducimos missing?) (en lugar de retornar NULL): df &lt;- tribble( ~x, list(a = 1, b = 2), list(a = 2, c = 4) ) df %&gt;% mutate( a = map_dbl(x, &quot;a&quot;), b = map_dbl(x, &quot;b&quot;, .null = NA_real_) ) #&gt; # A tibble: 2 x 3 #&gt; x a b #&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &lt;list [2]&gt; 1 2 #&gt; 2 &lt;list [2]&gt; 2 NA 15.5.2 Desanidando unnest() trabaja repitiendo la columna regular una vez para cada elemento de la columna-lista. Por ejemplo, en el siguiente ejemplo sencillo repetimos la primera fila 4 veces (porque el primer elemento de y tiene largo cuatro), y la segunda fila una vez: tibble(x = 1:2, y = list(1:4, 1)) %&gt;% unnest(y) #&gt; # A tibble: 5 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 2 #&gt; 3 1 3 #&gt; 4 1 4 #&gt; 5 2 1 Esto significa que no puedes simultáneamente desanidar dos columnas que contengan un número diferente de elementos: # Funciona, porque y y z tienen el mismo número de elementos en # cada fila df1 &lt;- tribble( ~x, ~y, ~z, 1, c(&quot;a&quot;, &quot;b&quot;), 1:2, 2, &quot;c&quot;, 3 ) df1 #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [2]&gt; &lt;int [2]&gt; #&gt; 2 2 &lt;chr [1]&gt; &lt;dbl [1]&gt; df1 %&gt;% unnest(y, z) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 1 #&gt; 2 1 b 2 #&gt; 3 2 c 3 # No funciona porque y y z tienen un número diferente de elementos df2 &lt;- tribble( ~x, ~y, ~z, 1, &quot;a&quot;, 1:2, 2, c(&quot;b&quot;, &quot;c&quot;), 3 ) df2 #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; &lt;int [2]&gt; #&gt; 2 2 &lt;chr [2]&gt; &lt;dbl [1]&gt; df2 %&gt;% unnest(y, z) #&gt; All nested columns must have the same number of elements. El mismo principio aplica al desanidar columnas-lista de data frames. Puedes desanidar múltiples columnas-lista siempre que todos los data frames de cada fila tengan la misma cantidad de filas. 15.5.3 Ejercicios ¿Por qué podría ser útil la función lengths() para crear columnas de vectores atómicos a partir de columnas-lista? Lista los tipos de vectores más comúnes que se encuentran en un data frame. ¿Qué hace que las listas sean diferentes? 15.6 Haciendo datos ordenados con broom El paquete broom provee tres herramientas generales para transformar modelos en en data frames ordenados: broom::glance(modelo) retorna una fila para cada modelo. Cada columna tiene una medida de resumen del modelo: o bien una medida de la calidad del modelo, o complejidad, o una combinación de ambos. broom::tidy(modelo) retorna una fila por cada coeficiente en el modelo. Cada columna brinda información acerca de la estimación o su variabilidad. broom::augment(modelo, datos) retorna una fila por cada fila en datos, agregando valores adicionales como residuos, y estadísticos de influencia. "],
["model-intro.html", "16 Introducción 16.1 Generación de hipótesis vs. confirmación de hipótesis", " 16 Introducción Ahora que estás equipado con herramientas de programación poderosas, finalmente podemos regresar a modelar. Utilizarás tus nuevas herramientas de domador de datos y de programación para ajustar muchos modelos y entender como funcionan. El foco de este libro es la exploración, no la confirmación o la inferencia formal. Pero aprenderás un par de herramientas básicas que te ayudarán a entender la variación en tus modelos. El objetivo de un modelo es proveer un resumen simple y de baja dimensionalidad sobre un conjunto de datos. Idealmente, el modelo capturará “señales” verdaderas (por ejemplo patrones generados por el fenómeno de interés) e ignorará el “ruido” (es decir, variaciones aleatorias que no nos interesan). Sólo vamos a cubrir modelos “predictivos”, los cuales, como su nombre sugiere, generan predicciones. Hay otro tipo de modelo que no vamos a discutir: los modelos de “descubrimiento de datos”. Estos modelos no hacen predicciones, si no que ayudan a descubrir relaciones interesantes entre tus datos. (Estos dos tipos de modelos suelen ser llamados supervisados y no supervisados, pero no creo que esa terminología sea particularmente esclarecedora.) Este libro no te dará un entendimiento profundo de la teoría matemática que subyace a los modelos. Sin embargo, construirá tu intuición sobre cómo funcionan los modelos estadísticos y te proporcionará una familia de herramientas útiles que te permitirán utilizar modelos para comprender mejor tus datos: En conceptos básicos, aprenderás cómo los modelos funcionan mecánicamente, centrándonos en la importante familia de modelos lineales. Aprenderás herramientas generales para obtener información sobre lo que un modelo predictivo te dice sobre tus datos, centrándonos en conjuntos de datos simples simulados. En [construcción de modelos], aprenderás cómo usar modelos para extraer patrones conocidos en datos reales. Una vez que reconozcas un patrón importante, es útil hacerlo explícito en un modelo, porque entonces podrás ver más fácilmente aquellas señales sutiles que quedan en tus datos. En muchos modelos, aprenderás cómo usar muchos modelos simples para ayudar a comprender conjuntos de datos complejos. Esta es una técnica poderosa, pero para acceder necesitarás combinar herramientas de modelado y de programación. Deliberadamente dejamos fuera del capítulo las explicaciones de herramientas para evaluar cuantitativamente los modelos porque dicha evaluación precisa requiere conocer un par de grandes ideas que simplemente no tenemos espacio para cubrir aquí. Por ahora, dependerás de la evaluación cualitativa y de tu escepticismo natural. En [Aprender más sobre los modelos], te indicaremos otros recursos donde podrás seguir aprendiendo. 16.1 Generación de hipótesis vs. confirmación de hipótesis En este libro vamos a usar los modelos como una herramienta para la exploración, completando la tríada de las herramientas para EDA que se introdujeron en la Parte 1. Los modelos no se suelen enseñar de esta manera, pero como verás, son herramientas importantes para la exploración. Tradicionalmente, el enfoque del modelado está en la inferencia, es decir, para confirmar que una hipótesis es verdadera; hacerlo correctamente no es complicado, pero si difícil. Hay un par de ideas que debes comprender para poder hacer la inferencia correctamente: Cada observación puede ser utilizada para exploración o para confirmación, pero no para ambas. Puedes usar una observación tantas veces como quieras para la exploración, pero solo una vez para confirmación. En el instante que usaste una observación dos veces, pasaste de confirmar a explorar. Esto es necesario porque, para confirmar una hipótesis, debes usar datos independientes de los datos utilizados para generarla. De lo contrario, serás demasiado optimista. No hay absolutamente nada de malo en la exploración, pero nunca debes vender un análisis exploratorio como análisis confirmatorio porque es fundamentalmente erróneo. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: Si realmente quieres realizar un análisis confirmatorio, un enfoque es dividir los datos en tres partes antes de comenzar el análisis: El 60% de los datos van a un conjunto de entrenamiento (del inglés, training) o exploración. Puedes hacer lo que quieras con estos datos, desde visualizarlo a ajustar toneladas de modelos. 20% va a un conjunto de consulta (del inglés, query). Se puede usar esta información para comparar modelos o hacer visualizaciones a mano, pero no está permitido usarlo como parte de un proceso automatizado. El otro 20% se reserva para un conjunto de validación (del inglés, test). Sólo se pueden usar estos datos UNA VEZ, para probar tu modelo final. Esta partición de los datos, te permite explorar con los datos de entrenamiento, generando ocasionalmente hipótesis candidatas que se verifican con el conjunto de consultas. Cuando estés seguro de tener el modelo correcto, se verifica una vez con los datos del conjunto de validación. (Se debe tener en cuenta que, incluso cuando realice modelado confirmatorio, se necesita hacer EDA. Si no se realiza ninguna EDA, los problemas de calidad que tengan los datos quedarán ocultos). "],
["pipes.html", "17 Pipes 17.1 Introducción 17.2 Alternativas a los pipes 17.3 Cuando no usar el pipe 17.4 Otras herramientas de magrittr", " 17 Pipes 17.1 Introducción Un pipe es una herramienta poderosa para claramente expresar una secuencia de operaciones múltiples. Hasta aquí, has venido usándolas sin saber cómo funcionan, o cuales son las alternativas. Ahora, en este capítulo, es tiempo de explorar las Pipes en más detalle. Aprenderás las alternativas a las Pipes, cuando no deberías usar las pipes y algunas herramientas relacionadas útiles. 17.1.1 Prerequisitos Un pipe, %&gt;%, viene del paquete magrittr de Stefan Milton Bache. Los paquetes en el tidyverse cargan %&gt;% automáticamente, así que no cargaras magrittr explícitamente. Aquí, sin embargo, nos enfocamos en la tubería, y no estamos cargando otros paquetes, por lo que nosotros lo cargaremos explícitamente library(magrittr) library(datos) 17.2 Alternativas a los pipes El objetivo de un pipe es ayudarte a escribir código en una manera que sea más fácil de leer y entender. Para ver porque un pipe es tan útil, vamos a explorar diferentes formas de escribir el mismo código. Usemos el código para contar una historia acerca de un pequeño conejito llamado Foo Foo: Pequeño conejito Foo Foo Fue saltando por el bosque Recogiendo ratones del campo Y golpeándolos en la cabeza Este es un poema popular para los niños que se acompaña mediante gesticulación con las manos. Empezaremos por definir un objeto que representa un pequeño conejito Foo Foo: foo_foo &lt;- pequeño_conejito() Y usaremos una función para cada verbo: saltar(), recoger(), and golpear(). Usando este objeto y estos verbos, hay (al menos) cuatro maneras en las que podemos volver a contar la historia en código: Guardar cada paso intermedio como un nuevo objeto. Sobreescribir el objeto original muchas veces. Componer las funciones. Usar un pipe. Trabajaremos a través de cada enfoque, mostrándote el código y hablando de las ventajas y desventajas. 17.2.1 Pasos intermedios El enfoque más sencillo es guardar cada paso como un nuevo objeto: foo_foo_1 &lt;- saltar(foo_foo, a_traves_del = bosque) foo_foo_2 &lt;- recoger(foo_foo_1, up = ratones_del_campo) foo_foo_3 &lt;- golpear(foo_foo_2, en_la = cabeza) La máxima desventaja de esta manera es la obligación de nombrar cada paso intermedio. Si hay nombres naturales, es una buena idea, y deberías hacerlo. Pero muchas veces, como en este ejemplo, no hay nombres naturales, y agregando sufijos numéricos para hacer los nombres únicos. Esto conduce a dos problemas: El código está abarrotado con nombres poco importantes. Hay que incrementar cuidadosamente el sufijo en cada línea. Cada vez que escribo código como este, invariablemente uso el número incorrecto en una línea y luego pierdo 10 minutos rascándome la cabeza tratándome de dar cuenta que hay de malo con mi código. Probablemente te preocupes que esta forma cree muchas copias de tus datos y ocupe demasiada memoria. Sorprendentemente, este no es el caso. Primero, ten en cuenta que preocuparse proactivamente acerca de la memoria no es una manera útil de invertir tu tiempo: preocupate acerca de ello cuando se convierta en un problema (ej. cuando te quedes sin memoria), no antes. En segundo lugar, R no es estúpido, compartirá las columnas a lo largo de los marcos de datos, cuando sea posible. Veamos a una tubería de manipulación de datos cuando agregamos un nueva columna al ggplot2::diamantes: diamantes2 &lt;- diamantes %&gt;% dplyr::mutate(precio_por_quilate = precio / quilate) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang pryr::object_size(diamantes) #&gt; Registered S3 method overwritten by &#39;pryr&#39;: #&gt; method from #&gt; print.bytes Rcpp #&gt; 3.46 MB pryr::object_size(diamantes2) #&gt; 3.89 MB pryr::object_size(diamantes, diamantes2) #&gt; 3.89 MB pryr::object_size() ofrece la cantidad de memoria ocupada por todos los argumentos. Este resultado puede parecer contraintuitivo al principio: diamantes ocupa 3.46 MB, diamantes2 ocupa 3.89 MB, diamantes y diamantes2 juntos ocupan 3.89 MB! ¿Cómo funciona esto? Bien, diamantes2 tiene 10 columnas en común con diamantes: no hay necesidad de duplicar los datos, así que ambos dataframes tienen variables en común. Estas variables solo serán copiadas si tu modificas una de ellas. En el siguiente ejemplo, nosotros modificamos un solo valor en diamantes$quilates. Esto significa que la variable carat no será más compartida entre dos dataframes, y una copia debe ser realizada. El tamaño de cada dataframe no se cambia, pero el tamaño colectivo se incrementa: diamantes$quilates[1] &lt;- NA #&gt; Warning: Unknown or uninitialised column: &#39;quilates&#39;. pryr::object_size(diamantes) #&gt; 3.67 MB pryr::object_size(diamantes2) #&gt; 3.89 MB pryr::object_size(diamantes, diamantes2) #&gt; 4.1 MB (Notar que usamos pryr::object_size() aquí, no en la construcción object.size(). object.size() solo toma un solo objeto así que puede computar como los datos son compartidos a través de múltiples objetos.) 17.2.2 Sobreescribir el original En vez de crear objetos en cada paso intermedio, nosotros podemos sobreescribir el objeto original: foo_foo &lt;- hop(foo_foo, a_traves_del = bosque) foo_foo &lt;- scoop(foo_foo, arriba = ratones_del_campo) foo_foo &lt;- bop(foo_foo, en_la = cabeza) Esto es menos tipeo (y menos pensamiento), así que es menos probable que cometas errores. Sin embargo, hay dos problemas: Debugging es doloroso: si cometes un error vas a necesitar correr de nuevo el pipeline desde el principio. La repetición de un objeto siendo transformado (hemos escrito foo_foo 6 veces!) oscurece lo que está siendo cambiado en cada línea. 17.2.3 Composición de funciones Otro enfoque es abandonar la asignación y encadenar las llamadas a las funciones juntas: saltar( recoger( golpear(bla_bla, por_el = bosque), arriba = raton_de_campo ), en = la_cabeza ) Aquí está la desventaja es que se debe leer de dentro hacia afuera, de derecha a izquierda, y que los argumentos terminan separados dagwood sandwhich). En resumen, este código es difícil para un ser humano. 17.2.4 Uso de pipe Finalmente, podemos usar pipe: foo_foo %&gt;% saltar(a_través_del = bosque) %&gt;% recoger(arriba = ratones_campo) %&gt;% golpear(en = la_cabeza) Esta es mi forma preferida, ya que se enfoca en los verbos, no en los sustantivos. Puedes leer esto como una serie de composición de funciones como un set de acciones imperativas. Foo salta, luego recoge y luego golpea. La desventaja, por supuesto, es que necesitas estar familiarizado con el uso de los pipes. Si nunca has visto %&gt;% antes, no tendras idea acerca de lo que realiza el código. Afortunadamente, la mayoría de la gente entiende la idea facilmente, así que cuando compartes tu código con otros que no están familiarizados con los pipes, tu pueden enseñarles facilmente. El pipe trabaja realizando una “transformación léxica”: detrás de escena, magrittr reensambla el código en el pipe a una forma que funciona por sobreescritura en un objeto intermedio. Cuando se ejecuta un pipe como el de arriba, magrittr hace algo como esto: mi_pipe &lt;- function(.) { . &lt;- saltar(., a_traves_del = bosque) . &lt;- recoger(., arriba = ratones_campo) golpear(., en = la_cabeza) } mi_pipe(foo_foo) Esto significa que un pipe no trabajara para dos clases de funciones: 1. Funciones que usan el entorno actual. Por ejemplo, asignar()creará una nueva variable con el nombre dado en el entorno actual: assign(&quot;x&quot;, 10) x #&gt; [1] 10 &quot;x&quot; %&gt;% assign(100) x #&gt; [1] 10 El uso de la asignación con el pipe no funciona porque lo asigna a un entorno temporario usado por %&gt;%. Si quieres usar la asignación con pipe, debes explicitar el entorno: env &lt;- environment() &quot;x&quot; %&gt;% assign(100, envir = env) x #&gt; [1] 100 Otras funciones con este problema incluyen get() y load(). Funciones que usan una evaluación perezosa. En R, los argumentos de las funciones son solamente computados cuando la función los usa, no antes de llamar a la función. El pipe computa cada elemento de a turno, así que no puedes confiar en este comportamiento. Un lugar para este problema es tryCatch(), que te permite capturar y manejar errores: tryCatch(stop(&quot;!&quot;), error = function(e) &quot;An error&quot;) #&gt; [1] &quot;An error&quot; stop(&quot;!&quot;) %&gt;% tryCatch(error = function(e) &quot;An error&quot;) #&gt; Error in eval(lhs, parent, parent): ! Hay una relativa gran cantidad de funciones con este comportamiento, incluyendo a try(), suppressMessages(), y suppressWarnings() en R base. 17.3 Cuando no usar el pipe El pipe es una herramienta poderosa, pero no es la única herramienta a tu disposición, ¡y no soluciona ningún problema! Los pipes son mayoritariamente usados para reescribir una secuencia lineal bastante corta de operaciones. Pienso que deberías buscar otra herramienta cuando: Tus pipes no son más largas que (digamos) 10 pasos. En ese caso, creamos objetos intermedios con nombres significativos. Esto hará el debugging más fácil, porque puede más facilmente chequear los resultados intermedios, y hace más simple entender tu código, porque los nombres de las variables pueden ayudar a comunicar la intención. Tienes múltiples inputs y outputs. Si no hay un objeto primario para ser transformado, pero sí dos o más objetos siendo combinados juntos, no uses el pipe. Si estas empezando a pensar acerca de un gráfico directo con una estructura compleja de dependencia. Los pipes son fundamentalmente lineales y expresan relaciones complejas que típicamente llevarán a un código confuso. 17.4 Otras herramientas de magrittr Todos los paquetes en tidyverse automaticamente harán %&gt;% disponible para tí, así que normalmente no se carga magrittr explicitamente. De todas formas, hay otras herramientas útiles dentro de magrittr que podrías querer utilizar: Cuando se trabaja en un pipe más complejo, es a veces más útil llamar a una función por sus efectos secundarios. Tal vez quieras imprimir el objeto actual, o graficarlo, o guardarlo en el disco. Muchas veces, estas funciones no devuelven nada, efectivamente terminando el pipe. Para trabajar en torno a este problema, puedes usar el pipe “T”. %T&gt;%trabaja igual que %&gt;% excepto que este devuelve el lado izquierdo en vez del lado derecho. Se lo llama “T” porque literalmente tiene la forma de una T. rnorm(100) %&gt;% matrix(ncol = 2) %&gt;% plot() %&gt;% str() #&gt; NULL rnorm(100) %&gt;% matrix(ncol = 2) %T&gt;% plot() %&gt;% str() #&gt; num [1:50, 1:2] -0.387 -0.785 -1.057 -0.796 -1.756 ... Si estas trabajando con funciones que no están basados en una API (ej., tu pasas vectores individuales, no un dataframe o expresiones a ser evaluadas en el contexto de un dataframe), pueden encontrar %$% útil. El operador explota las variables en un dataframe así que te puedes referir a ellos explícitamente. Esto es útil cuando se trabaja con muchas funciones en R base: mtautos %$% cor(cilindrada, millas) #&gt; [1] -0.848 Para asignaciones magrittr provee el operador %&lt;&gt;% te permite reemplazar el código como: mtautos &lt;- mtautos %&gt;% transform(cilindros = cilindros * 2) con mtautos %&lt;&gt;% transform(cilindros = cilindros * 2) No soy partidario de este operador porque pienso que una asignación es una operación especial que siempre debe ser clara cuando sucede. En mi opinión, un poco de duplicación (ej repetir el nombre de un objeto dos veces) está bien para lograr hacerlo más explícito. "],
["programar-intro.html", "18 Introducción 18.1 Aprendiendo más", " 18 Introducción En esta parte del libro, mejorarás tus aptitudes de programación. La programación es una habilidad que cruza transversalmente todos los trabajos de ciencia de datos: deberás usar una computadora para hacer ciencia de datos, no puedes hacerlo sólo con tu cabeza o con lápiz y papel. Programar produce código, y el código es una herramienta de comunicación. Obviamente el código le dice a la computadora qué es lo que quieres que haga, pero también comunica el significado a otros humanos. Es importante pensar el código como un medio de comunicación, porque todo proyecto que realices es esencialmente colaborativo. Aún cuando no estés trabajando con otras personas, seguramente en el futuro trabajarás con ese mismo código. Por eso es necesario escribir código claro, para que otros (o tú en el futuro) puedan entender por qué encaraste un análisis de la manera que lo hiciste. Esto significa que mejorando la programación también mejorarás la comunicación. Pasado el tiempo, querrás que tu código resulte no solo fácil de escribir, sino también fácil de leer para otros. Escribir código es similar, en muchas formas, a escribir prosa. Un paralelismo que encuentro particularmente útil es que en ambos casos reescribir es clave para la claridad. La primera expresión de tus ideas difícilmente sea clara, y posiblemente necesitarás reescribir muchas veces. Después de resolver el problema de análisis de datos, en general es mejor mirar tu código y pensar si es obvio o no lo que has hecho. Si utilizas un poco de tiempo reescribiendo tu código mientras las ideas están frescas, puedes ganar mucho tiempo después, cuando necesites recrear el código que hiciste. Esto no significa que debas reescribir todas las funciones: necesitas equilibrar qué es lo que tienes que mejorar ahora para ganar tiempo a largo plazo. (Pero cuanto más reescribas tus funciones, más probable será que tu primer intento sea cada vez más claro). En los próximos cuatro capítulos, aprenderás habilidades que te permitirán abordar nuevos programas como también resolver problemas ya existentes de manera más sencilla y con mayor claridad: En [pipes], navegarás dentro del pipe, %&gt;%, y aprenderás más sobre cómo trabaja, qué alternativas existen y cuándo no conviene usarlo. Copiar y pegar (copy-and-paste) es muy práctico, pero deberías evitar usarlo más de dos veces. Repetir el código dentro de un programa es peligroso porque puede llevar a errores e inconsistencias. Por eso, en funciones, aprenderás a escribir funciones que permiten extraer código repertido y convertirlo en código fácilmente reutilizable. A medida que comiences a escribir funciones más potentes, necesitarás una base sólida en las estructuras de datos de R, que te proporcionará vectores. Deberás dominar los cuatro tipos de vectores atómicos clásicos, también las tres clases S3 construídas sobre ellos, y entender los misterios de las listas y los data frame. Las funciones previenen que repitamos código, pero muchas veces necesitas repertir las mismas acciones con diferentes entradas. Necesitas, entonces, herramientas de iteración que te permitan hacer cosas similares una y otra vez. Estas herramientas incluyen ciclos (loops) y programación funcional que aprenderás en [iteración]. 18.1 Aprendiendo más El objetivo de estos capítulos es enseñarte lo básico de programación que necesitas para hacer ciencia de datos, que resulta una cantidad considerable de información. Una vez que hayas dominado el material de este libro, creo firmemente que deberías invertir tiempo en mejorar tus habilidades de programación. Aprender más sobre programación es una inversión a largo plazo: no obtendrás resultados inmediatos, pero con el tiempo te permitirá resolver nuevos problemas más rápidamente, y te permitirá reutilizar tus ideas de problemas anteriores en nuevos escenarios. Para profundizar necesitas estudiar R como un lenguaje de programación, no sólo como un ambiente interactivo para ciencia de datos. Escribimos dos libros que te ayudarán en ese sentido. Estos libros sólo están disponibles en inglés: Hands on Programming with R, por Garrett Grolemund. Ésta es una introducción a R como lenguaje de programación y es un buen lugar para empezar si R es tu primer lenguaje de programación. Cubre casi el mismo material que estos capítulos, pero con un estilo diferente y con distintos ejemplos de motivación (basados en el casino). Es un complemento muy útil si consideras que estos cuatros capítulos van demasiado rápido. Advanced R por Hadley Wickham. Este libro bucea en los detalles del lenguaje de programación R. Éste es un buen punto por donde empezar si ya tienes experiencia en programación. Es también una buena manera de seguir una vez que hayas internalizado las ideas de estos cuatro capítulos. Puedes leerlo online en el sitio http://adv-r.had.co.nz. "],
["relational-data.html", "19 Relational data 19.1 Introduction 19.2 nycflights13 19.3 Keys 19.4 Mutating joins 19.5 Filtering joins 19.6 Join problems 19.7 Set operations", " 19 Relational data 19.1 Introduction It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. Relations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair. Sometimes both elements of a pair can be the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents. To work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data: Mutating joins, which add new variables to one data frame from matching observations in another. Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table. Set operations, which treat observations as if they were set elements. The most common place to find relational data is in a relational database management system (or RDBMS), a term that encompasses almost all modern databases. If you’ve used a database before, you’ve almost certainly used SQL. If so, you should find the concepts in this chapter familiar, although their expression in dplyr is a little different. Generally, dplyr is a little easier to use than SQL because dplyr is specialised to do data analysis: it makes common data analysis operations easier, at the expense of making it more difficult to do other things that aren’t commonly needed for data analysis. 19.1.1 Prerequisites We will explore relational data from nycflights13 using the two-table verbs from dplyr. library(tidyverse) library(nycflights13) 19.2 nycflights13 We will use the nycflights13 package to learn about relational data. nycflights13 contains four tibbles that are related to the flights table that you used in [data transformation]: airlines lets you look up the full carrier name from its abbreviated code: airlines #&gt; # A tibble: 16 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E Endeavor Air Inc. #&gt; 2 AA American Airlines Inc. #&gt; 3 AS Alaska Airlines Inc. #&gt; 4 B6 JetBlue Airways #&gt; 5 DL Delta Air Lines Inc. #&gt; 6 EV ExpressJet Airlines Inc. #&gt; # … with 10 more rows airports gives information about each airport, identified by the faa airport code: airports #&gt; # A tibble: 1,458 x 8 #&gt; faa name lat lon alt tz dst tzone #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/New… #&gt; 2 06A Moton Field Municipal A… 32.5 -85.7 264 -6 A America/Chi… #&gt; 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/Chi… #&gt; 4 06N Randall Airport 41.4 -74.4 523 -5 A America/New… #&gt; 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/New… #&gt; 6 0A9 Elizabethton Municipal … 36.4 -82.2 1593 -5 A America/New… #&gt; # … with 1,452 more rows planes gives information about each plane, identified by its tailnum: planes #&gt; # A tibble: 3,322 x 9 #&gt; tailnum year type manufacturer model engines seats speed engine #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 N10156 2004 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… #&gt; 2 N102UW 1998 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 3 N103US 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 4 N104UW 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 5 N10575 2002 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… #&gt; 6 N105UW 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; # … with 3,316 more rows weather gives the weather at each NYC airport for each hour: weather #&gt; # A tibble: 26,115 x 15 #&gt; origin year month day hour temp dewp humid wind_dir wind_speed #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 #&gt; 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 #&gt; 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 #&gt; 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 #&gt; 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 #&gt; 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 #&gt; # … with 2.611e+04 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, #&gt; # precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; One way to show the relationships between the different tables is with a drawing: This diagram is a little overwhelming, but it’s simple compared to some you’ll see in the wild! The key to understanding diagrams like this is to remember each relation always concerns a pair of tables. You don’t need to understand the whole thing; you just need to understand the chain of relations between the tables that you are interested in. For nycflights13: flights connects to planes via a single variable, tailnum. flights connects to airlines through the carrier variable. flights connects to airports in two ways: via the origin and dest variables. flights connects to weather via origin (the location), and year, month, day and hour (the time). 19.2.1 Exercises Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? 19.3 Keys The variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation. For example, each plane is uniquely identified by its tailnum. In other cases, multiple variables may be needed. For example, to identify an observation in weather you need five variables: year, month, day, hour, and origin. There are two types of keys: A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table. A foreign key uniquely identifies an observation in another table. For example, the flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane. A variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airport table. Once you’ve identified the primary keys in your tables, it’s good practice to verify that they do indeed uniquely identify each observation. One way to do that is to count() the primary keys and look for entries where n is greater than one: planes %&gt;% count(tailnum) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 0 x 2 #&gt; # … with 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt; weather %&gt;% count(year, month, day, hour, origin) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 3 x 6 #&gt; year month day hour origin n #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2013 11 3 1 EWR 2 #&gt; 2 2013 11 3 1 JFK 2 #&gt; 3 2013 11 3 1 LGA 2 Sometimes a table doesn’t have an explicit primary key: each row is an observation, but no combination of variables reliably identifies it. For example, what’s the primary key in the flights table? You might think it would be the date plus the flight or tail number, but neither of those are unique: flights %&gt;% count(year, month, day, flight) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 29,768 x 5 #&gt; year month day flight n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 1 2 #&gt; 2 2013 1 1 3 2 #&gt; 3 2013 1 1 4 2 #&gt; 4 2013 1 1 11 3 #&gt; 5 2013 1 1 15 2 #&gt; 6 2013 1 1 21 2 #&gt; # … with 2.976e+04 more rows flights %&gt;% count(year, month, day, tailnum) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 64,928 x 5 #&gt; year month day tailnum n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2013 1 1 N0EGMQ 2 #&gt; 2 2013 1 1 N11189 2 #&gt; 3 2013 1 1 N11536 2 #&gt; 4 2013 1 1 N11544 3 #&gt; 5 2013 1 1 N11551 2 #&gt; 6 2013 1 1 N12540 2 #&gt; # … with 6.492e+04 more rows When starting to work with this data, I had naively assumed that each flight number would be only used once per day: that would make it much easier to communicate problems with a specific flight. Unfortunately that is not the case! If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key. A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines. 19.3.1 Exercises Add a surrogate key to flights. Identify the keys in the following datasets Lahman::Batting, babynames::babynames nasaweather::atmos fueleconomy::vehicles ggplot2::diamonds (You might need to install some packages and read some documentation.) Draw a diagram illustrating the connections between the Batting, Master, and Salaries tables in the Lahman package. Draw another diagram that shows the relationship between Master, Managers, AwardsManagers. How would you characterise the relationship between the Batting, Pitching, and Fielding tables? 19.4 Mutating joins The first tool we’ll look at for combining a pair of tables is the mutating join. A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other. Like mutate(), the join functions add variables to the right, so if you have a lot of variables already, the new variables won’t get printed out. For these examples, we’ll make it easier to see what’s going on in the examples by creating a narrower dataset: flights2 &lt;- flights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier) flights2 #&gt; # A tibble: 336,776 x 8 #&gt; year month day hour origin dest tailnum carrier #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA #&gt; 2 2013 1 1 5 LGA IAH N24211 UA #&gt; 3 2013 1 1 5 JFK MIA N619AA AA #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL #&gt; 6 2013 1 1 5 EWR ORD N39463 UA #&gt; # … with 3.368e+05 more rows (Remember, when you’re in RStudio, you can also use View() to avoid this problem.) Imagine you want to add the full airline name to the flights2 data. You can combine the airlines and flights2 data frames with left_join(): flights2 %&gt;% select(-origin, -dest) %&gt;% left_join(airlines, by = &quot;carrier&quot;) #&gt; # A tibble: 336,776 x 7 #&gt; year month day hour tailnum carrier name #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 N14228 UA United Air Lines Inc. #&gt; 2 2013 1 1 5 N24211 UA United Air Lines Inc. #&gt; 3 2013 1 1 5 N619AA AA American Airlines Inc. #&gt; 4 2013 1 1 5 N804JB B6 JetBlue Airways #&gt; 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. #&gt; 6 2013 1 1 5 N39463 UA United Air Lines Inc. #&gt; # … with 3.368e+05 more rows The result of joining airlines to flights2 is an additional variable: name. This is why I call this type of join a mutating join. In this case, you could have got to the same place using mutate() and R’s base subsetting: flights2 %&gt;% select(-origin, -dest) %&gt;% mutate(name = airlines$name[match(carrier, airlines$carrier)]) #&gt; # A tibble: 336,776 x 7 #&gt; year month day hour tailnum carrier name #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 N14228 UA United Air Lines Inc. #&gt; 2 2013 1 1 5 N24211 UA United Air Lines Inc. #&gt; 3 2013 1 1 5 N619AA AA American Airlines Inc. #&gt; 4 2013 1 1 5 N804JB B6 JetBlue Airways #&gt; 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. #&gt; 6 2013 1 1 5 N39463 UA United Air Lines Inc. #&gt; # … with 3.368e+05 more rows But this is hard to generalise when you need to match multiple variables, and takes close reading to figure out the overall intent. The following sections explain, in detail, how mutating joins work. You’ll start by learning a useful visual representation of joins. We’ll then use that to explain the four mutating join functions: the inner join, and the three outer joins. When working with real data, keys don’t always uniquely identify observations, so next we’ll talk about what happens when there isn’t a unique match. Finally, you’ll learn how to tell dplyr which variables are the keys for a given join. 19.4.1 Understanding joins To help you learn how joins work, I’m going to use a visual representation: x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot; ) y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; ) The coloured column represents the “key” variable: these are used to match the rows between the tables. The grey column represents the “value” column that is carried along for the ride. In these examples I’ll show a single key variable, but the idea generalises in a straightforward way to multiple keys and multiple values. A join is a way of connecting each row in x to zero, one, or more rows in y. The following diagram shows each potential match as an intersection of a pair of lines. (If you look closely, you might notice that we’ve switched the order of the key and value columns in x. This is to emphasise that joins match based on the key; the value is just carried along for the ride.) In an actual join, matches will be indicated with dots. The number of dots = the number of matches = the number of rows in the output. 19.4.2 Inner join The simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal: (To be precise, this is an inner equijoin because the keys are matched using the equality operator. Since most joins are equijoins we usually drop that specification.) The output of an inner join is a new data frame that contains the key, the x values, and the y values. We use by to tell dplyr which variable is the key: x %&gt;% inner_join(y, by = &quot;key&quot;) #&gt; # A tibble: 2 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 The most important property of an inner join is that unmatched rows are not included in the result. This means that generally inner joins are usually not appropriate for use in analysis because it’s too easy to lose observations. 19.4.3 Outer joins An inner join keeps observations that appear in both tables. An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins: A left join keeps all observations in x. A right join keeps all observations in y. A full join keeps all observations in x and y. These joins work by adding an additional “virtual” observation to each table. This observation has a key that always matches (if no other key matches), and a value filled with NA. Graphically, that looks like: The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others. Another way to depict the different types of joins is with a Venn diagram: However, this is not a great representation. It might jog your memory about which join preserves the observations in which table, but it suffers from a major limitation: a Venn diagram can’t show what happens when keys don’t uniquely identify an observation. 19.4.4 Duplicate keys So far all the diagrams have assumed that the keys are unique. But that’s not always the case. This section explains what happens when the keys are not unique. There are two possibilities: One table has duplicate keys. This is useful when you want to add in additional information as there is typically a one-to-many relationship. Note that I’ve put the key column in a slightly different position in the output. This reflects that the key is a primary key in y and a foreign key in x. x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 2, &quot;x3&quot;, 1, &quot;x4&quot; ) y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot; ) left_join(x, y, by = &quot;key&quot;) #&gt; # A tibble: 4 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 #&gt; 3 2 x3 y2 #&gt; 4 1 x4 y1 Both tables have duplicate keys. This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product: x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 2, &quot;x3&quot;, 3, &quot;x4&quot; ) y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 2, &quot;y3&quot;, 3, &quot;y4&quot; ) left_join(x, y, by = &quot;key&quot;) #&gt; # A tibble: 6 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 #&gt; 3 2 x2 y3 #&gt; 4 2 x3 y2 #&gt; 5 2 x3 y3 #&gt; 6 3 x4 y4 19.4.5 Defining the key columns So far, the pairs of tables have always been joined by a single variable, and that variable has the same name in both tables. That constraint was encoded by by = &quot;key&quot;. You can use other values for by to connect the tables in other ways: The default, by = NULL, uses all variables that appear in both tables, the so called natural join. For example, the flights and weather tables match on their common variables: year, month, day, hour and origin. flights2 %&gt;% left_join(weather) #&gt; Joining, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;) #&gt; # A tibble: 336,776 x 18 #&gt; year month day hour origin dest tailnum carrier temp dewp humid #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA 39.0 28.0 64.4 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA 39.9 25.0 54.8 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA 39.0 27.0 61.6 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 39.0 27.0 61.6 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL 39.9 25.0 54.8 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA 39.0 28.0 64.4 #&gt; # … with 3.368e+05 more rows, and 7 more variables: wind_dir &lt;dbl&gt;, #&gt; # wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, #&gt; # visib &lt;dbl&gt;, time_hour &lt;dttm&gt; A character vector, by = &quot;x&quot;. This is like a natural join, but uses only some of the common variables. For example, flights and planes have year variables, but they mean different things so we only want to join by tailnum. flights2 %&gt;% left_join(planes, by = &quot;tailnum&quot;) #&gt; # A tibble: 336,776 x 16 #&gt; year.x month day hour origin dest tailnum carrier year.y type #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA 1999 Fixe… #&gt; 2 2013 1 1 5 LGA IAH N24211 UA 1998 Fixe… #&gt; 3 2013 1 1 5 JFK MIA N619AA AA 1990 Fixe… #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 2012 Fixe… #&gt; 5 2013 1 1 6 LGA ATL N668DN DL 1991 Fixe… #&gt; 6 2013 1 1 5 EWR ORD N39463 UA 2012 Fixe… #&gt; # … with 3.368e+05 more rows, and 6 more variables: manufacturer &lt;chr&gt;, #&gt; # model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt; Note that the year variables (which appear in both input data frames, but are not constrained to be equal) are disambiguated in the output with a suffix. A named character vector: by = c(&quot;a&quot; = &quot;b&quot;). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. For example, if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to: flights2 %&gt;% left_join(airports, c(&quot;dest&quot; = &quot;faa&quot;)) #&gt; # A tibble: 336,776 x 15 #&gt; year month day hour origin dest tailnum carrier name lat lon #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA Geor… 30.0 -95.3 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA Geor… 30.0 -95.3 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA Miam… 25.8 -80.3 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 &lt;NA&gt; NA NA #&gt; 5 2013 1 1 6 LGA ATL N668DN DL Hart… 33.6 -84.4 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA Chic… 42.0 -87.9 #&gt; # … with 3.368e+05 more rows, and 4 more variables: alt &lt;int&gt;, tz &lt;dbl&gt;, #&gt; # dst &lt;chr&gt;, tzone &lt;chr&gt; flights2 %&gt;% left_join(airports, c(&quot;origin&quot; = &quot;faa&quot;)) #&gt; # A tibble: 336,776 x 15 #&gt; year month day hour origin dest tailnum carrier name lat lon #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA Newa… 40.7 -74.2 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA La G… 40.8 -73.9 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA John… 40.6 -73.8 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 John… 40.6 -73.8 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL La G… 40.8 -73.9 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA Newa… 40.7 -74.2 #&gt; # … with 3.368e+05 more rows, and 4 more variables: alt &lt;int&gt;, tz &lt;dbl&gt;, #&gt; # dst &lt;chr&gt;, tzone &lt;chr&gt; 19.4.6 Exercises Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States: airports %&gt;% semi_join(flights, c(&quot;faa&quot; = &quot;dest&quot;)) %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() (Don’t worry if you don’t understand what semi_join() does — you’ll learn about it next.) You might want to use the size or colour of the points to display the average delay for each airport. Add the location of the origin and destination (i.e. the lat and lon) to flights. Is there a relationship between the age of a plane and its delays? What weather conditions make it more likely to see a delay? What happened on June 13 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather. 19.4.7 Other implementations base::merge() can perform all four types of mutating join: dplyr merge inner_join(x, y) merge(x, y) left_join(x, y) merge(x, y, all.x = TRUE) right_join(x, y) merge(x, y, all.y = TRUE), full_join(x, y) merge(x, y, all.x = TRUE, all.y = TRUE) The advantages of the specific dplyr verbs is that they more clearly convey the intent of your code: the difference between the joins is really important but concealed in the arguments of merge(). dplyr’s joins are considerably faster and don’t mess with the order of the rows. SQL is the inspiration for dplyr’s conventions, so the translation is straightforward: dplyr SQL inner_join(x, y, by = &quot;z&quot;) SELECT * FROM x INNER JOIN y USING (z) left_join(x, y, by = &quot;z&quot;) SELECT * FROM x LEFT OUTER JOIN y USING (z) right_join(x, y, by = &quot;z&quot;) SELECT * FROM x RIGHT OUTER JOIN y USING (z) full_join(x, y, by = &quot;z&quot;) SELECT * FROM x FULL OUTER JOIN y USING (z) Note that “INNER” and “OUTER” are optional, and often omitted. Joining different variables between the tables, e.g. inner_join(x, y, by = c(&quot;a&quot; = &quot;b&quot;)) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins). 19.5 Filtering joins Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables. There are two types: semi_join(x, y) keeps all observations in x that have a match in y. anti_join(x, y) drops all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows. For example, imagine you’ve found the top ten most popular destinations: top_dest &lt;- flights %&gt;% count(dest, sort = TRUE) %&gt;% head(10) top_dest #&gt; # A tibble: 10 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ORD 17283 #&gt; 2 ATL 17215 #&gt; 3 LAX 16174 #&gt; 4 BOS 15508 #&gt; 5 MCO 14082 #&gt; 6 CLT 14064 #&gt; # … with 4 more rows Now you want to find each flight that went to one of those destinations. You could construct a filter yourself: flights %&gt;% filter(dest %in% top_dest$dest) #&gt; # A tibble: 141,145 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 542 540 2 923 #&gt; 2 2013 1 1 554 600 -6 812 #&gt; 3 2013 1 1 554 558 -4 740 #&gt; 4 2013 1 1 555 600 -5 913 #&gt; 5 2013 1 1 557 600 -3 838 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # … with 1.411e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; But it’s difficult to extend that approach to multiple variables. For example, imagine that you’d found the 10 days with highest average delays. How would you construct the filter statement that used year, month, and day to match it back to flights? Instead you can use a semi-join, which connects the two tables like a mutating join, but instead of adding new columns, only keeps the rows in x that have a match in y: flights %&gt;% semi_join(top_dest) #&gt; Joining, by = &quot;dest&quot; #&gt; # A tibble: 141,145 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 542 540 2 923 #&gt; 2 2013 1 1 554 600 -6 812 #&gt; 3 2013 1 1 554 558 -4 740 #&gt; 4 2013 1 1 555 600 -5 913 #&gt; 5 2013 1 1 557 600 -3 838 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # … with 1.411e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Graphically, a semi-join looks like this: Only the existence of a match is important; it doesn’t matter which observation is matched. This means that filtering joins never duplicate rows like mutating joins do: The inverse of a semi-join is an anti-join. An anti-join keeps the rows that don’t have a match: Anti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don’t have a match in planes: flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = TRUE) #&gt; # A tibble: 722 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 2512 #&gt; 2 N725MQ 575 #&gt; 3 N722MQ 513 #&gt; 4 N723MQ 507 #&gt; 5 N713MQ 483 #&gt; 6 N735MQ 396 #&gt; # … with 716 more rows 19.5.1 Exercises What does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) Filter flights to only show flights with planes that have flown at least 100 flights. Combine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models. Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? What does anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) tell you? What does anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) tell you? You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. 19.6 Join problems The data you’ve been working with in this chapter has been cleaned up so that you’ll have as few problems as possible. Your own data is unlikely to be so nice, so there are a few things that you should do with your own data to make your joins go smoothly. Start by identifying the variables that form the primary key in each table. You should usually do this based on your understanding of the data, not empirically by looking for a combination of variables that give a unique identifier. If you just look for variables without thinking about what they mean, you might get (un)lucky and find a combination that’s unique in your current data but the relationship might not be true in general. For example, the altitude and longitude uniquely identify each airport, but they are not good identifiers! airports %&gt;% count(alt, lon) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 0 x 3 #&gt; # … with 3 variables: alt &lt;int&gt;, lon &lt;dbl&gt;, n &lt;int&gt; Check that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation! Check that your foreign keys match primary keys in another table. The best way to do this is with an anti_join(). It’s common for keys not to match because of data entry errors. Fixing these is often a lot of work. If you do have missing keys, you’ll need to be thoughtful about your use of inner vs. outer joins, carefully considering whether or not you want to drop rows that don’t have a match. Be aware that simply checking the number of rows before and after the join is not sufficient to ensure that your join has gone smoothly. If you have an inner join with duplicate keys in both tables, you might get unlucky as the number of dropped rows might exactly equal the number of duplicated rows! 19.7 Set operations The final type of two-table verb are the set operations. Generally, I use these the least frequently, but they are occasionally useful when you want to break a single complex filter into simpler pieces. All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets: intersect(x, y): return only observations in both x and y. union(x, y): return unique observations in x and y. setdiff(x, y): return observations in x, but not in y. Given this simple data: df1 &lt;- tribble( ~x, ~y, 1, 1, 2, 1 ) df2 &lt;- tribble( ~x, ~y, 1, 1, 1, 2 ) The four possibilities are: intersect(df1, df2) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 # Note that we get 3 rows, not 4 union(df1, df2) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 1 2 setdiff(df1, df2) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 1 setdiff(df2, df1) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 "],
["r-markdown-formats.html", "20 R Markdown formats 20.1 Introduction 20.2 Output options 20.3 Documents 20.4 Notebooks 20.5 Presentations 20.6 Dashboards 20.7 Interactivity 20.8 Websites 20.9 Other formats 20.10 Learning more", " 20 R Markdown formats 20.1 Introduction So far you’ve seen R Markdown used to produce HTML documents. This chapter gives a brief overview of some of the many other types of output you can produce with R Markdown. There are two ways to set the output of a document: Permanently, by modifying the YAML header: title: &quot;Viridis Demo&quot; output: html_document Transiently, by calling rmarkdown::render() by hand: rmarkdown::render(&quot;diamond-sizes.Rmd&quot;, output_format = &quot;word_document&quot;) This is useful if you want to programmatically produce multiple types of output. RStudio’s knit button renders a file to the first format listed in its output field. You can render to additional formats by clicking the dropdown menu beside the knit button. 20.2 Output options Each output format is associated with an R function. You can either write foo or pkg::foo. If you omit pkg, the default is assumed to be rmarkdown. It’s important to know the name of the function that makes the output because that’s where you get help. For example, to figure out what parameters you can set with html_document, look at ?rmarkdown::html_document. To override the default parameter values, you need to use an expanded output field. For example, if you wanted to render an html_document with a floating table of contents, you’d use: output: html_document: toc: true toc_float: true You can even render to multiple outputs by supplying a list of formats: output: html_document: toc: true toc_float: true pdf_document: default Note the special syntax if you don’t want to override any of the default options. 20.3 Documents The previous chapter focused on the default html_document output. There are a number of basic variations on that theme, generating different types of documents: pdf_document makes a PDF with LaTeX (an open source document layout system), which you’ll need to install. RStudio will prompt you if you don’t already have it. word_document for Microsoft Word documents (.docx). odt_document for OpenDocument Text documents (.odt). rtf_document for Rich Text Format (.rtf) documents. md_document for a Markdown document. This isn’t typically useful by itself, but you might use it if, for example, your corporate CMS or lab wiki uses markdown. github_document: this is a tailored version of md_document designed for sharing on GitHub. Remember, when generating a document to share with decision makers, you can turn off the default display of code by setting global options in the setup chunk: knitr::opts_chunk$set(echo = FALSE) For html_documents another option is to make the code chunks hidden by default, but visible with a click: output: html_document: code_folding: hide 20.4 Notebooks A notebook, html_notebook, is a variation on a html_document. The rendered outputs are very similar, but the purpose is different. A html_document is focused on communicating with decision makers, while a notebook is focused on collaborating with other data scientists. These different purposes lead to using the HTML output in different ways. Both HTML outputs will contain the fully rendered output, but the notebook also contains the full source code. That means you can use the .nb.html generated by the notebook in two ways: You can view it in a web browser, and see the rendered output. Unlike html_document, this rendering always includes an embedded copy of the source code that generated it. You can edit it in RStudio. When you open an .nb.html file, RStudio will automatically recreate the .Rmd file that generated it. In the future, you will also be able to include supporting files (e.g. .csv data files), which will be automatically extracted when needed. Emailing .nb.html files is a simple way to share analyses with your colleagues. But things will get painful as soon as they want to make changes. If this starts to happen, it’s a good time to learn Git and GitHub. Learning Git and GitHub is definitely painful at first, but the collaboration payoff is huge. As mentioned earlier, Git and GitHub are outside the scope of the book, but there’s one tip that’s useful if you’re already using them: use both html_notebook and github_document outputs: output: html_notebook: default github_document: default html_notebook gives you a local preview, and a file that you can share via email. github_document creates a minimal md file that you can check into git. You can easily see how the results of your analysis (not just the code) change over time, and GitHub will render it for you nicely online. 20.5 Presentations You can also use R Markdown to produce presentations. You get less visual control than with a tool like Keynote or PowerPoint, but automatically inserting the results of your R code into a presentation can save a huge amount of time. Presentations work by dividing your content into slides, with a new slide beginning at each first (#) or second (##) level header. You can also insert a horizontal rule (***) to create a new slide without a header. R Markdown comes with three presentation formats built-in: ioslides_presentation - HTML presentation with ioslides slidy_presentation - HTML presentation with W3C Slidy beamer_presentation - PDF presentation with LaTeX Beamer. Two other popular formats are provided by packages: revealjs::revealjs_presentation - HTML presentation with reveal.js. Requires the revealjs package. rmdshower, https://github.com/MangoTheCat/rmdshower, provides a wrapper around the shower, https://github.com/shower/shower, presentation engine 20.6 Dashboards Dashboards are a useful way to communicate large amounts of information visually and quickly. Flexdashboard makes it particularly easy to create dashboards using R Markdown and a convention for how the headers affect the layout: Each level 1 header (#) begins a new page in the dashboard. Each level 2 header (##) begins a new column. Each level 3 header (###) begins a new row. For example, you can produce this dashboard: Using this code: --- title: &quot;Diamonds distribution dashboard&quot; output: flexdashboard::flex_dashboard --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) knitr::opts_chunk$set(fig.width = 5, fig.asp = 1 / 3) ``` ## Column 1 ### Carat ```{r} ggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.1) ``` ### Cut ```{r} ggplot(diamonds, aes(cut)) + geom_bar() ``` ### Colour ```{r} ggplot(diamonds, aes(color)) + geom_bar() ``` ## Column 2 ### The largest diamonds ```{r} diamonds %&gt;% arrange(desc(carat)) %&gt;% head(100) %&gt;% select(carat, cut, color, price) %&gt;% DT::datatable() ``` Flexdashboard also provides simple tools for creating sidebars, tabsets, value boxes, and gauges. To learn more about flexdashboard visit http://rmarkdown.rstudio.com/flexdashboard/. 20.7 Interactivity Any HTML format (document, notebook, presentation, or dashboard) can contain interactive components. 20.7.1 htmlwidgets HTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualisations. For example, take the leaflet map below. If you’re viewing this page on the web, you can drag the map around, zoom in and out, etc. You obviously can’t do that in a book, so rmarkdown automatically inserts a static screenshot for you. library(leaflet) leaflet() %&gt;% setView(174.764, -36.877, zoom = 16) %&gt;% addTiles() %&gt;% addMarkers(174.764, -36.877, popup = &quot;Maungawhau&quot;) The great thing about htmlwidgets is that you don’t need to know anything about HTML or JavaScript to use them. All the details are wrapped inside the package, so you don’t need to worry about it. There are many packages that provide htmlwidgets, including: dygraphs, http://rstudio.github.io/dygraphs/, for interactive time series visualisations. DT, http://rstudio.github.io/DT/, for interactive tables. threejs, https://github.com/bwlewis/rthreejs for interactive 3d plots. DiagrammeR, http://rich-iannone.github.io/DiagrammeR/ for diagrams (like flow charts and simple node-link diagrams). To learn more about htmlwidgets and see a more complete list of packages that provide them visit http://www.htmlwidgets.org/. 20.7.2 Shiny htmlwidgets provide client-side interactivity — all the interactivity happens in the browser, independently of R. On one hand, that’s great because you can distribute the HTML file without any connection to R. However, that fundamentally limits what you can do to things that have been implemented in HTML and JavaScript. An alternative approach is to use shiny, a package that allows you to create interactivity using R code, not JavaScript. To call Shiny code from an R Markdown document, add runtime: shiny to the header: title: &quot;Shiny Web App&quot; output: html_document runtime: shiny Then you can use the “input” functions to add interactive components to the document: library(shiny) textInput(&quot;name&quot;, &quot;What is your name?&quot;) numericInput(&quot;age&quot;, &quot;How old are you?&quot;, NA, min = 0, max = 150) You can then refer to the values with input$name and input$age, and the code that uses them will be automatically re-run whenever they change. I can’t show you a live shiny app here because shiny interactions occur on the server-side. This means that you can write interactive apps without knowing JavaScript, but you need a server to run them on. This introduces a logistical issue: Shiny apps need a Shiny server to be run online. When you run shiny apps on your own computer, shiny automatically sets up a shiny server for you, but you need a public facing shiny server if you want to publish this sort of interactivity online. That’s the fundamental trade-off of shiny: you can do anything in a shiny document that you can do in R, but it requires someone to be running R. Learn more about Shiny at http://shiny.rstudio.com/. 20.8 Websites With a little additional infrastructure you can use R Markdown to generate a complete website: Put your .Rmd files in a single directory. index.Rmd will become the home page. Add a YAML file named _site.yml provides the navigation for the site. For example: name: &quot;my-website&quot; navbar: title: &quot;My Website&quot; left: - text: &quot;Home&quot; href: index.html - text: &quot;Viridis Colors&quot; href: 1-example.html - text: &quot;Terrain Colors&quot; href: 3-inline.html Execute rmarkdown::render_site() to build _site, a directory of files ready to deploy as a standalone static website, or if you use an RStudio Project for your website directory. RStudio will add a Build tab to the IDE that you can use to build and preview your site. Read more at http://rmarkdown.rstudio.com/rmarkdown_websites.html. 20.9 Other formats Other packages provide even more output formats: The bookdown package, https://github.com/rstudio/bookdown, makes it easy to write books, like this one. To learn more, read Authoring Books with R Markdown, by Yihui Xie, which is, of course, written in bookdown. Visit http://www.bookdown.org to see other bookdown books written by the wider R community. The prettydoc package, https://github.com/yixuan/prettydoc/, provides lightweight document formats with a range of attractive themes. The rticles package, https://github.com/rstudio/rticles, compiles a selection of formats tailored for specific scientific journals. See http://rmarkdown.rstudio.com/formats.html for a list of even more formats. You can also create your own by following the instructions at http://rmarkdown.rstudio.com/developer_custom_formats.html. 20.10 Learning more To learn more about effective communication in these different formats I recommend the following resources: To improve your presentation skills, I recommend Presentation Patterns, by Neal Ford, Matthew McCollough, and Nathaniel Schutta. It provides a set of effective patterns (both low- and high-level) that you can apply to improve your presentations. If you give academic talks, I recommend reading the Leek group guide to giving talks. I haven’t taken it myself, but I’ve heard good things about Matt McGarrity’s online course on public speaking: https://www.coursera.org/learn/public-speaking. If you are creating a lot of dashboards, make sure to read Stephen Few’s Information Dashboard Design: The Effective Visual Communication of Data. It will help you create dashboards that are truly useful, not just pretty to look at. Effectively communicating your ideas often benefits from some knowledge of graphic design. The Non-Designer’s Design Book is a great place to start. "],
["flujo-de-trabajo-en-r-markdown.html", "21 Flujo de trabajo en R Markdown", " 21 Flujo de trabajo en R Markdown Antes, discutimos un flujo de trabajo básico para capturar tu código de R cuando trabjas interactivamente en la consola, la cual captura lo que trabaja en el editor de script. R Markdown une la consola y el editor de script, desdibujando los limites entre exploración interactiva y captura de código a largo plazo. Puedes iterar rápidamente dentro un bloque, editando y re-ejecutando con Cmd/Ctrl + Shift + Enter. Cuando estés conforme, sigue adelante y inicia un nuevo bloque. R Markdown es también importante ya que integra firmemente prosa y código. Esto hace que sea un gran notebook de analísis, porque permite desarrollar código y registrar tus pensamientos. Un notebook de analísis comparte muchos de los mismos objetivos que tiene un notebook de laboratorio clasico en las ciencias físicas. Puede: Registrar qué se hizo y por qué se hizo. Independientemente de cuan buena sea tu memoria, si no registras lo que haces, llegará un momento cuando hayas olvidado detalles importantes. Apoyar el pensamiento riguroso. Es mas probable que logres un análisis fuerte si registras tus pensamientos mientras avanzas, y continuas reflexionando sobre ellos. Esto también te ahorra tiempo cuando eventualmente escribes tu analisis para compartir con otros. Ayudar a que otros comprendan tu trabajo. Es raro hacer un analisis de datos por sí sola, dado que muy seguido trabajarás como parte de un equipo. Un notebook de laboratorio ayuda a que compartas no solo lo que has hecho, sino también por qué lo hiciste con tus colegas o compañeros de laboratorio. Muchos de estos buenos consejos sobre el uso más efectivo de notebooks de laboratorio pueden también ser trasladados para utilizar un notebooks de analísis. He extraído de mis propias experiencias y los consejos de Colin Purrington sobre notebooks de laboratorio (http://colinpurrington.com/tips/lab-notebooks) para sugerir los siguientes consejos: Asegúrate de que cada notebook tenga un titulo descriptivo, un nombre de archivo evocativo, y un primer parrafo que describa brevemente los objetivos del analisis. Utiliza el campo para fecha del encabezado YAML para registrar la fecha en la que comienzas a trabajar en el notebook: title: My title date: 2016-08-23 Utiliza el formato ISO8601 AAAA-MM-DD para que no haya ambiguidad. ¡Utilízalo incluso si no escribes normalmente fechas de ese modo! Si pasas mucho timepo en una idea de analísis y resulta ser un callejón sin salida, ¡no lo elimines! Escribe una nota breve sobre por qué falló y déjala en el notebook .Esto te ayudará a evitar ir por el mismo callejon sin salida cuando regreses a ese analisis en el futuro. Generalmente, es mejor que hagas la entrada de datos fuera de R. Pero si necesitas registrar un pequeño bloque de datos, establécelo de modo claro usando tibble::tribble(). Si descubres un error en un archivo de datos, nunca lo modifiques directamente, en su lugar escribe código para corregir el valor. Explica por qué lo corregiste. Antes de concluir por el día, asgurate de que puedes knit el notebook (si utilizas cacheo, asgurate de limpiar los caches). Esto te permitirá corregir cualquier problema mientras el código esta todavia fresco en tu mente. Si quieres que tu código sea reproducible a largo plazo (esto quiere decir que puedas regresar a ejecutarlo el mes próximo o el año próximo), necesitarás registrar las versiones de los paquetes que tu código usa. Un enfoque riguroso es usar packrat, http://rstudio.github.io/packrat/, el cual almacena paquetes en tu directorio de proyecto, or checkpoint, https://github.com/RevolutionAnalytics/checkpoint, el cual reinstala paquetes disponibles en una fecha determinada . Un truco rápido y sucio es incluir un bloque que ejecute sessionInfo() — , esto no te permitirá recrear fácilmente tus paquetes tal y como estan hoy, pero por lo menos sabrás cuales eran. Crearás muchos, muchos, muchos notebooks de analisis a lo largo de tu carrera. ¿Cómo puedes organizarlos de modo tal que puedas encontrarlos otra vez en el futuro? Recomiendo almacenarlos en proyectos individuales, y tener un buen esquema de nombrado. "],
["r-markdown.html", "22 R Markdown 22.1 Introducción 22.2 R Markdown básico 22.3 Formateo de texto con Markdown 22.4 Bloques de código 22.5 Solucionando problemas 22.6 Encabezado YAML 22.7 Aprendiendo más", " 22 R Markdown 22.1 Introducción R Markdown provee un marco de referencia para la ciencia de datos, combinando tu código, sus resultados, y tus comentarios en prosa. Los documentos de R Markdown son completamente reproducibles y soportan docenas de formatos de salida tales como PDFs, archivos de Word, presentaciones y más. Los archivos R Markdown están diseñados para ser usados de tres maneras: Para comunicarse con los tomadores de decisiones, quienes desean enfocarse en las conclusiones, no en el código que subyace al análisis. Para colaborar con otros científicos de datos (¡incluyendo a tu yo futuro!), quienes están interesados tanto en tus conclusiones como en el modo en el que llegaste a ellas (es decir, el código). Como un ambiente en el cual hacer ciencia de datos, como un notebook de laboratorio moderno donde puedes capturar no sólo que hiciste, sino también estabas pensando cuando lo hacías. R Markdown integra una cantidad de paquetes de R y herramientas externas. Esto implica que la ayuda ,en general, no está disponible a través de ?. En su lugar, mientras trabajas a lo largo de este capitulo, y utilizas R Markdown en el futuro, mantén estos recursos cerca: Hoja de referencia de R Markdown : Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet Guía de referencia R Markdown : Help &gt; Cheatsheets &gt; R Markdown Reference Guide Ambas hojas también se encuentran disponibles en http://rstudio.com/cheatsheets&gt;. 22.1.1 Prerequisitos Necesitas el paquete rmarkdown, pero no necesitas cargarlo o instalarlo explícitamente ya que RStudio hace ambas acciones automaticamente cuando es necesario. 22.2 R Markdown básico Este es un archivo R Markdown, un archivo de texto plano que tiene la extensión .Rmd: --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: html_document --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) ``` We have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below: ```{r, echo = FALSE} smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.01) ``` Contiene tres tipos importantes de contenido: Un encabezado YAML (opcional) rodeado de --- Bloques de código de R rodeado de ```. Texto mezclado con texto simple formateado con # Encabezado e _itálicas_. Cuando abres un archivo .Rmd, obtienes una interfaz de notebook donde el código y el output están intercalados. Puedes ejecutar cada bloque de código haciendo clic el ícono ejecutar ( se parece a un botón de reproducir en la parte superior del bloque de código), o presionando Cmd/Ctrl + Shift + Enter. RStudio ejecuta el código y muestra los resultados incustrados en el código: Para producir un reporte completo que contenga todo el texto, código y resultados, hacerclic en “Knit” o presionar Cmd/Ctrl + Shift + K. Puede hacerse tambien de manera programática con rmarkdown::render(&quot;1-example.Rmd&quot;). Esto mostrará el reporte en el panel viewer y crea un archivo HTML independiente que puedes compartir con otros. Cuando haces knit el documento (knit en español significa tejer), R Markdown envía el .Rmd a knitr, http://yihui.name/knitr/, que ejecuta todos los bloques de código y crea un nuevo documento markdown (.md) que incluye el código y su output. El archivo markdown generado por knitr es procesado entonces por pandoc, http://pandoc.org/, que es el responsable de crear el archivo terminado. La ventaja de este flujo de trabajo en dos pasos es que puedes crear un muy amplio rango de formatos de salida, como aprenderás en [Formatos de R markdown ]. Para comenzar con tu propio archivo .Rmd, selecciona File &gt; New File &gt; R Markdown… en la barra de menú. Rstudio iniciará un asistente que puedes usar para pre-rellenar tu archivo con contenido útil que te recuerde como funcionan las principales características de R Markdown. Las siguientes secciones profundizan en los tres componentes de un documento de R Markdown en más detalle: el texto markdown, los bloques de código y el encabezado YAML. 22.2.1 Ejercicios Crea un nuevo notebook usando File &gt; New File &gt; R Notebook. Lee las instrucciones. Practica ejecutando los bloques. Verifica que puedes modificar el código,re-ejecútalo, y observa la salida modificada. Crea un nuevo documento R Markdown con File &gt; New File &gt; R Markdown… Haz clic en el icono apropiado de Knit. Haz Knit usando el atajo de teclado apropiado. Verifica que puedes modificar el input y la actualizacion del output. Compara y contrasta el notebook de R con los archivos de R markdown que has creado antes. ¿Cómo son similares los outputs? ¿Cómo son diferentes? ¿Cómo son similares los inputs? ¿En qué se diferencian? ¿Qué ocurre si copias el encabezado YAML de uno al otro? Crea un nuevo documento R Markdown para cada uno de los tres formatos incorporados: HTML, PDF and Word. Haz knit en cada uno de estos tres documentos. ¿Como difiere el output? ¿Cómo difiere el input? (Puedes necesitar instalar LaTeX para poder compilar el output en PDF— RStudio preguntará si esto es necesario). 22.3 Formateo de texto con Markdown La prosa en los archivos .Rmd está escrita en Markdown, un set liviano de convenciones para dar formato a archivos de texto plano. Markdown está diseñado para ser fácil de leer y fácil de escribir. Es tambien muy fácil de aprender. La guía abajo muestra como usar el Markdown de Pandoc, una version ligeramente extendida de markdown que R Markdown comprende. Text formatting ------------------------------------------------------------ *italic* or _italic_ **bold** __bold__ `code` superscript^2^ and subscript~2~ Headings ------------------------------------------------------------ # 1st Level Header ## 2nd Level Header ### 3rd Level Header Lists ------------------------------------------------------------ * Bulleted list item 1 * Item 2 * Item 2a * Item 2b 1. Numbered list item 1 1. Item 2. The numbers are incremented automatically in the output. Links and images ------------------------------------------------------------ &lt;http://example.com&gt; [linked phrase](http://example.com) ![optional caption text](path/to/img.png) Tables ------------------------------------------------------------ First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell La mejor manera de aprender es simplemente probar. Tomará unos días, pero pronto se convertirá en algo natural, y no necesitarás pensar en ellas. Si te olvidas, puedes tener una útil hoja de referencia con Help &gt; Markdown Quick Reference. 22.3.1 Ejercicios Practica lo que has aprendido crando un CV breve. El título debería ser tu nombre, y deberías incluir encabezados para (por lo menos) educación o empleo. Cada una de las secciones debería incluir una lista punteada de trabajos/ títulos obtenidos. Resalta año en negrita. Usando la referencia rapida de R Markdown, descubre como: Agregar una nota al pie. Agregar una linea horizontal. Agregar una cita en bloque. Copia y pega los contenidos de diamond-sizes.Rmd desde https://github.com/hadley/r4ds/tree/master/rmarkdown a un documento local de R Markdown. Revisa que puedes ejecutarlo, agrega texto despues del poligono de frecuencias que describa sus caracteristicas más llamativas. 22.4 Bloques de código Para ejecutar código dentro de un documento R Markdown, necesitas insertar un bloque. Hay tres maneras para hacerlo: El atajo de teclado Cmd/Ctrl + Alt + I El icono “Insertar” en la barra de edición Tipeando manualmente los delimitadores de bloque ```{r} y ```. Obviamente, recomendaría que aprendieras a usar el atajo de teclado. A largo plazo, te ahorrará mucho tiempo. Puedes continuar ejecutando el código usando el atajo de teclado que para este momento (espero!) ya conoces y amas : Cmd/Ctrl + Enter. Sin embargo, los bloques de código tienen otro atajo de teclado: Cmd/Ctrl + Shift + Enter, que ejecuta todo el código en el bloque. Piensa el bloque como una función. Un bloque debería ser relativamente autónomo,y enfocado alrededor de una sola tarea. Las siguientes secciones decriben el encabezado de bloque que consiste en ```{r, seguido por un nombre opcional para el bloque, seguido entonces por opciones separadas por comas, y concluyendo con }. Inmediatamente después sigue tu código de R el bloque y el fin del bloque se indica con un ``` final. 22.4.1 Nombres en bloques Los bloques puede tener opcionalmente nombres : ```{r nombre}. Esto presenta tres ventajas: Puedes navegar más fácilmente a bloques específicos usando el navegador de código desplegable abajo a la izquierda en el editor de script: Los gráficos producidos por los bloques tendrán nombres útiles que hace que sean más fáciles de utilizar en otra parte. Máa sobre esto en otras opciones importantes. Puedes crear redes de bloque cacheados para evitar re-ejecutar computos costosos en cada ejecucion. Más sobre esto mas adelante. Hay un nombre de bloque que tiene comportamiento especial: setup. Cuando te encuentras en modo notebook, el bloque llamado setup se ejecutará automáticamente una vez, antes de ejecutar cualquier otro código. 22.4.2 Opciones en bloques La salida de los bloques puede personalizarse con options, argumentos suministrados al encabezado del bloque. Knitr provee casi 60 opciones para que puedas usar para personalizar tus bloques de código. Aqui cubriremos las opciones de bloques mas imporantes que usaras más frecuentemente. Puedes ver la lista completa en http://yihui.name/knitr/options/. El set de opciones más importantes controla si tu bloque de código es ejecutado y que resultados estarán insertos en el reporte terminado: eval = FALSE evita que código sea evaluado. (Y obviamente si el código no es ejecutado no se generaran resultados). Esto es útil para mostrar códigos de ejemplo,o para deshabilitar un gran bloque de código sin comentar cada línea. include = FALSE ejecuta el código, pero no muestra el código o los resultados en el documento final. Usa esto para que código de configuracion que no quieres que abarrote tu reporte. echo = FALSE evita que se vea el código, pero no los resultados en el archivo final. Utiliza esto cuando quieres escribir reportes enfocados a personas que no quieren ver el código subyacente de R. message = FALSE o warning = FALSE evita que aparezcan mensajes o advertencias en el archivo final. results = 'hide' oculta el output impreso; fig.show = 'hide' oculta gráficos. error = TRUE causa que el render continúe incluso si el código devuelve un error. Esto es algo que raramente quieres incluir en la version final de tu reporte, pero puede ser muy útil si necesitas depurar exactamente que ocurre dentro de tu .Rmd. Es también útil si estas enseñando R y quieres incluir deliberadamente un error. Por defecto, error = FALSE provoca que el knitting falle si hay incluso un error en el documento. La siguiente tabla resume que tipos de output suprime cada opción: Opción Ejecuta Muestra Output Gráficos Mensajes Advertencias eval = FALSE - - - - - include = FALSE - - - - - echo = FALSE - results = &quot;hide&quot; - fig.show = &quot;hide&quot; - message = FALSE - warning = FALSE - 22.4.3 Tablas Por defecto, R Markdown imprime data frames y matrices tal como se ven en la consola: mtcars[1:5, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 Si prefieres que los datos tengan formato adicional puedes usar la función knitr::kable. El siguente código genera una Tabla 22.1. knitr::kable( mtcars[1:5, ], caption = &quot;Un kable de knitr.&quot; ) Table 22.1: Un kable de knitr. mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 Lee la documentación para ?knitr::kable para ver los otros modos en los que puedes personalizar la tabla. Para una mayor personalización, considera los paquetes xtable, stargazer, pander, tables, y ascii. Cada uno provee un set de herramientas para generar tablas con formato a partir código de R. Hay tambien una gran cantidad de opciones para controlar como las figuras estan embebidas o incrustadas. Aprenderás sobre esto en guardando tus gráficos. 22.4.4 Caching Normalmente, cada knit de un documento empieza desde una sesión limpia. Esto es genial para reproducibilidad, porque se asegura que has capturado cada cómputo importante en el código. Sin embargo, puede ser dolorosos si tienes cómputos que toman mucho tiempo. La solución es cache = TRUE. Cuando está funcionando, esto guarda el output del bloque a un archivo especialmente en el disco. En corridas subsecuentes, knitr revisara si el código ha cambiado y si no ha cambiado, reutilizará los resultados del cache. El sistema de cache debe ser usado con cuidado, porque por defecto está solo basado en el código, no en sus dependencias. Por ejemplo , aqui el bloque datos_procesados depende del bloque datos_crudos: {r datos_crudos} datosCrudos &lt;- readr::read_csv(&quot;un_archivo_muy_grande.csv&quot;) {r datos_procesados, cache = TRUE} datosProcesados &lt;- datosCrudos %&gt;% filter(!is.na(variableImportada)) %&gt;% mutate(nuevaVariable = transformacionComplicada(x, y, z)) Cacheing el bloque processed_data significa que tendrás que re-ejecutar si cambia el pipeline de dplyr, pero no podrás re-ejecutarlo si cambia el read_csv(). Puedes evitar este problema con la opción de bloque dependson: {r datos_procesados, cache = TRUE, dependson = &quot;raw_data&quot;} datosProcesados &lt;- datosCrudos %&gt;% filter(!is.na(variableImportada)) %&gt;% mutate(nuevaVariable = transformacionComplicada(x, y, z)) dependson debería incluir un vector de caracteres para cada bloque que el bloque cached depende. Knitr actualizará los resultados para el vector cached cada vez que detecta que una de sus dependencias ha cambiado. Nota que los bloques de código no se actualizaran si el archivo un_archivo_muy_grande.csv cambia, porque knitr hace cache solo los cambios dentro del archivo .Rmd. Si quieres seguir los cambios a ese archivo puedes usar la opción cache.extra. Esta es una expresión arbritaria de R que invalidará el cache cada vez que cambie. Una buena función a usar es file.info(): genera mucha información sobre el archivo incluyendo cuando fue su última modificación. Puedes escribir entonces: {r raw_data, cache.extra = file.info(&quot;a_very_large_file.csv&quot;)} rawdata &lt;- readr::read_csv(&quot;a_very_large_file.csv&quot;) {r datos_crudos, cache.extra = file.info(&quot;un_archivo_muy_grande.csv&quot;)} datosCrudos &lt;- readr::read_csv(&quot;un_archivo_muy_grande.csv&quot;) A medida que tus estrategias de caching se vuelven progresivamente mas complicadas, es una buena idea limpiar regularmente todos tus caches con knitr::clean_cache(). Siguiendo el consejo de David Robinson para nombrar estos bloques: cada bloque está nombrado por el objeto primario que crea. Esto hace mucho más fácil entender la especificacion dependson. 22.4.5 Opciones globales A medida que trabajes más con knitr, descubrirás que algunas de las opciones de bloque por defecto no se ajustan a tus necesidades y querrás cambiarlas. Puedes hacer esto incluyendo knitr::opts_chunk$set() en un bloque de código. Por ejemplo, cuando escribo libros y tutoriales seteo: knitr::opts_chunk$set( comment = &quot;#&gt;&quot;, collapse = TRUE ) Esto utiliza mi formato preferido de comentarios, y se asegura que el código y el output se mantienen entrelazados. Por otro lado, si preparas un reporte, puedes setear: knitr::opts_chunk$set( echo = FALSE ) Esto ocultara el código por defecto, asi que solo mostrará los bloques que deliberadamente has elegido mostrar( con echo = TRUE). Puedes considerar setear message = FALSE y warning = FALSE, pero eso puede hacer mas díficil de depurar problemas porque no verías ningun mensajes en el documento final. 22.4.6 Código en la línea Hay otro modo de incrustar código R en un documento R Markdown: directamente en el texto, con:`r `. Esto puede ser muy útil si mencionas propiedades de tu datos en el texto. Por ejemplo, en el documento de ejemplo que utilice al comienzo del capitulo tenía: Tenemos datos sobre `r nrow(diamonds)` diamantes. Solo `r nrow(diamonds) - nrow(smaller)` son mayores que 2.5 quilates. La distribucion de lo restante se muestra abajo: Cuando hacemos knit, los resultados de estos computós estan insertos en el texto: Tenemos datos de 53940 diamantes. solo 126 son mas grandes que 2.5 quilates. La distribucion de lo restante se muestra abajo: Cuando insertas números en el texto, format() es tu amigo. Esto permite que se seten el número de digitos para que no imprimas con un grado rídiculo de precision, y una big.mark para hacer que los números sean mas fáciles de leer. Siempre combino estos en una función de ayuda: comma &lt;- function(x) format(x, digits = 2, big.mark = &quot;,&quot;) comma(3452345) #&gt; [1] &quot;3,452,345&quot; comma(.12358124331) #&gt; [1] &quot;0.12&quot; 22.4.7 Ejercicios Incluye una seccion que explore como los tamaños de diamantes varian por corte, color y claridad. Asume que escribes un reporte para alguien que no conoce R, y en lugar de setear echo = FALSE en cada bloque, setear una opción global. Descarga diamond-sizes.Rmd de https://github.com/hadley/r4ds/tree/master/rmarkdown. Agrega una sección que describa los 20 diamantes mas grandes, incluyendo una tabla que muestre sus atributos más importantes. Modifica diamonds-sizes.Rmd para usar comma() para producir un formato de output ordenado. Tambien incluye el porcentaje de diamantes que son mayores a 2.5 quilates. Setea una red de bloques donde d depende de c y b, y tanto b y c dependen de a. Haz que cada bloque imprima lubridate::now(), set cache = TRUE, y verifica entonces tu comprension del almacenamiento en cache. 22.5 Solucionando problemas Los documentos de solución de problemas en R Markdown pueden ser un desafío porque no te encuentras en un ambiente de R interactivo, y necesitarás saber algunos trucos nuevos. La primer cosa que debes intentar es recrear el problema en una sesión interactiva. Reinicia R, después “Ejecuta todos los bloques” (ya sea en el menú de código , bajo la zona de Ejecutar ), o con el atajo del teclado Ctrl + Alt + R. Si tienes suerte, eso recreará el problema, y podrás descubrir lo que esta ocurriendo interactivamente. Si eso no ayuda, debe haber algo diferente entre tu ambiente interactivo y el ambiente de R Markdown. Tendrás que explorar sistemáticamente las opciones. La diferencia más común es el directorio de trabajo: el directorio de trabajo de R Markdown es el directorio en el que se encuentra. Revisa que el directorio de trabajo es el que esperas incluyendo getwd() en un bloque. A continuación, piensa todas las cosas que pueden causar el error. Necesitarás revisar sistemáticamente que tu sesión de R y tu sesión de R Markdown son lo mismo. La manera mas fácil de hacer esto es setear error = TRUE en el bloque que causa problemas, usa entonces print() y str() para revisar que la configuración es la esperada. 22.6 Encabezado YAML Puedes controlar otras configuraciones de “documento completo” retocando los parametros del encabezado YAML. Estarás preguntandote que significa YAML: en inglés “yet another markup language” signifca algo como “otro lenguaje markup”, el cual esta diseñado para representar datos jerárquicos de modo tal que es fácil de escribir y leer para humanos. R Markdown utiliza esto para controlar muchos detalles del output. Aqui discutiremos dos: parametros del documento y bibliografías. 22.6.1 Parámetros Los documentos R Markdown pueden incluir uno o mas parámetros cuyos valores pueden ser seteado cuando haces render al reporte. Los parámetros son útiles cuandos quieres re-renderizar el mismo reporte con valores distintos con varios inputs clave. Por ejemplo, podrias querer producir reportes de venta por ramas, resultados de examen por alumno, resumenes demograficos por pais. Para declarar uno o mas parametros, utiliza el campo params. Este ejemplo utiliza el parametro my_class para determinar que clase de auto mostrar: --- output: html_document params: my_class: &quot;suv&quot; --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) class &lt;- mpg %&gt;% filter(class == params$my_class) ``` # Fuel economy for `r params$my_class`s ```{r, message = FALSE} ggplot(class, aes(displ, hwy)) + geom_point() + geom_smooth(se = FALSE) ``` Como puedes ver, los parámetros estan disponibles dentro de los bloques de código como una lista de solo lectura llamada params. Puedes escribir vectores átomicos directamente en el encabezado YAML. Puedes tambien ejecutar arbitrariamente expresiones de R introduciendo previamente el valor del parámetro con !r. Esta es una buena manera de especificar parametros de fecha/hora. params: start: !r lubridate::ymd(&quot;2015-01-01&quot;) snapshot: !r lubridate::ymd_hms(&quot;2015-01-01 12:30:00&quot;) En RStudio, puedes hacer clic en la opción &quot;Knit with Parameters en el menú desplegable Knit para setear parámetros,renderizar y previsualizar en un un simple paso amigable para el usuario. Puedes personalizar el diálogo seteando otras opciones en el encabezado. Ver para mas detalles http://rmarkdown.rstudio.com/developer_parameterized_reports.html#parameter_user_interfaces. De manera alternativa, si necesitas producir varios reportes parametrizados puedes incluir rmarkdown::render() con una lista de params: rmarkdown::render(&quot;fuel-economy.Rmd&quot;, params = list(my_class = &quot;suv&quot;)) Esto es particularmente poderoso en conjuncion con purrr:pwalk(). El siguiente ejemplo crea un reporte para cada valor de class que se encuentra mpg. Primero creamos un data frame que tiene una fila para cada clase, dando el filename del reporte y los params: reports &lt;- tibble( class = unique(mpg$class), filename = stringr::str_c(&quot;fuel-economy-&quot;, class, &quot;.html&quot;), params = purrr::map(class, ~ list(my_class = .)) ) reports #&gt; # A tibble: 7 x 3 #&gt; class filename params #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 compact fuel-economy-compact.html &lt;list [1]&gt; #&gt; 2 midsize fuel-economy-midsize.html &lt;list [1]&gt; #&gt; 3 suv fuel-economy-suv.html &lt;list [1]&gt; #&gt; 4 2seater fuel-economy-2seater.html &lt;list [1]&gt; #&gt; 5 minivan fuel-economy-minivan.html &lt;list [1]&gt; #&gt; 6 pickup fuel-economy-pickup.html &lt;list [1]&gt; #&gt; # … with 1 more row Entonces unimos los nombres de las columnas con los nombres de los argumentos de render(), y utiliza parallel del paquete purrr para render() una vez para cada fila: reports %&gt;% select(output_file = filename, params) %&gt;% purrr::pwalk(rmarkdown::render, input = &quot;fuel-economy.Rmd&quot;) 22.6.2 Bibliografías y citas Pandoc puede generar automaticamente citas y bibliografia en varios estilos. Para usar esta caracteristica, especifica un archivo de bibliografia usando el campo bibliography en el encabezado de tu archivo. El campo debe incluir una ruta del directorio que contiene tu archivo .Rmd al archivo que contiene el archivo de la bibliografía: bibliography: rmarkdown.bib Puedes usar muchos formatos comunes de biliografía incluyendo BibLaTeX, BibTeX, endnote, medline. Para crear una cita dentro de tu archivo .Rmd, usa una clave compuesta de ‘@’ + el identificador de la cita del archivo de la bibliografia. Despues ubica esta cita entre corchetes. Aqui hay algunos ejemplos: Multiple citas se separan con un `;`: Bla bla[@smith04; @doe99]. Puedes incluir comentarios arbritarios dentro de los corchetes: Bla bla [ver @doe99, pp. 33-35; tambiée @smith04, ch. 1]. Remover los corchetes para crear una cita dentro del texto @smith04 dice bla, o @smith04 [p. 33] says bla. Agrega un signo `-` antes de la cita para eliminar el nombre del autor: Smith dice bla [-@smith04]. Cuando R Markdown hace render de tu archivo, construirá y agregará una bibliografía al final del documento. La bibliografia contendrá cada una de las referencias citadas de tu archivo de bibliografia, pero no contendrá un encabezado de sección. Como resultado, es una práctica común finalizar el archivo con un encabezado de sección para la bibliografía, tales como # Referencias or # Bibliografia. Puedes cambiar el estilo de tus citas y bibliografia referenciando un archivo CSL (del ingles,“citation style language”, lengaje de estilo de citas ) en el campo csl: bibliography: rmarkdown.bib csl: apa.csl Tal y como en el campo de bilbiografia, tu archivo csl deberia contener una ruta al archivo. Aqui asumo que el archivo csl esta en el mismo directorio que el archivo .Rmd. Un buen lugar para encontrar archivos de estilos para estilos de bilbiografia comunes es http://github.com/citation-style-language/styles. 22.7 Aprendiendo más R Markdown es todavía relativamente reciente, y todavia está creciendo rápidamente. El mejor lugar para estar al tanto de las innovaciones es el sitio oficial de R Markdown: http://rmarkdown.rstudio.com. Hay dos tópicos importantes que no hemos mencionado aquí: colaboraciones y los detalles de comunicar de manera precisa tus ideas a otros humanos. La colaboración es una parte vital de la ciencia de datos moderna, podria hacer tu vida mucho mas facil usando herramientas de control de versión, tales como Git y GitHub. Recomendamos dos recursos gratuitos que te enseñaran Git: “Happy Git with R”: una introduccion amigable al usario a Git and GitHub a usarios de R, de Jenny Bryan. El libro esta disponible de manera libre online en: http://happygitwithr.com El capitulo “Git and GitHub” de R Packages, de Hadley Wickham. Puedes también leerlo online: &lt;http://r- pkgs.had.co.nz/git.html&gt;. Tampoco he mencionado lo que deberías en realidad escribir para poder comunicar claramente los resultados de tu análisis. Para mejorar tu escritura, recomiendo leer cualquiera de estos libros: Style: Lessons in Clarity and Grace de Joseph M. Williams &amp; Joseph Bizup, or The Sense of Structure: Writing from the Reader’s Perspective de George Gopen. Ambos libros te ayudarán a entender la estructura de oraciones y párrafos, y te dar&lt;n las herramienta para hacer mas clara tu escritura ( Estos libros son bastante caros si son comprados nuevos, pero dado que son usados en muchas clases de inglés hay muchas copias baratas de segunda mano). George Gopen tambien a escrito varios articulos cortos sobre escritura en https://www.georgegopen.com/the-litigation-articles.html. Están dirigidos a abogados, pero casi todo también se aplica a los científicos de datos. "],
["strings.html", "23 Strings 23.1 Introduction 23.2 String basics 23.3 Matching patterns with regular expressions 23.4 Tools 23.5 Other types of pattern 23.6 Other uses of regular expressions 23.7 stringi", " 23 Strings 23.1 Introduction This chapter introduces you to string manipulation in R. You’ll learn the basics of how strings work and how to create them by hand, but the focus of this chapter will be on regular expressions, or regexps for short. Regular expressions are useful because strings usually contain unstructured or semi-structured data, and regexps are a concise language for describing patterns in strings. When you first look at a regexp, you’ll think a cat walked across your keyboard, but as your understanding improves they will soon start to make sense. 23.1.1 Prerequisites This chapter will focus on the stringr package for string manipulation. stringr is not part of the core tidyverse because you don’t always have textual data, so we need to load it explicitly. library(tidyverse) library(stringr) 23.2 String basics You can create strings with either single quotes or double quotes. Unlike other languages, there is no difference in behaviour. I recommend always using &quot;, unless you want to create a string that contains multiple &quot;. string1 &lt;- &quot;This is a string&quot; string2 &lt;- &#39;If I want to include a &quot;quote&quot; inside a string, I use single quotes&#39; If you forget to close a quote, you’ll see +, the continuation character: &gt; &quot;This is a string without a closing quote + + + HELP I&#39;M STUCK If this happen to you, press Escape and try again! To include a literal single or double quote in a string you can use \\ to “escape” it: double_quote &lt;- &quot;\\&quot;&quot; # or &#39;&quot;&#39; single_quote &lt;- &quot;&#39;&quot; # or &quot;&#39;&quot; That means if you want to include a literal backslash, you’ll need to double it up: &quot;\\\\&quot;. Beware that the printed representation of a string is not the same as string itself, because the printed representation shows the escapes. To see the raw contents of the string, use writeLines(): x &lt;- c(&quot;\\&quot;&quot;, &quot;\\\\&quot;) x #&gt; [1] &quot;\\&quot;&quot; &quot;\\\\&quot; writeLines(x) #&gt; &quot; #&gt; \\ There are a handful of other special characters. The most common are &quot;\\n&quot;, newline, and &quot;\\t&quot;, tab, but you can see the complete list by requesting help on &quot;: ?'&quot;', or ?&quot;'&quot;. You’ll also sometimes see strings like &quot;\\u00b5&quot;, this is a way of writing non-English characters that works on all platforms: x &lt;- &quot;\\u00b5&quot; x #&gt; [1] &quot;µ&quot; Multiple strings are often stored in a character vector, which you can create with c(): c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;) #&gt; [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; 23.2.1 String length Base R contains many functions to work with strings but we’ll avoid them because they can be inconsistent, which makes them hard to remember. Instead we’ll use functions from stringr. These have more intuitive names, and all start with str_. For example, str_length() tells you the number of characters in a string: str_length(c(&quot;a&quot;, &quot;R for data science&quot;, NA)) #&gt; [1] 1 18 NA The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions: 23.2.2 Combining strings To combine two or more strings, use str_c(): str_c(&quot;x&quot;, &quot;y&quot;) #&gt; [1] &quot;xy&quot; str_c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) #&gt; [1] &quot;xyz&quot; Use the sep argument to control how they’re separated: str_c(&quot;x&quot;, &quot;y&quot;, sep = &quot;, &quot;) #&gt; [1] &quot;x, y&quot; Like most other functions in R, missing values are contagious. If you want them to print as &quot;NA&quot;, use str_replace_na(): x &lt;- c(&quot;abc&quot;, NA) str_c(&quot;|-&quot;, x, &quot;-|&quot;) #&gt; [1] &quot;|-abc-|&quot; NA str_c(&quot;|-&quot;, str_replace_na(x), &quot;-|&quot;) #&gt; [1] &quot;|-abc-|&quot; &quot;|-NA-|&quot; As shown above, str_c() is vectorised, and it automatically recycles shorter vectors to the same length as the longest: str_c(&quot;prefix-&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), &quot;-suffix&quot;) #&gt; [1] &quot;prefix-a-suffix&quot; &quot;prefix-b-suffix&quot; &quot;prefix-c-suffix&quot; Objects of length 0 are silently dropped. This is particularly useful in conjunction with if: name &lt;- &quot;Hadley&quot; time_of_day &lt;- &quot;morning&quot; birthday &lt;- FALSE str_c( &quot;Good &quot;, time_of_day, &quot; &quot;, name, if (birthday) &quot; and HAPPY BIRTHDAY&quot;, &quot;.&quot; ) #&gt; [1] &quot;Good morning Hadley.&quot; To collapse a vector of strings into a single string, use collapse: str_c(c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), collapse = &quot;, &quot;) #&gt; [1] &quot;x, y, z&quot; 23.2.3 Subsetting strings You can extract parts of a string using str_sub(). As well as the string, str_sub() takes start and end arguments which give the (inclusive) position of the substring: x &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;) str_sub(x, 1, 3) #&gt; [1] &quot;App&quot; &quot;Ban&quot; &quot;Pea&quot; # negative numbers count backwards from end str_sub(x, -3, -1) #&gt; [1] &quot;ple&quot; &quot;ana&quot; &quot;ear&quot; Note that str_sub() won’t fail if the string is too short: it will just return as much as possible: str_sub(&quot;a&quot;, 1, 5) #&gt; [1] &quot;a&quot; You can also use the assignment form of str_sub() to modify strings: str_sub(x, 1, 1) &lt;- str_to_lower(str_sub(x, 1, 1)) x #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;pear&quot; 23.2.4 Locales Above I used str_to_lower() to change the text to lower case. You can also use str_to_upper() or str_to_title(). However, changing case is more complicated than it might at first appear because different languages have different rules for changing case. You can pick which set of rules to use by specifying a locale: # Turkish has two i&#39;s: with and without a dot, and it # has a different rule for capitalising them: str_to_upper(c(&quot;i&quot;, &quot;ı&quot;)) #&gt; [1] &quot;I&quot; &quot;I&quot; str_to_upper(c(&quot;i&quot;, &quot;ı&quot;), locale = &quot;tr&quot;) #&gt; [1] &quot;İ&quot; &quot;I&quot; The locale is specified as a ISO 639 language code, which is a two or three letter abbreviation. If you don’t already know the code for your language, Wikipedia has a good list. If you leave the locale blank, it will use the current locale, as provided by your operating system. Another important operation that’s affected by the locale is sorting. The base R order() and sort() functions sort strings using the current locale. If you want robust behaviour across different computers, you may want to use str_sort() and str_order() which take an additional locale argument: x &lt;- c(&quot;apple&quot;, &quot;eggplant&quot;, &quot;banana&quot;) str_sort(x, locale = &quot;en&quot;) # English #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;eggplant&quot; str_sort(x, locale = &quot;haw&quot;) # Hawaiian #&gt; [1] &quot;apple&quot; &quot;eggplant&quot; &quot;banana&quot; 23.2.5 Exercises In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? What stringr function are they equivalent to? How do the functions differ in their handling of NA? In your own words, describe the difference between the sep and collapse arguments to str_c(). Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters? What does str_wrap() do? When might you want to use it? What does str_trim() do? What’s the opposite of str_trim()? Write a function that turns (e.g.) a vector c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2. 23.3 Matching patterns with regular expressions Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful. To learn regular expressions, we’ll use str_view() and str_view_all(). These functions take a character vector and a regular expression, and show you how they match. We’ll start with very simple regular expressions and then gradually get more and more complicated. Once you’ve mastered pattern matching, you’ll learn how to apply those ideas with various stringr functions. 23.3.1 Basic matches The simplest patterns match exact strings: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;an&quot;) The next step up in complexity is ., which matches any character (except a newline): str_view(x, &quot;.a.&quot;) But if “.” matches any character, how do you match the character “.”? You need to use an “escape” to tell the regular expression you want to match it exactly, not use its special behaviour. Like strings, regexps use the backslash, \\, to escape special behaviour. So to match an ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So to create the regular expression \\. we need the string &quot;\\\\.&quot;. # To create the regular expression, we need \\\\ dot &lt;- &quot;\\\\.&quot; # But the expression itself only contains one: writeLines(dot) #&gt; \\. # And this tells R to look for an explicit . str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;bef&quot;), &quot;a\\\\.c&quot;) If \\ is used as an escape character in regular expressions, how do you match a literal \\? Well you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write &quot;\\\\\\\\&quot; — you need four backslashes to match one! x &lt;- &quot;a\\\\b&quot; writeLines(x) #&gt; a\\b str_view(x, &quot;\\\\\\\\&quot;) In this book, I’ll write regular expression as \\. and strings that represent the regular expression as &quot;\\\\.&quot;. 23.3.1.1 Exercises Explain why each of these strings don’t match a \\: &quot;\\&quot;, &quot;\\\\&quot;, &quot;\\\\\\&quot;. How would you match the sequence &quot;'\\? What patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string? 23.3.2 Anchors By default, regular expressions will match any part of a string. It’s often useful to anchor the regular expression so that it matches from the start or end of the string. You can use: ^ to match the start of the string. $ to match the end of the string. x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;^a&quot;) str_view(x, &quot;a$&quot;) To remember which is which, try this mnemonic which I learned from Evan Misshula: if you begin with power (^), you end up with money ($). To force a regular expression to only match a complete string, anchor it with both ^ and $: x &lt;- c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;) str_view(x, &quot;apple&quot;) str_view(x, &quot;^apple$&quot;) You can also match the boundary between words with \\b. I don’t often use this in R, but I will sometimes use it when I’m doing a search in RStudio when I want to find the name of a function that’s a component of other functions. For example, I’ll search for \\bsum\\b to avoid matching summarise, summary, rowsum and so on. 23.3.2.1 Exercises How would you match the literal string &quot;$^$&quot;? Given the corpus of common words in stringr::words, create regular expressions that find all words that: Start with “y”. End with “x” Are exactly three letters long. (Don’t cheat by using str_length()!) Have seven letters or more. Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words. 23.3.3 Character classes and alternatives There are a number of special patterns that match more than one character. You’ve already seen ., which matches any character apart from a newline. There are four other useful tools: \\d: matches any digit. \\s: matches any whitespace (e.g. space, tab, newline). [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. Remember, to create a regular expression containing \\d or \\s, you’ll need to escape the \\ for the string, so you’ll type &quot;\\\\d&quot; or &quot;\\\\s&quot;. A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex. Many people find this more readable. # Look for a literal character that normally has special meaning in a regex str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;a[.]c&quot;) str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;.[*]c&quot;) str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;a[ ]&quot;) This works for most (but not all) regex metacharacters: $ . | ? * + ( ) [ {. Unfortunately, a few characters have special meaning even inside a character class and must be handled with backslash escapes: ] \\ ^ and -. You can use alternation to pick between one or more alternative patterns. For example, abc|d..f will match either ‘“abc”’, or &quot;deaf&quot;. Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz. Like with mathematical expressions, if precedence ever gets confusing, use parentheses to make it clear what you want: str_view(c(&quot;grey&quot;, &quot;gray&quot;), &quot;gr(e|a)y&quot;) 23.3.3.1 Exercises Create regular expressions to find all words that: Start with a vowel. That only contain consonants. (Hint: thinking about matching “not”-vowels.) End with ed, but not with eed. End with ing or ise. Empirically verify the rule “i before e except after c”. Is “q” always followed by a “u”? Write a regular expression that matches a word if it’s probably written in British English, not American English. Create a regular expression that will match telephone numbers as commonly written in your country. 23.3.4 Repetition The next step up in power involves controlling how many times a pattern matches: ?: 0 or 1 +: 1 or more *: 0 or more x &lt;- &quot;1888 is the longest year in Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CC?&quot;) str_view(x, &quot;CC+&quot;) str_view(x, &quot;C[LX]+&quot;) Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+. You can also specify the number of matches precisely: {n}: exactly n {n,}: n or more {,m}: at most m {n,m}: between n and m str_view(x, &quot;C{2}&quot;) str_view(x, &quot;C{2,}&quot;) str_view(x, &quot;C{2,3}&quot;) By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them. This is an advanced feature of regular expressions, but it’s useful to know that it exists: str_view(x, &quot;C{2,3}?&quot;) str_view(x, &quot;C[LX]+?&quot;) 23.3.4.1 Exercises Describe the equivalents of ?, +, * in {m,n} form. Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ &quot;\\\\{.+\\\\}&quot; \\d{4}-\\d{2}-\\d{2} &quot;\\\\\\\\{4}&quot; Create regular expressions to find all words that: Start with three consonants. Have three or more vowels in a row. Have two or more vowel-consonant pairs in a row. Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner. 23.3.5 Grouping and backreferences Earlier, you learned about parentheses as a way to disambiguate complex expressions. Parentheses also create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \\1, \\2 etc. For example, the following regular expression finds all fruits that have a repeated pair of letters. str_view(fruit, &quot;(..)\\\\1&quot;, match = TRUE) (Shortly, you’ll also see how they’re useful in conjunction with str_match().) 23.3.5.1 Exercises Describe, in words, what these expressions will match: (.)\\1\\1 &quot;(.)(.)\\\\2\\\\1&quot; (..)\\1 &quot;(.).\\\\1.\\\\1&quot; &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot; Construct regular expressions to match words that: Start and end with the same character. Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.) Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.) 23.4 Tools Now that you’ve learned the basics of regular expressions, it’s time to learn how to apply them to real problems. In this section you’ll learn a wide array of stringr functions that let you: Determine which strings match a pattern. Find the positions of matches. Extract the content of matches. Replace matches with new values. Split a string based on a match. A word of caution before we continue: because regular expressions are so powerful, it’s easy to try and solve every problem with a single regular expression. In the words of Jamie Zawinski: Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. As a cautionary tale, check out this regular expression that checks if a email address is valid: (?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?: \\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:( ?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\0 31]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\ ](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+ (?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?: (?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n) ?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\ r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n) ?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t] )*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])* )(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*) *:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+ |\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r \\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?: \\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t ]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031 ]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\]( ?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(? :(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(? :\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(? :(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)? [ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*:(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]| \\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt; @,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot; (?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(? :[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[ \\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000- \\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|( ?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,; :\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([ ^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot; .\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\ ]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\ [\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\ r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\] |\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\0 00-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\ .|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@, ;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(? :[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])* (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[ ^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\] ]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)(?:,\\s*( ?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:( ?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[ \\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t ])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t ])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(? :\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+| \\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?: [^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\ ]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n) ?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot; ()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n) ?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt; @,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@, ;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)? (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?: \\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[ &quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t]) *))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]) +|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\ .(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:( ?:\\r\\n)?[ \\t])*))*)?;\\s*) This is a somewhat pathological example (because email addresses are actually surprisingly complex), but is used in real code. See the stackoverflow discussion at http://stackoverflow.com/a/201378 for more details. Don’t forget that you’re in a programming language and you have other tools at your disposal. Instead of creating one complex regular expression, it’s often easier to write a series of simpler regexps. If you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one. 23.4.1 Detect matches To determine if a character vector matches a pattern, use str_detect(). It returns a logical vector the same length as the input: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_detect(x, &quot;e&quot;) #&gt; [1] TRUE FALSE TRUE Remember that when you use a logical vector in a numeric context, FALSE becomes 0 and TRUE becomes 1. That makes sum() and mean() useful if you want to answer questions about matches across a larger vector: # How many common words start with t? sum(str_detect(words, &quot;^t&quot;)) #&gt; [1] 65 # What proportion of common words end with a vowel? mean(str_detect(words, &quot;[aeiou]$&quot;)) #&gt; [1] 0.277 When you have complex logical conditions (e.g. match a or b but not c unless d) it’s often easier to combine multiple str_detect() calls with logical operators, rather than trying to create a single regular expression. For example, here are two ways to find all words that don’t contain any vowels: # Find all words containing at least one vowel, and negate no_vowels_1 &lt;- !str_detect(words, &quot;[aeiou]&quot;) # Find all words consisting only of consonants (non-vowels) no_vowels_2 &lt;- str_detect(words, &quot;^[^aeiou]+$&quot;) identical(no_vowels_1, no_vowels_2) #&gt; [1] TRUE The results are identical, but I think the first approach is significantly easier to understand. If your regular expression gets overly complicated, try breaking it up into smaller pieces, giving each piece a name, and then combining the pieces with logical operations. A common use of str_detect() is to select the elements that match a pattern. You can do this with logical subsetting, or the convenient str_subset() wrapper: words[str_detect(words, &quot;x$&quot;)] #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; str_subset(words, &quot;x$&quot;) #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; Typically, however, your strings will be one column of a data frame, and you’ll want to use filter instead: df &lt;- tibble( word = words, i = seq_along(word) ) df %&gt;% filter(str_detect(words, &quot;x$&quot;)) #&gt; # A tibble: 4 x 2 #&gt; word i #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 box 108 #&gt; 2 sex 747 #&gt; 3 six 772 #&gt; 4 tax 841 A variation on str_detect() is str_count(): rather than a simple yes or no, it tells you how many matches there are in a string: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_count(x, &quot;a&quot;) #&gt; [1] 1 3 1 # On average, how many vowels per word? mean(str_count(words, &quot;[aeiou]&quot;)) #&gt; [1] 1.99 It’s natural to use str_count() with mutate(): df %&gt;% mutate( vowels = str_count(word, &quot;[aeiou]&quot;), consonants = str_count(word, &quot;[^aeiou]&quot;) ) #&gt; # A tibble: 980 x 4 #&gt; word i vowels consonants #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 a 1 1 0 #&gt; 2 able 2 2 2 #&gt; 3 about 3 3 2 #&gt; 4 absolute 4 4 4 #&gt; 5 accept 5 2 4 #&gt; 6 account 6 3 4 #&gt; # … with 974 more rows Note that matches never overlap. For example, in &quot;abababa&quot;, how many times will the pattern &quot;aba&quot; match? Regular expressions say two, not three: str_count(&quot;abababa&quot;, &quot;aba&quot;) #&gt; [1] 2 str_view_all(&quot;abababa&quot;, &quot;aba&quot;) Note the use of str_view_all(). As you’ll shortly learn, many stringr functions come in pairs: one function works with a single match, and the other works with all matches. The second function will have the suffix _all. 23.4.2 Exercises For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls. Find all words that start or end with x. Find all words that start with a vowel and end with a consonant. Are there any words that contain at least one of each different vowel? What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) 23.4.3 Extract matches To extract the actual text of a match, use str_extract(). To show that off, we’re going to need a more complicated example. I’m going to use the Harvard sentences, which were designed to test VOIP systems, but are also useful for practicing regexps. These are provided in stringr::sentences: length(sentences) #&gt; [1] 720 head(sentences) #&gt; [1] &quot;The birch canoe slid on the smooth planks.&quot; #&gt; [2] &quot;Glue the sheet to the dark blue background.&quot; #&gt; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; #&gt; [4] &quot;These days a chicken leg is a rare dish.&quot; #&gt; [5] &quot;Rice is often served in round bowls.&quot; #&gt; [6] &quot;The juice of lemons makes fine punch.&quot; Imagine we want to find all sentences that contain a colour. We first create a vector of colour names, and then turn it into a single regular expression: colours &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) colour_match #&gt; [1] &quot;red|orange|yellow|green|blue|purple&quot; Now we can select the sentences that contain a colour, and then extract the colour to figure out which one it is: has_colour &lt;- str_subset(sentences, colour_match) matches &lt;- str_extract(has_colour, colour_match) head(matches) #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;blue&quot; Note that str_extract() only extracts the first match. We can see that most easily by first selecting all the sentences that have more than 1 match: more &lt;- sentences[str_count(sentences, colour_match) &gt; 1] str_view_all(more, colour_match) str_extract(more, colour_match) #&gt; [1] &quot;blue&quot; &quot;green&quot; &quot;orange&quot; This is a common pattern for stringr functions, because working with a single match allows you to use much simpler data structures. To get all matches, use str_extract_all(). It returns a list: str_extract_all(more, colour_match) #&gt; [[1]] #&gt; [1] &quot;blue&quot; &quot;red&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;green&quot; &quot;red&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;orange&quot; &quot;red&quot; You’ll learn more about lists in lists and iteration. If you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest: str_extract_all(more, colour_match, simplify = TRUE) #&gt; [,1] [,2] #&gt; [1,] &quot;blue&quot; &quot;red&quot; #&gt; [2,] &quot;green&quot; &quot;red&quot; #&gt; [3,] &quot;orange&quot; &quot;red&quot; x &lt;- c(&quot;a&quot;, &quot;a b&quot;, &quot;a b c&quot;) str_extract_all(x, &quot;[a-z]&quot;, simplify = TRUE) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;a&quot; &quot;&quot; &quot;&quot; #&gt; [2,] &quot;a&quot; &quot;b&quot; &quot;&quot; #&gt; [3,] &quot;a&quot; &quot;b&quot; &quot;c&quot; 23.4.3.1 Exercises In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem. From the Harvard sentences data, extract: The first word from each sentence. All words ending in ing. All plurals. 23.4.4 Grouped matches Earlier in this chapter we talked about the use of parentheses for clarifying precedence and for backreferences when matching. You can also use parentheses to extract parts of a complex match. For example, imagine we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space. noun &lt;- &quot;(a|the) ([^ ]+)&quot; has_noun &lt;- sentences %&gt;% str_subset(noun) %&gt;% head(10) has_noun %&gt;% str_extract(noun) #&gt; [1] &quot;the smooth&quot; &quot;the sheet&quot; &quot;the depth&quot; &quot;a chicken&quot; &quot;the parked&quot; #&gt; [6] &quot;the sun&quot; &quot;the huge&quot; &quot;the ball&quot; &quot;the woman&quot; &quot;a helps&quot; str_extract() gives us the complete match; str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group: has_noun %&gt;% str_match(noun) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; #&gt; [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; #&gt; [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; #&gt; [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; #&gt; [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; #&gt; [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; #&gt; [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; #&gt; [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; #&gt; [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; (Unsurprisingly, our heuristic for detecting nouns is poor, and also picks up adjectives like smooth and parked.) If your data is in a tibble, it’s often easier to use tidyr::extract(). It works like str_match() but requires you to name the matches, which are then placed in new columns: tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;article&quot;, &quot;noun&quot;), &quot;(a|the) ([^ ]+)&quot;, remove = FALSE ) #&gt; # A tibble: 720 x 3 #&gt; sentence article noun #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 The birch canoe slid on the smooth planks. the smooth #&gt; 2 Glue the sheet to the dark blue background. the sheet #&gt; 3 It&#39;s easy to tell the depth of a well. the depth #&gt; 4 These days a chicken leg is a rare dish. a chicken #&gt; 5 Rice is often served in round bowls. &lt;NA&gt; &lt;NA&gt; #&gt; 6 The juice of lemons makes fine punch. &lt;NA&gt; &lt;NA&gt; #&gt; # … with 714 more rows Like str_extract(), if you want all matches for each string, you’ll need str_match_all(). 23.4.4.1 Exercises Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. Find all contractions. Separate out the pieces before and after the apostrophe. 23.4.5 Replacing matches str_replace() and str_replace_all() allow you to replace matches with new strings. The simplest use is to replace a pattern with a fixed string: x &lt;- c(&quot;apple&quot;, &quot;pear&quot;, &quot;banana&quot;) str_replace(x, &quot;[aeiou]&quot;, &quot;-&quot;) #&gt; [1] &quot;-pple&quot; &quot;p-ar&quot; &quot;b-nana&quot; str_replace_all(x, &quot;[aeiou]&quot;, &quot;-&quot;) #&gt; [1] &quot;-ppl-&quot; &quot;p--r&quot; &quot;b-n-n-&quot; With str_replace_all() you can perform multiple replacements by supplying a named vector: x &lt;- c(&quot;1 house&quot;, &quot;2 cars&quot;, &quot;3 people&quot;) str_replace_all(x, c(&quot;1&quot; = &quot;one&quot;, &quot;2&quot; = &quot;two&quot;, &quot;3&quot; = &quot;three&quot;)) #&gt; [1] &quot;one house&quot; &quot;two cars&quot; &quot;three people&quot; Instead of replacing with a fixed string you can use backreferences to insert components of the match. In the following code, I flip the order of the second and third words. sentences %&gt;% str_replace(&quot;([^ ]+) ([^ ]+) ([^ ]+)&quot;, &quot;\\\\1 \\\\3 \\\\2&quot;) %&gt;% head(5) #&gt; [1] &quot;The canoe birch slid on the smooth planks.&quot; #&gt; [2] &quot;Glue sheet the to the dark blue background.&quot; #&gt; [3] &quot;It&#39;s to easy tell the depth of a well.&quot; #&gt; [4] &quot;These a days chicken leg is a rare dish.&quot; #&gt; [5] &quot;Rice often is served in round bowls.&quot; 23.4.5.1 Exercises Replace all forward slashes in a string with backslashes. Implement a simple version of str_to_lower() using replace_all(). Switch the first and last letters in words. Which of those strings are still words? 23.4.6 Splitting Use str_split() to split a string up into pieces. For example, we could split sentences into words: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;) #&gt; [[1]] #&gt; [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [8] &quot;planks.&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; #&gt; [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; #&gt; [8] &quot;rare&quot; &quot;dish.&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; Because each component might contain a different number of pieces, this returns a list. If you’re working with a length-1 vector, the easiest thing is to just extract the first element of the list: &quot;a|b|c|d&quot; %&gt;% str_split(&quot;\\\\|&quot;) %&gt;% .[[1]] #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; Otherwise, like the other stringr functions that return a list, you can use simplify = TRUE to return a matrix: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;, simplify = TRUE) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [2,] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; &quot;dark&quot; &quot;blue&quot; #&gt; [3,] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; #&gt; [4,] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; #&gt; [5,] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; #&gt; [,8] [,9] #&gt; [1,] &quot;planks.&quot; &quot;&quot; #&gt; [2,] &quot;background.&quot; &quot;&quot; #&gt; [3,] &quot;a&quot; &quot;well.&quot; #&gt; [4,] &quot;rare&quot; &quot;dish.&quot; #&gt; [5,] &quot;&quot; &quot;&quot; You can also request a maximum number of pieces: fields &lt;- c(&quot;Name: Hadley&quot;, &quot;Country: NZ&quot;, &quot;Age: 35&quot;) fields %&gt;% str_split(&quot;: &quot;, n = 2, simplify = TRUE) #&gt; [,1] [,2] #&gt; [1,] &quot;Name&quot; &quot;Hadley&quot; #&gt; [2,] &quot;Country&quot; &quot;NZ&quot; #&gt; [3,] &quot;Age&quot; &quot;35&quot; Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary()s: x &lt;- &quot;This is a sentence. This is another sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) str_split(x, &quot; &quot;)[[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence.&quot; &quot;This&quot; &quot;is&quot; #&gt; [7] &quot;another&quot; &quot;sentence.&quot; str_split(x, boundary(&quot;word&quot;))[[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; &quot;This&quot; &quot;is&quot; #&gt; [7] &quot;another&quot; &quot;sentence&quot; 23.4.6.1 Exercises Split up a string like &quot;apples, pears, and bananas&quot; into individual components. Why is it better to split up by boundary(&quot;word&quot;) than &quot; &quot;? What does splitting with an empty string (&quot;&quot;) do? Experiment, and then read the documentation. 23.4.7 Find matches str_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them. 23.5 Other types of pattern When you use a pattern that’s a string, it’s automatically wrapped into a call to regex(): # The regular call: str_view(fruit, &quot;nana&quot;) # Is shorthand for str_view(fruit, regex(&quot;nana&quot;)) You can use the other arguments of regex() to control details of the match: ignore_case = TRUE allows characters to match either their uppercase or lowercase forms. This always uses the current locale. bananas &lt;- c(&quot;banana&quot;, &quot;Banana&quot;, &quot;BANANA&quot;) str_view(bananas, &quot;banana&quot;) str_view(bananas, regex(&quot;banana&quot;, ignore_case = TRUE)) multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string. x &lt;- &quot;Line 1\\nLine 2\\nLine 3&quot; str_extract_all(x, &quot;^Line&quot;)[[1]] #&gt; [1] &quot;Line&quot; str_extract_all(x, regex(&quot;^Line&quot;, multiline = TRUE))[[1]] #&gt; [1] &quot;Line&quot; &quot;Line&quot; &quot;Line&quot; comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: &quot;\\\\ &quot;. phone &lt;- regex(&quot; \\\\(? # optional opening parens (\\\\d{3}) # area code [) -]? # optional closing parens, space, or dash (\\\\d{3}) # another three numbers [ -]? # optional space or dash (\\\\d{3}) # three more numbers &quot;, comments = TRUE) str_match(&quot;514-791-8141&quot;, phone) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] &quot;514-791-814&quot; &quot;514&quot; &quot;791&quot; &quot;814&quot; dotall = TRUE allows . to match everything, including \\n. There are three other functions you can use instead of regex(): fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example. microbenchmark::microbenchmark( fixed = str_detect(sentences, fixed(&quot;the&quot;)), regex = str_detect(sentences, &quot;the&quot;), times = 20 ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval #&gt; fixed 62 72.9 114 104 114 414 20 #&gt; regex 306 311.1 372 319 338 740 20 Beware using fixed() with non-English data. It is problematic because there are often multiple ways of representing the same character. For example, there are two ways to define “á”: either as a single character or as an “a” plus an accent: a1 &lt;- &quot;\\u00e1&quot; a2 &lt;- &quot;a\\u0301&quot; c(a1, a2) #&gt; [1] &quot;á&quot; &quot;á&quot; a1 == a2 #&gt; [1] FALSE They render identically, but because they’re defined differently, fixed() doesn’t find a match. Instead, you can use coll(), defined next, to respect human character comparison rules: str_detect(a1, fixed(a2)) #&gt; [1] FALSE str_detect(a1, coll(a2)) #&gt; [1] TRUE coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. Unfortunately different parts of the world use different rules! # That means you also need to be aware of the difference # when doing case insensitive matches: i &lt;- c(&quot;I&quot;, &quot;İ&quot;, &quot;i&quot;, &quot;ı&quot;) i #&gt; [1] &quot;I&quot; &quot;İ&quot; &quot;i&quot; &quot;ı&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE)) #&gt; [1] &quot;I&quot; &quot;i&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE, locale = &quot;tr&quot;)) #&gt; [1] &quot;İ&quot; &quot;i&quot; Both fixed() and regex() have ignore_case arguments, but they do not allow you to pick the locale: they always use the default locale. You can see what that is with the following code; more on stringi later. stringi::stri_locale_info() #&gt; $Language #&gt; [1] &quot;en&quot; #&gt; #&gt; $Country #&gt; [1] &quot;US&quot; #&gt; #&gt; $Variant #&gt; [1] &quot;&quot; #&gt; #&gt; $Name #&gt; [1] &quot;en_US&quot; The downside of coll() is speed; because the rules for recognising which characters are the same are complicated, coll() is relatively slow compared to regex() and fixed(). As you saw with str_split() you can use boundary() to match boundaries. You can also use it with the other functions: x &lt;- &quot;This is a sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) str_extract_all(x, boundary(&quot;word&quot;)) #&gt; [[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; 23.5.1 Exercises How would you find all strings containing \\ with regex() vs. with fixed()? What are the five most common words in sentences? 23.6 Other uses of regular expressions There are two useful function in base R that also use regular expressions: apropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function. apropos(&quot;replace&quot;) #&gt; [1] &quot;%+replace%&quot; &quot;replace&quot; &quot;replace_na&quot; #&gt; [4] &quot;setReplaceMethod&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; #&gt; [7] &quot;str_replace_na&quot; &quot;theme_replace&quot; dir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with: head(dir(pattern = &quot;\\\\.Rmd$&quot;)) #&gt; [1] &quot;communicate-plots.Rmd&quot; &quot;communicate.Rmd&quot; &quot;datetimes.Rmd&quot; #&gt; [4] &quot;EDA.Rmd&quot; &quot;explore.Rmd&quot; &quot;factors.Rmd&quot; (If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx()): 23.7 stringi stringr is built on top of the stringi package. stringr is useful when you’re learning because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions. stringi, on the other hand, is designed to be comprehensive. It contains almost every function you might ever need: stringi has 244 functions to stringr’s 49. If you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_. 23.7.1 Exercises Find the stringi functions that: Count the number of words. Find duplicated strings. Generate random text. How do you control the language that stri_sort() uses for sorting? "],
["tibbles.html", "24 Tibbles 24.1 Introducción 24.2 Creando tibbles 24.3 Tibbles vs. data.frame 24.4 Interactuando con código previo 24.5 Ejercicios", " 24 Tibbles 24.1 Introducción A lo largo de este libro, trabajaremos con “tibbles” (pronunciado /tibl/) en lugar de los tradicionales data.frames de R. Los tibbles son data frames, que modifican algunas características antiguas para hacernos la vida más fácil. R es un lenguaje viejo y algunas cosas que eran útiles hace 10 o 20 años, actualmente pueden resultar inconvenientes. Es difícil modificar R base sin alterar el código existente, así que la mayor parte de la innovación ocurre en paquetes. Aquí describiremos el paquete tibble, que provee data frames prácticos que facilitan el trabajo con el tidyverse. La mayoría de las veces voy a usar el término tibble y data frame de manera indistinta; para referirme al data frame de R lo voy a llamar data.frame. Si luego de leer este capítulo te quedas con ganas de aprender más sobre tibbles, quizás disfrutes vignette(&quot;tibble&quot;). 24.1.1 Requisitos En este capítulo exploraré el paquete tibble, parte de los paquetes principales del tidyverse. library(tidyverse) 24.2 Creando tibbles La mayoría de las funciones que usarás en este libro producen tibbles, ya que éstos son una de las características principales de tidyverse. La mayoría de los paquetes de R suelen usar data frames clásicos, así que quizás quieras convertir un data frame en un tibble. Esto lo puedes hacer con as_tibble(): as_tibble(iris) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; # … with 144 more rows Puedes crear un nuevo tibble a partir de vectores individuales con tibble(). Esta función recicla vectores de longitud 1 automáticamente y te permite usar variables creadas dentro de la propia función, como se muestra abajo. tibble( x = 1:5, y = 1, z = x^2 + y ) #&gt; # A tibble: 5 x 3 #&gt; x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2 #&gt; 2 2 1 5 #&gt; 3 3 1 10 #&gt; 4 4 1 17 #&gt; 5 5 1 26 Si ya estás familiarizado con data.frame(), es importante que tomes en cuenta que tibble() hace menos cosas: nunca cambia el tipo de los inputs (p.e. ¡nunca convierte caracteres en factores!), nunca cambia el nombre de las variables, y nunca asigna nombres a las filas. Un tibble puede usar nombres de columnas que no son nombres de variables válidos en R; es decir nombres no sintácticos. Por ejemplo, pueden empezar con un caracter diferente a una letra, o contener caracteres poco comunes, como espacios. Para referirse a estas variables, tienes que rodearlos de acentos graves, `: tb &lt;- tibble( `:)` = &quot;sonrisa&quot;, ` ` = &quot;espacio&quot;, `2000` = &quot;número&quot; ) tb #&gt; # A tibble: 1 x 3 #&gt; `:)` ` ` `2000` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 sonrisa espacio número También necesitarás los espacios abiertos al trabajar con estas variables en otros paquetes, como ggplot2, dplyr y tidyr. Otra forma de crear un tibble es con tribble(), que es una abreviación de tibble transpuesto. Esta función está pensada para realizar la entrada de datos en el código: los nombres de las columnas se definen con fórmulas (comienzan con ~), y cada entrada está separada por comas. Esto permite escribir pocos datos de manera legible. tribble( ~x, ~y, ~z, #--|--|---- &quot;a&quot;, 2, 3.6, &quot;b&quot;, 1, 8.5 ) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 2 3.6 #&gt; 2 b 1 8.5 Usualmente agrego un comentario para dejar en claro cuál es el encabezado (esta línea debe empezar con #). 24.3 Tibbles vs. data.frame Existen dos diferencias principales entre el uso de un tibble y un data.frame clásico: la impresión en la consola y la selección de los subconjuntos. 24.3.1 Impresión en la consola Los tibbles tienen un método de impresión en la consola refinado: sólo muestran las primeras 10 filas, y sólo aquellas columnas que entran en el ancho de la pantalla. Esto simplifica y facilita trabajar con bases de datos grandes. Además del nombre, cada columna muestra su tipo. Esto último es una característica útil tomada de str(). tibble( a = lubridate::now() + runif(1e3) * 86400, b = lubridate::today() + runif(1e3) * 30, c = 1:1e3, d = runif(1e3), e = sample(letters, 1e3, replace = TRUE) ) #&gt; # A tibble: 1,000 x 5 #&gt; a b c d e #&gt; &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2019-05-14 22:13:48 2019-05-21 1 0.368 n #&gt; 2 2019-05-15 16:18:58 2019-05-26 2 0.612 l #&gt; 3 2019-05-15 10:42:37 2019-06-05 3 0.415 p #&gt; 4 2019-05-15 00:03:54 2019-06-04 4 0.212 m #&gt; 5 2019-05-14 20:28:11 2019-06-01 5 0.733 i #&gt; 6 2019-05-15 07:29:08 2019-05-28 6 0.460 n #&gt; # … with 994 more rows Los tibbles están diseñados para no inundar tu consola accidentalmente al mirar data frames muy grandes. Sin embargo, a veces es necesario un output mayor que el que se obtiene por default. Existen algunas opciones que pueden ayudar. Primero, puedes usar print() en el data frame y controlar el número de filas (n) y el ancho (width) mostrado. Por otro lado, width = Inf muestra todas las columnas: vuelos %&gt;% print(n = 10, width = Inf) También puedes controlar las características de impresión, modificando las opciones que estan determinadas por default. options(tibble.print_max = n, tibble.print_min = m): si hay más de n filas, mostrar solo m filas. Usa options(tibble.print_min = Inf) para mostrar siempre todas las filas. Usa options(tibble.width = Inf) para mostrar siempre todas las columnas sin importar el ancho de la pantalla. Puedes ver una lista completa de opciones en la ayuda del paquete con package?tibble. La opción final es usar el visualizador de datos de RStudio para obtener una versión interactiva del data frame completo. Esto también es útil luego de realizar una larga cadena de manipulaciones. vuelos %&gt;% View() 24.3.2 Selección de subconjuntos Hasta ahora, todas las herramientas que aprendiste funcionan con el data frame completo. Si quieres recuperar una variable individual, necesitas algunas herramientas nuevas: $ y [[. Mientras que [[ permite extraer variables usando tanto su nombre como su suposición, con $ sólo se puede extraer mediante el nombre. La única diferencia es que $ permite escribir un poco menos. df &lt;- tibble( x = runif(5), y = rnorm(5) ) # Extraer usando el nombre df$x #&gt; [1] 0.7330 0.2344 0.6604 0.0329 0.4605 df[[&quot;x&quot;]] #&gt; [1] 0.7330 0.2344 0.6604 0.0329 0.4605 # Extraer indicando la posición df[[1]] #&gt; [1] 0.7330 0.2344 0.6604 0.0329 0.4605 Para usarlos con un pipe, necesitarás usar el marcador de posición .: df %&gt;% .$x #&gt; [1] 0.7330 0.2344 0.6604 0.0329 0.4605 df %&gt;% .[[&quot;x&quot;]] #&gt; [1] 0.7330 0.2344 0.6604 0.0329 0.4605 En comparación a un data.frame, los tibbles son más estrictos: nunca funcionan con coincidencias parciales, y generan una advertencia si la columna a la que intentas de acceder no existe. 24.4 Interactuando con código previo Algunas funciones previas no funcionan con tibbles. Si te encuentras uno de esos casos, usa as.data.frame() para convertir un tibble de nuevo en un data.frame: class(as.data.frame(tb)) #&gt; [1] &quot;data.frame&quot; La principal razón de que algunas funciones previas no funcionen con tibbles es la función [. En este libro no usamos mucho [ porque dplyr::filter() y dplyr::select() resuelven los mismos problemas con un código más claro (aprenderás un poco sobre ello en vector subsetting XXX). Con los data frames de R base, [ a veces devuelve un data frame y a veces devuelve un vector. Con tibbles, [ siempre devuelve otro tibble. 24.5 Ejercicios ¿Cómo puedes saber si un objeto es un tibble? (Sugerencia: imprime mtautos en consola, que es un data frame clásico). Compara y contrasta las siguientes operaciones aplicadas a un data.frame y a un tibble equivalente. ¿Qué es diferente? ¿Por qué podría causarte problemas el comportamiento por defecto del data frame? df &lt;- data.frame(abc = 1, xyz = &quot;a&quot;) df$x df[, &quot;xyz&quot;] df[, c(&quot;abc&quot;, &quot;xyz&quot;)] Si tienes el nombre de una variable guardada en un objeto, p.e., var &lt;- &quot;mpg&quot;, ¿cómo puedes extraer esta variable de un tibble? Practica referenciar nombres no sintácticos en el siguiente data frame: Extrayendo la variable llamada 1. Generando un gráfico de dispersión de 1 vs 2. Creando una nueva columna llamada 3 que sea el resultado de la división de 2 por 1. Renombrando las columnas como uno, dos y tres. molesto &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) ¿Qué hace tibble::enframe()? ¿Cuándo lo usarías? ¿Qué opción controla cuántos nombres de columnas adicionales se muestran al pie de un tibble? "],
["datos-ordenados.html", "25 Datos ordenados 25.1 Introducción 25.2 Datos ordenados 25.3 Reunir y Esparcir 25.4 Separar y unir 25.5 Valores faltantes 25.6 Estudio de caso 25.7 Datos no ordenados", " 25 Datos ordenados 25.1 Introducción “Todas las familias felices se parecen unas a otras, pero cada familia infeliz lo es a su manera.” –– León Tolstoy “Todos los datos ordenados se parecen unos a otros, pero cada dato desordenado lo es a su manera” — Hadley Wickham En este capítulo aprenderás una metodología consistente para organizar datos en R, a esta metodología le llamaremos tidy data (datos ordenados). Llevar tus datos a este formato requiere algo de trabajo previo, sin embargo dicho trabajo tiene retorno positivo en el largo plazo. Una vez que tengas tus datos ordenados y las herramientas para ordenar datos que provee el tidyverse, vas a gastar mucho menos tiempo pasando de una forma de representar datos a otra, permietiéndote destinar más tiempo a las preguntas analíticas. Este capítulo te dará una introducción práctica a datos ordenados (o tidy data en inglés) y las herramientas que provee el paquete tidyr. Si desear saber más acerca de la teoría subyacente, puede que te guste el artículo Tidy Data publicado en la revista Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 25.1.1 Prerrequisitos En este capítulo nos enfocaremos en tidyr, un paquete que provee un conjunto de herramientas que te ayudarán a ordenar datos desordenados. tidyr es parte del núcleo del tidyverse. library(tidyverse) library(datos) 25.2 Datos ordenados Puedes representar la misma información de múltiples formas. El ejemplo a continuación muestra los mismos datos ordenados de cuatro manera distintas. Cada dataset muestra los mismos valores de cuatro variables pais, anio, poblacion y casos, pero cada conjunto de datos organiza los valores de forma distinta. tabla1 #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 tabla2 #&gt; # A tibble: 12 x 4 #&gt; pais anio tipo cuenta #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 casos 745 #&gt; 2 Afganistán 1999 población 19987071 #&gt; 3 Afganistán 2000 casos 2666 #&gt; 4 Afganistán 2000 población 20595360 #&gt; 5 Brasil 1999 casos 37737 #&gt; 6 Brasil 1999 población 172006362 #&gt; # … with 6 more rows tabla3 #&gt; # A tibble: 6 x 3 #&gt; pais anio tasa #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 # Dividido en dos tibbles tabla4a # casos #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 745 2666 #&gt; 2 Brasil 37737 80488 #&gt; 3 China 212258 213766 tabla4b # poblacion #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 19987071 20595360 #&gt; 2 Brasil 172006362 174504898 #&gt; 3 China 1272915272 1280428583 Las anteriores son representaciones de los mismos datos subyacentes, pero no todas son igualmente fáciles de usar. Un tipo de conjunto de datos, el conjunto de datos ordenado, será mucho más fácil de trabajar en el tidyverse. Existen tres reglas interrelacionadas que hacen que un conjunto de datos sea ordenado: Cada variable tiene su propia columna. Cada observación tiene su propia fila. Cada valor tiene su propia celda. La figura 25.1 muestra estas reglas visualmente. Figure 25.1: Reglas que hacen que un conjunto de datos sea ordenado: las variables están en columnas, las observaciones en filas, y los valores en celdas. Estas reglas están interrelacionadas ya que es imposible cumplir dos de las tres. Esta interrelación lleva a un conjunto práctico de instrucciones mucho más simple: Coloca cada conjunto de datos en un tibble. Coloca cada variable en una columna. En este ejemplo, solo tabla1 está ordenado. Es la única representación en que cada columna es una variable. ¿Por qué asegurarse de que los datos estén ordenados? Existen dos principales ventajas: Existe una ventaja general de elegir una forma consistente de almacenar datos. Si tienes una estructura de datos consistente, es más fácil aprender las herramientas que funcionan con ella ya que tienen una uniformidad subyacente. Existe una ventaja específica al situar las variables en las columnas ya que permite que la naturaleza vectorizada de R brille. Como habrás aprendido en mutate y summary, muchas de las funciones que vienen con R trabajan con vectores de valores. Esto hace que transformar datos ordenados sea casi natural. dplyr, ggplot2 y el resto de los paquetes del tidyverse están diseñados para trabajar con datos ordenados. Aquí hay algunos ejemplos de cómo se podría trabajar con tabla1. # Calcular tasa por cada 10,000 habitantes tabla1 %&gt;% mutate(tasa = casos / poblacion * 10000) #&gt; # A tibble: 6 x 5 #&gt; pais anio casos poblacion tasa #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afganistán 1999 745 19987071 0.373 #&gt; 2 Afganistán 2000 2666 20595360 1.29 #&gt; 3 Brasil 1999 37737 172006362 2.19 #&gt; 4 Brasil 2000 80488 174504898 4.61 #&gt; 5 China 1999 212258 1272915272 1.67 #&gt; 6 China 2000 213766 1280428583 1.67 # Calcular casos por anio tabla1 %&gt;% count(anio, wt = casos) #&gt; # A tibble: 2 x 2 #&gt; anio n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1999 250740 #&gt; 2 2000 296920 # Visualizar cambios en el tiempo library(ggplot2) ggplot(tabla1, aes(anio, casos)) + geom_line(aes(group = pais), colour = &quot;grey50&quot;) + geom_point(aes(colour = pais)) 25.2.1 Ejercicios Usando prosa, describe como las variables y observaciones se organizan en las tablas de ejemplo. Calcula la tasa para las tablas tabla2 y tabla4a + tabla4b. Necesitarás las siguientes operaciones: Extrae el número de casos de tuberculosis por país y año. Extrae la población por país y año. Divide los casos por la población y multiplica por 10000. Inserta los datos en el lugar adecuado. ¿Cuál representación es más fácil de trabajar? ¿Cuál es la más difícil? ¿Por qué? Recrea el gráfico que muestra el cambio en el número de casos usando la tabla2 en lugar de la tabla1. ¿Qué debes hacer en primer lugar? 25.3 Reunir y Esparcir Los principios de los datos ordenados parecen tan obvios que te preguntarás si acaso vas a encontrar un dataset que no esté ordenado. Desafortunadamente, gran parte de los datos que vas a encontrar están desordenados. Existen dos principales razones para esto: La mayoría de las personas no están familiarizadas con los principios de datos ordenados y es difícil derivarlos por cuenta propia a menos que pases mucho tiempo trabajando con datos. Los datos a menudo están organizados para facilitar tareas distintas del análisis. Por ejemplo, los datos se organizan para que su registro sea lo más sencillo posible. Esto significa que para la mayoría de los análisis, necesitarás ordenar los datos. El primer paso siempre es entender el significado de las variables y observaciones. Esto a veces es fácil, otras veces deberás consultar con quienes crearon el dataset. El segundo paso es resolver uno de los siguientes problemas frecuentes: Una variable se esparce entre varias columnas Una observación se esparce entre múltiples filas. Típicamente un dataset tiene uno de los problemas, ¡si contiene ambos significa que tienes muy mala suerte! Para solucionar estos problemas necesitarás las dos funciones más importantes de tidyr: gather() (reunir) y spread() (esparcir). 25.3.1 Gather Un problema común se tiene cuando en un dataset los nombres de las columnas no representan nombres de variables, sino que representan los valores de una variable. Tomando el caso de la tabla4a: los nombres de las columnas 1999 y 2000 representan los valores de la variable anio y cada fila representa dos observaciones en lugar de una. tabla4a #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 745 2666 #&gt; 2 Brasil 37737 80488 #&gt; 3 China 212258 213766 Para ordenar un dataset como este necesitamos reunir (gather) tales columnas en un nuevo par de variables. Para describir dicha operación necesitamos tres parámetros: El conjunto de columnas que representan valores y no variables. En este ejemplo son las columnas 1999 y 2000. El nombre de la variable cuyos valores forman los nombres de las columnas. Llamaremos a esto key (llave) y en este caso corresponde a anio. El nombre de la variable cuyos valores se esparcen por las celdas. Llamaremos a esto value (valor) y en este caso corresponde al número de casos. Juntando estos parámetros se puede realizar una llamada a gather(): tabla4a %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;casos&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais anio casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 #&gt; 2 Brasil 1999 37737 #&gt; 3 China 1999 212258 #&gt; 4 Afganistán 2000 2666 #&gt; 5 Brasil 2000 80488 #&gt; 6 China 2000 213766 Las columnas a reunir quedan seleccionadas siguiendo el estilo de notación de dplyr::select(). En este caso hay dos columnas, por lo que las listamos individualmente. Nota que “1999” y “2000” son nombres no-sintáxicos (debido a que no comienzan con una letra) por lo que los rodeamos con acentos graves (o backticks, en inglés). Para refrescar tu memoria respecto de la selección de columnas, consulta select. Figure 25.2: Reuniendo tabla4 a un formato ordenado. En el resultado final, las columnas reunidas se eliminan y obtenemos la nuevas variables key y value. De otro modo, la relacién entre las variables originales se mantiene. Visualmente, esto se observa en la Figura 25.2. Podemos usar gather() para ordenar tabla4b de modo similar. La única diferencia es la variable almacenada en los valores de las celdas: tabla4b %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;poblacion&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais anio poblacion #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 19987071 #&gt; 2 Brasil 1999 172006362 #&gt; 3 China 1999 1272915272 #&gt; 4 Afganistán 2000 20595360 #&gt; 5 Brasil 2000 174504898 #&gt; 6 China 2000 1280428583 Para combinar las versiones ordenadas de tabla4a y tabla4b en un único tibble, necesitamos usar dplyr::left_join(), función que aprenderás en [datos relacionales]. tidy4a &lt;- tabla4a %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;casos&quot;) tidy4b &lt;- tabla4b %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;poblacion&quot;) left_join(tidy4a, tidy4b) #&gt; Joining, by = c(&quot;pais&quot;, &quot;anio&quot;) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Brasil 1999 37737 172006362 #&gt; 3 China 1999 212258 1272915272 #&gt; 4 Afganistán 2000 2666 20595360 #&gt; 5 Brasil 2000 80488 174504898 #&gt; 6 China 2000 213766 1280428583 25.3.2 Spread Extender (spread) es lo opuesto de gather. Lo usas cuando una observación aparece en múltiples filas. Por ejemplo, toma la tabla tabla2: una observación es un país en un año, pero cada observación aparece en dos filas. tabla2 #&gt; # A tibble: 12 x 4 #&gt; pais anio tipo cuenta #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 casos 745 #&gt; 2 Afganistán 1999 población 19987071 #&gt; 3 Afganistán 2000 casos 2666 #&gt; 4 Afganistán 2000 población 20595360 #&gt; 5 Brasil 1999 casos 37737 #&gt; 6 Brasil 1999 población 172006362 #&gt; # … with 6 more rows Para ordenar esto, primero analiza la representación de un modo similar a cómo se haría con gather(). Esta vez, sin embargo, necesitamos únicamente dos parámetros: La columna que contiene los nombres de las variables, la columna key. En este caso corresponde a tipo. La columna que contiene valores de múltiples variables, la columna value. En este caso corresponde a cuenta. Una vez resuelto esto, podemos usar spread(), como se muestra programáticamente abajo y visualmente en la Figura 25.3. tabla2 %&gt;% spread(key = tipo, value = cuenta) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos población #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 Figure 25.3: Esparcir la tabla2 la vuelve ordenada Como te habrás dado cuenta a partir de los argumentos comunes key y value, gather() y spread() son complementarios. gather() genera tablas estrechas y largas, spread() genera tablas anchas y cortas. 25.3.3 Ejercicios ¿Por qué gather() y spread() no son perfectamente simétricas? Observa cuidadosamente el siguiente ejemplo: stocks &lt;- tibble( anio = c(2015, 2015, 2016, 2016), semestre = c(1, 2, 1, 2), retorno = c(1.88, 0.59, 0.92, 0.17) ) stocks %&gt;% spread(anio, retorno) %&gt;% gather(&quot;anio&quot;, &quot;retorno&quot;, `2015`:`2016`) (Pista: observa los tipos de variables y piensa en los nombres de las columnas) Tanto spread() como gather() tienen el argumento convert (convertir). ¿Qué hace dicho argumento? ¿Por qué falla el siguiente código? tabla4a %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;casos&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais anio casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 #&gt; 2 Brasil 1999 37737 #&gt; 3 China 1999 212258 #&gt; 4 Afganistán 2000 2666 #&gt; 5 Brasil 2000 80488 #&gt; 6 China 2000 213766 ¿Por qué no se puede extender la siguiente tabla? ¿Cómo agregarias una nueva columna para resolver el problema? personas &lt;- tribble( ~nombre, ~llave, ~valor, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;edad&quot;, 45, &quot;Phillip Woods&quot;, &quot;estatura&quot;, 186, &quot;Phillip Woods&quot;, &quot;edad&quot;, 50, &quot;Jessica Cordero&quot;, &quot;edad&quot;, 37, &quot;Jessica Cordero&quot;, &quot;estatura&quot;, 156 ) Ordena la siguiente tabla. ¿Necesitas extenderla o contraerla? ¿Cuáles son las variables? embarazo &lt;- tribble( ~embarazo, ~hombre, ~mujer, &quot;si&quot;, NA, 10, &quot;no&quot;, 20, 12 ) 25.4 Separar y unir Hasta ahora has aprendido a ordenar las tablas tabla2 y tabla4, pero no la tabla3 que tiene un problema diferente: contiene una columna (tasa) la cual contiene dos variables (casos y poblacion). Para solucionar este problema, necesitamos la función separate() (separar). También aprenderás acerca del complemento de separate(): unite() (unir), que se usa cuando una única variable se reparte en varias columnas. 25.4.1 Separar separate() divide una columna en varias columnas, dividiendo de acuerdo a la posición de un carácter separador. Tomando la tabla3: tabla3 #&gt; # A tibble: 6 x 3 #&gt; pais anio tasa #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 La columna tasa contiene las variables casos y poblacion, necesitamos dividir esto en dos variables. separate() toma el nombre de la columna a separar y el nombre de las columnas a donde irá el resultado, tal como se muestra en la Figura 25.4 y el código a continuación. tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;)) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 Figure 25.4: Separar la tabla3 la vuelve ordenada Por defecto, separate() dividirá una columna donde aparezca un carácter no alfanumérico (i.e. un carácter que no es un número o letra). Por ejemplo, en el siguiente código, separate() divide los valores de tasa donde aparece la slash. Si deseas usar un carácter específico para separar una columna, puedes especificarlo en el argumento sep de separate(). Por ejemplo, lo anterior se puede re-escribir del siguiente modo: tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;), sep = &quot;/&quot;) (Formalmente, sep es una expresión regular y aprenderás más sobre esto en strings.) Mira atentamente los tipos de columna: notarás que casos y poblacion son columnas de tipo carácter. Este es el comportamiento por defecto en separate(): preserva el tipo de columna. Aquí, sin embargo, no es muy útil ya que se trata de números. Podemos pedir a separate() que intente convertir a un tipo más adecuado usando convert = TRUE: tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;), convert = TRUE) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 También puedes pasar un vector de enteros a sep. separate() interpreta los enteros como las posiciones donde dividir. Los valores positivos comienzan en 1 al extremo izquierdo de las cadenas de texto; los valores negativos comienzan en -1 al extremo derecho de las cadena de texto. Cuando uses enteros para separar cadenas de textos, el largo de sep debe ser uno menos que el número de nombres en into. Puedes usar este arreglo para separar los últimos dos dígitos de cada año. Esto deja los datos menos ordenados, pero es útil en otros casos, como se verá más adelante. tabla3 %&gt;% separate(anio, into = c(&quot;siglo&quot;, &quot;anio&quot;), sep = 2) #&gt; # A tibble: 6 x 4 #&gt; pais siglo anio tasa #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afganistán 19 99 745/19987071 #&gt; 2 Afganistán 20 00 2666/20595360 #&gt; 3 Brasil 19 99 37737/172006362 #&gt; 4 Brasil 20 00 80488/174504898 #&gt; 5 China 19 99 212258/1272915272 #&gt; 6 China 20 00 213766/1280428583 25.4.2 Unir unite() es el inverso de separate(): combina múltiples columnas en una única columna. Necesitarás esta función con mucha menos frecuencia que separate(), pero aún así es una buena herramienta para tener en el bolsillo trasero. Figure 25.5: Unir la tabla5 la vuelve ordenada Podemos usar unite() para unir las columnas siglo y anio creadas en el ejemplo anterior. Los datos están guardados en datos::tabla5. unite() toma un data frame, el nombre de la nueva variable a crear, y un conjunto de columnas a combinar, las que se especifican siguiendo el estilo de la función dplyr::select(): tabla5 %&gt;% unite(nueva, siglo, anio) #&gt; # A tibble: 6 x 3 #&gt; pais nueva tasa #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afganistán 19_99 745/19987071 #&gt; 2 Afganistán 20_00 2666/20595360 #&gt; 3 Brasil 19_99 37737/172006362 #&gt; 4 Brasil 20_00 80488/174504898 #&gt; 5 China 19_99 212258/1272915272 #&gt; 6 China 20_00 213766/1280428583 En este caso también necesitamos el arguento sep. El separador por defecto es el guión bajo (_) entre los valores de las distintas columnas. Si no queremos una separación usamos &quot;&quot;: tabla5 %&gt;% unite(nueva, siglo, anio, sep = &quot;&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais nueva tasa #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 25.4.3 Ejercicios ¿Qué hacen los argumentos extra y fill en separate()? Experimenta con las diversas opciones a partir de los siguientes datasets de ejemplo. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) Tanto unite() como separate() tienen un argumento remove. ¿Qué es lo que hace? ¿Por qué lo dejarías en FALSE? Compara y contrasta separate() y extract(). ¿Por qué existen tres variaciones de separación (por posición, separador y grupos), pero solo una forma de unir? 25.5 Valores faltantes Cambiar la representación de un dataset conlleva el riesgo de generar valores faltantes. Sorprendentemente, un valor puede perderse de dos formas: Explícita, i.e. aparece como NA. Implícita, i.e. simplemente no aparece en los datos. Ilustremos esta idea con un dataset muy sencillo: acciones &lt;- tibble( anio = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), trimestre = c(1, 2, 3, 4, 2, 3, 4), retorno = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) Existen dos valores faltantes en este dataset: El retorno del cuarto trimestre de 2015 que está explícitamente perdido, debido a que la celda donde el valor debiera estar contiene NA. El retorno del primer semestre de 2016 está implícitamente perdido, debido a que simplemente no aparece en el dataset. Una forma de pensar respecto de esta diferencia es al estilo de un mantra Zen: Un valor perdido explícito es la presencia de una ausencia; un valor perdido implícito es la ausencia de una presencia. La forma en que se representa un dataset puede dejar explícitos los valores implícitos. Por ejemplo, podemos volver explícitos los valores faltantes implícitos al mover los años a las columnas: acciones %&gt;% spread(anio, retorno) #&gt; # A tibble: 4 x 3 #&gt; trimestre `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 NA #&gt; 2 2 0.59 0.92 #&gt; 3 3 0.35 0.17 #&gt; 4 4 NA 2.66 Debido a que estos valores faltantes explícitos pueden no ser tan importantes en otras representaciones de los datos, puedes especificar na.rm = TRUE en gather() para dejar explícitos los valores faltantes implícitos: acciones %&gt;% spread(anio, retorno) %&gt;% gather(anio, retorno, `2015`:`2016`, na.rm = TRUE) #&gt; # A tibble: 6 x 3 #&gt; trimestre anio retorno #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 2 2015 0.59 #&gt; 3 3 2015 0.35 #&gt; 4 2 2016 0.92 #&gt; 5 3 2016 0.17 #&gt; 6 4 2016 2.66 Otra herramienta importante para hacer explícitos los valores faltantes en datos ordenados es complete(): acciones %&gt;% complete(anio, trimestre) #&gt; # A tibble: 8 x 3 #&gt; anio trimestre retorno #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015 1 1.88 #&gt; 2 2015 2 0.59 #&gt; 3 2015 3 0.35 #&gt; 4 2015 4 NA #&gt; 5 2016 1 NA #&gt; 6 2016 2 0.92 #&gt; # … with 2 more rows complete() toma un conjunto de columnas y encuentra todas las combinaciones únicas. Luego se asegura de que el dataset original contiene todos los valores, completando con NAs donde sea necesario. Existe otra herramienta importante que deberías conocer al momento de trabajar con valores faltantes. En algunos casos en que la fuente de datos se ha usado principalmente para ingresar datos, los valores faltantes indican que el valor previo debe arrastrarse hacia adelante: tratamiento &lt;- tribble( ~sujeto, ~tratamiento, ~respuesta, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) Puedes completar los valores faltantes usando fill(). Esta función toma un conjunto de columnas sobre las cuales los valores faltantes son reemplazados por el valor anterior más cercano que se haya reportado (también conocido como el método LOCF, del inglés last observation carried forward). tratamiento %&gt;% fill(sujeto) #&gt; # A tibble: 4 x 3 #&gt; sujeto tratamiento respuesta #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Derrick Whitmore 1 7 #&gt; 2 Derrick Whitmore 2 10 #&gt; 3 Derrick Whitmore 3 9 #&gt; 4 Katherine Burke 1 4 25.5.1 Ejercicios Compara y contrasta el argumento fill que se usa en spread() con complete(). ¿Qué hace el argumento de dirección en fill()? 25.6 Estudio de caso Para finalizar el capítulo, combinemos todo lo que aprendiste para atacar un problema real de ordenamiento de datos. El dataset datos::oms contiene datos de tuberculosis (TB) detallados por año, país, edad, sexo y método de diagnóstico. Los datos provienen del Informe de Tuberculosis de la Organización Mundial de la Salud 2014, disponible en http://www.who.int/tb/country/data/download/en/. Existe abundante información epidemiológica en este dataset, pero es complicado trabajar con estos datos tal como son entregados: oms #&gt; # A tibble: 7,240 x 60 #&gt; pais iso2 iso3 anio nuevos_fpp_h014 nuevos_fpp_h1524 nuevos_fpp_h2534 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afgh… AF AFG 1980 NA NA NA #&gt; 2 Afgh… AF AFG 1981 NA NA NA #&gt; 3 Afgh… AF AFG 1982 NA NA NA #&gt; 4 Afgh… AF AFG 1983 NA NA NA #&gt; 5 Afgh… AF AFG 1984 NA NA NA #&gt; 6 Afgh… AF AFG 1985 NA NA NA #&gt; # … with 7,234 more rows, and 53 more variables: nuevos_fpp_h3534 &lt;int&gt;, #&gt; # nuevos_fpp_h4554 &lt;int&gt;, nuevos_fpp_h5564 &lt;int&gt;, nuevos_fpp_h65 &lt;int&gt;, #&gt; # nuevos_fpp_m014 &lt;int&gt;, nuevos_fpp_m1524 &lt;int&gt;, nuevos_fpp_m2534 &lt;int&gt;, #&gt; # nuevos_fpp_m3534 &lt;int&gt;, nuevos_fpp_m4554 &lt;int&gt;, #&gt; # nuevos_fpp_m5564 &lt;int&gt;, nuevos_fpp_m65 &lt;int&gt;, nuevos_fpn_h014 &lt;int&gt;, #&gt; # nuevos_fpn_h1524 &lt;int&gt;, nuevos_fpn_h2534 &lt;int&gt;, #&gt; # nuevos_fpn_h3534 &lt;int&gt;, nuevos_fpn_h4554 &lt;int&gt;, #&gt; # nuevos_fpn_h5564 &lt;int&gt;, nuevos_fpn_h65 &lt;int&gt;, nuevos_fpn_m014 &lt;int&gt;, #&gt; # nuevos_fpn_m1524 &lt;int&gt;, nuevos_fpn_m2534 &lt;int&gt;, #&gt; # nuevos_fpn_m3534 &lt;int&gt;, nuevos_fpn_m4554 &lt;int&gt;, #&gt; # nuevos_fpn_m5564 &lt;int&gt;, nuevos_fpn_m65 &lt;int&gt;, nuevos_ep_h014 &lt;int&gt;, #&gt; # nuevos_ep_h1524 &lt;int&gt;, nuevos_ep_h2534 &lt;int&gt;, nuevos_ep_h3534 &lt;int&gt;, #&gt; # nuevos_ep_h4554 &lt;int&gt;, nuevos_ep_h5564 &lt;int&gt;, nuevos_ep_h65 &lt;int&gt;, #&gt; # nuevos_ep_m014 &lt;int&gt;, nuevos_ep_m1524 &lt;int&gt;, nuevos_ep_m2534 &lt;int&gt;, #&gt; # nuevos_ep_m3534 &lt;int&gt;, nuevos_ep_m4554 &lt;int&gt;, nuevos_ep_m5564 &lt;int&gt;, #&gt; # nuevos_ep_m65 &lt;int&gt;, nuevosrecaida_h014 &lt;int&gt;, #&gt; # nuevosrecaida_h1524 &lt;int&gt;, nuevosrecaida_h2534 &lt;int&gt;, #&gt; # nuevosrecaida_h3534 &lt;int&gt;, nuevosrecaida_h4554 &lt;int&gt;, #&gt; # nuevosrecaida_h5564 &lt;int&gt;, nuevosrecaida_h65 &lt;int&gt;, #&gt; # nuevosrecaida_m014 &lt;int&gt;, nuevosrecaida_m1524 &lt;int&gt;, #&gt; # nuevosrecaida_m2534 &lt;int&gt;, nuevosrecaida_m3534 &lt;int&gt;, #&gt; # nuevosrecaida_m4554 &lt;int&gt;, nuevosrecaida_m5564 &lt;int&gt;, #&gt; # nuevosrecaida_m65 &lt;int&gt; Este es un ejemplo muy típico de un dataset de la vida real. Contiene columnas redundantes, códigos extraños de variables y muchos valores faltantes. En breve, oms está desordenado y necesitamos varios pasos para ordenarlo. Al igual que dplyr, tidyr está diseñado de modo tal que cada función hace bien una cosa. Esto significa que en una situación real deberás encadenar múltiples verbos. La mejor forma de comenzar es reunir las columnas que no representan variables. Miremos lo que hay: Pareciera ser que pais, iso2 e iso3 son variables redundantes que se refieren al país. anio es claramente una variable. No sabemos aún el significado de las otras columnas, pero dada la estructura de los nombres de las variables (e.g. nuevos_fpp_h014, nuevos_ep_h014, nuevos_ep_m014) parecieran ser valores y no variables. Necesitamos agrupar todas las columnas desde nuevos_fpp_h014 hasta recaidas_m65. No sabemos aún que representa esto, por lo que le daremos el nombre genérico de &quot;llave&quot;. Sabemos que las celdas representan la cuenta de casos, por lo que usaremos la variable casos. Existen múltiples valores faltantes en la representación actual, por lo que de momento usaremos na.rm para centrarnos en los valores que están presentes. oms1 &lt;- oms %&gt;% gather(nuevos_fpp_h014:nuevosrecaida_m65, key = &quot;llave&quot;, value = &quot;casos&quot;, na.rm = TRUE) oms1 #&gt; # A tibble: 76,046 x 6 #&gt; pais iso2 iso3 anio llave casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 nuevos_fpp_h014 0 #&gt; 2 Afghanistan AF AFG 1998 nuevos_fpp_h014 30 #&gt; 3 Afghanistan AF AFG 1999 nuevos_fpp_h014 8 #&gt; 4 Afghanistan AF AFG 2000 nuevos_fpp_h014 52 #&gt; 5 Afghanistan AF AFG 2001 nuevos_fpp_h014 129 #&gt; 6 Afghanistan AF AFG 2002 nuevos_fpp_h014 90 #&gt; # … with 7.604e+04 more rows Podemos tener una noción de la estructura de los valores en la nueva columna llave si hacemos un conteo: oms1 %&gt;% count(llave) #&gt; # A tibble: 56 x 2 #&gt; llave n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 nuevos_ep_h014 1038 #&gt; 2 nuevos_ep_h1524 1026 #&gt; 3 nuevos_ep_h2534 1020 #&gt; 4 nuevos_ep_h3534 1024 #&gt; 5 nuevos_ep_h4554 1020 #&gt; 6 nuevos_ep_h5564 1015 #&gt; # … with 50 more rows Puedes deducir lo siguiente por cuenta propia pensando y experimentando un poco, pero afortunadamente tenemos el diccionario de datos a mano. Este nos dice lo siguiente: Lo que aparece antes del primer _ en las columnas denota si la columna contiene casos nuevos o antiguos de tuberculosis. En este dataset, cada columna contiene nuevos casos. Lo que aparece luego de indicar si se refiere casos nuevos o antiguos es el tipo de tuberculosis: recaida se refiere a casos reincidentes ep se refiere a tuberculosis extra pulmonar fpn se refiere a casos de tuberculosis pulmonar que no se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar negativo) fpp se refiere a casos de tuberculosis pulmonar que se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar positivo) La letra que aparece después del último _ se refiere al sexo de los pacientes. El conjunto de datos agrupa en hombres (h) y mujeres (m). Los números finales se refieren al grupo etareo que se ha organizado en siete categorías: 014 = 0 – 14 años de edad 1524 = 15 – 24 años de edad 2534 = 25 – 34 años de edad 3544 = 35 – 44 años de edad 4554 = 45 – 54 años de edad 5564 = 55 – 64 años de edad 65 = 65 o más años de edad Necesitamos hacer un pequeño cambio al formato de los nombres de las columnas: desafortunadamente lo nombres de las columnas son ligeramente inconsistentes debido a que en lugar de nuevos_recaida tenemos nuevosrecaida (es difícil darse cuenta de esto en esta parte, pero si no lo arreglas habrá errores en los pasos siguientes). Aprenderás sobre str_replace() en strings, pero la idea básica es bastante simple: reemplazar los caracteres “nuevosrecaida” por “nuevos_recaida”. Esto genera nombres de variables consistentes. oms2 &lt;- oms1 %&gt;% mutate(llave = stringr::str_replace(llave, &quot;nuevosrecaida&quot;, &quot;nuevos_recaida&quot;)) oms2 #&gt; # A tibble: 76,046 x 6 #&gt; pais iso2 iso3 anio llave casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 nuevos_fpp_h014 0 #&gt; 2 Afghanistan AF AFG 1998 nuevos_fpp_h014 30 #&gt; 3 Afghanistan AF AFG 1999 nuevos_fpp_h014 8 #&gt; 4 Afghanistan AF AFG 2000 nuevos_fpp_h014 52 #&gt; 5 Afghanistan AF AFG 2001 nuevos_fpp_h014 129 #&gt; 6 Afghanistan AF AFG 2002 nuevos_fpp_h014 90 #&gt; # … with 7.604e+04 more rows Podemos separar los valores en cada código aplicando separate() dos veces. La primera aplicación dividirá los códigos en cada _. oms3 &lt;- oms2 %&gt;% separate(llave, c(&quot;nuevos&quot;, &quot;tipo&quot;, &quot;sexo_edad&quot;), sep = &quot;_&quot;) oms3 #&gt; # A tibble: 76,046 x 8 #&gt; pais iso2 iso3 anio nuevos tipo sexo_edad casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 nuevos fpp h014 0 #&gt; 2 Afghanistan AF AFG 1998 nuevos fpp h014 30 #&gt; 3 Afghanistan AF AFG 1999 nuevos fpp h014 8 #&gt; 4 Afghanistan AF AFG 2000 nuevos fpp h014 52 #&gt; 5 Afghanistan AF AFG 2001 nuevos fpp h014 129 #&gt; 6 Afghanistan AF AFG 2002 nuevos fpp h014 90 #&gt; # … with 7.604e+04 more rows A continuación podemos eliminar la columna nuevos ya que es constante en este dataset. Además eliminaremos iso2 e iso3 ya que son redundantes. oms3 %&gt;% count(nuevos) #&gt; # A tibble: 1 x 2 #&gt; nuevos n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 nuevos 76046 oms4 &lt;- oms3 %&gt;% select(-nuevos, -iso2, -iso3) Luego separamos sexo_edad en sexo y edad dividiendo luego del primer carácter: oms5 &lt;- oms4 %&gt;% separate(sexo_edad, c(&quot;sexo&quot;, &quot;edad&quot;), sep = 1) oms5 #&gt; # A tibble: 76,046 x 6 #&gt; pais anio tipo sexo edad casos #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1997 fpp h 014 0 #&gt; 2 Afghanistan 1998 fpp h 014 30 #&gt; 3 Afghanistan 1999 fpp h 014 8 #&gt; 4 Afghanistan 2000 fpp h 014 52 #&gt; 5 Afghanistan 2001 fpp h 014 129 #&gt; 6 Afghanistan 2002 fpp h 014 90 #&gt; # … with 7.604e+04 more rows ¡Ahora el dataset oms está ordenado! Hemos mostrado el código parte por parte, asignando los resultados intermedios a nuevas variables. Esta no es la forma típica de trabajo. En cambio, lo que se hace es formar incrementalmente un encadenamiento complejo: oms %&gt;% gather(llave, valor, nuevos_fpp_h014:nuevosrecaida_m65, na.rm = TRUE) %&gt;% mutate(llave = stringr::str_replace(llave, &quot;nuevosrecaida&quot;, &quot;nuevos_recaida&quot;)) %&gt;% separate(llave, c(&quot;nuevos&quot;, &quot;tipo&quot;, &quot;sexo_edad&quot;)) %&gt;% select(-nuevos, -iso2, -iso3) %&gt;% separate(sexo_edad, c(&quot;sexo&quot;, &quot;edad&quot;), sep = 1) 25.6.1 Ejercicios En este caso de estudio fijamos na.rm = TRUE para simplificar la verificación de que tenemos los valores correctos. ¿Es esto razonable? Piensa en como los valores faltantes están representados en este dataset. ¿Existen valores faltantes implícitos? ¿Cuál es la diferencia entre NA y cero? ¿Qué ocurre si omites la aplicación de mutate()? (mutate(llave = stringr::str_replace(llave, &quot;nuevosrecaida&quot;, &quot;nuevos_recaida&quot;))) Afirmamos que iso2 e iso3 son redundantes respecto a pais. Confirma esta premisa. Para cada país, año y sexo calcula el total del número de casos de tuberculosis. Crea una visualización informativa de los datos. 25.7 Datos no ordenados Antes de pasar a otros tópicos, es conveniente referirse brevemente a datos no ordenados. Anteriormente en el capítulo, usamos el término peyorativo “desordenados” para referirnos a datos no ordenados. Esto es una sobresimplificación: existen múltiples estructuras de datos debidamente fundamentadas que no corresponden a datos ordenados. Existen dos principales razones para usar otras estructuras de datos: Las representaciones alternativas pueden traer ventajas importantes en términos de desempeño o tamaño. Algunos áreas especializadas han evolucionado y tienen sus propias convenciones para almacenar datos, las que pueden diferir respecto de las convenciones de datos ordenados. Cada uno de estas razones significa que necesitarás algo distinto a un tibble (o data frame). Si tus datos naturalmente se ajustan a una estructura rectangular compuesta de observaciones y variables, pensamos que datos ordenados debería ser tu elección por defecto. Sin embargo, existen buenas razones para usar otras estructuras; datos ordenados no es la única forma. Si quieres aprender más acerca de datos no ordenados, recomendamos fuertemente este artículo del blog de Jeff Leek: http://simplystatistics.org/2016/02/17/non-tidy-data/ "],
["transform.html", "26 Transformación de datos 26.1 Introducción 26.2 Filtrar filas con filter() 26.3 Reordenar las filas con arrange() 26.4 Seleccionar columnas con select() 26.5 Añadir nuevas variables con mutate() 26.6 Resúmenes agrupados con summarise() 26.7 Transformaciones agrupadas (y filtros)", " 26 Transformación de datos 26.1 Introducción La visualización es una herramienta importante para para la generación de conocimiento, sin embargo es raro que obtengas los datos exactamente en la forma correcta que necesitas. A menudo necesitarás crear algunas variables nuevas o resúmenes, o tal vez solo quieras cambiar el nombre de las variables o reordenar las observaciones para facilitar el trabajo con los datos. En este capítulo aprenderás cómo hacer todo eso (¡y más!), incluyendo cómo transformar tus datos utilizando el paquete dplyr y el uso de un nuevo conjunto de datos sobre salida de vuelos de la ciudad de Nueva York en el año 2013. 26.1.1 Prerequisitos En este capítulo nos enfocaremos en cómo usar el paquete dplyr, otro miembro central de tidyverse. Ilustraremos las ideas clave con el conjunto de datos vuelos que aparecen originalmente en el paquete nycflights13, pero para este caso utilizaremos la versión traducida incluida en el paquete datos y usaremos ggplot2 para ayudarnos a comprender los datos. #devtools::install_github(&quot;cienciadedatos/datos&quot;) library(datos) library(tidyverse) Toma nota acerca del mensaje de conflictos que se imprime cuando carga el paquete tidyverse. Te dice que dplyr sobrescribe algunas funciones en R base. Si deseas usar la versión base de estas funciones después de cargar dplyr, necesitarás usar sus nombres completos: stats::filter() y stats::lag(). 26.1.2 vuelos Para explorar los verbos básicos de manipulación de datos de dplyr, usaremos vuelos. Este conjunto de datos contiene todos los vuelos 336, 776 que partieron de la ciudad de Nueva York durante el 2013. Los datos provienen del Departamento de Estadísticas de Transporte de los Estados Unidos, y están documentados en ?vuelos. vuelos #&gt; # A tibble: 336,776 x 19 #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 533 529 4 #&gt; 3 2013 1 1 542 540 2 #&gt; 4 2013 1 1 544 545 -1 #&gt; 5 2013 1 1 554 600 -6 #&gt; 6 2013 1 1 554 558 -4 #&gt; # … with 3.368e+05 more rows, and 13 more variables: #&gt; # horario_llegada &lt;int&gt;, llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, #&gt; # aerolinea &lt;chr&gt;, vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, #&gt; # destino &lt;chr&gt;, tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, #&gt; # minuto &lt;dbl&gt;, fecha_hora &lt;dttm&gt; Es posible que observes que este conjunto de datos se imprime de una forma un poco diferente a otros que podrías haber utilizado en el pasado: solo muestra las primeras filas y todas las columnas que caben en tu pantalla. Para ver todo el conjunto de datos, puedes ejecutar View(vuelos) que abrirá el conjunto de datos en el visor de RStudio. En este caso se imprime de manera diferente porque es un tibble. Los tibbles son marcos de datos, pero ligeramente ajustados para que funcionen mejor en el tidyverse. Por ahora, no necesitas preocuparte por las diferencias; hablaremos en más detalle de los tibbles en wrangle. También podrás haber notado la fila de tres (o cuatro) abreviaturas de letras debajo de los nombres de las columnas. Estos describen el tipo de cada variable: int significa enteros. dbl significa dobles, o números reales. chr significa vectores de caracteres o cadenas. dttm significa fechas y horas (una fecha + una hora). Hay otros tres tipos comunes de variables que no se usan en este conjunto de datos, pero que encontrará más adelante en el libro: lgl significa lógico, vectores que solo contienen TRUE (verdadero) o FALSE (falso). fctr significa factores, que R usa para representar variables categóricas con valores posibles fijos. date significa fechas. 26.1.3 Lo básico de dplyr En este capítulo, aprenderás las cinco funciones clave de dplyr que te permiten resolver la gran mayoría de tus desafíos de manipulación de datos: Filtrar o elegir las observaciones por sus valores (filter() — del inglés filtrar). Reordenar las filas (arrange() — del inglés organizar). Seleccionar las variables por sus nombres (select() — del inglés seleccionar). Crear nuevas variables con transformaciones de variables existentes (mutate() — del inglés mutar o transformar). Contraer muchos valores en un solo resumen (summarise() — del inglés resumir). Todas estas funciones se pueden usar junto con group_by() (del inglés agrupar por), que cambia el alcance de cada función para que actúe ya no sobre todo el conjunto de datos sino grupo por grupo. Estas seis funciones proporcionan los verbos para un lenguaje de manipulación de datos. Todos los verbos funcionan de manera similar: El primer argumento es un data frame. Los argumentos posteriores describen qué hacer con el data frame usando los nombres de las variables (sin comillas). El resultado es un nuevo data frame. En conjunto, estas propiedades hacen que sea fácil encadenar varios pasos simples para lograr un resultado complejo. Vamos a sumergirnos y ver cómo funcionan estos verbos. 26.2 Filtrar filas con filter() filter() te permite filtrar un subconjunto de observaciones basadas en sus valores. El primer argumento es el nombre del data frame. El segundo y los siguientes argumentos son las expresiones que filtran el data frame. Por ejemplo, podemos seleccionar todos los vuelos del 1 de enero con: filter(vuelos, mes == 1, dia == 1) #&gt; # A tibble: 842 x 19 #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 533 529 4 #&gt; 3 2013 1 1 542 540 2 #&gt; 4 2013 1 1 544 545 -1 #&gt; 5 2013 1 1 554 600 -6 #&gt; 6 2013 1 1 554 558 -4 #&gt; # … with 836 more rows, and 13 more variables: horario_llegada &lt;int&gt;, #&gt; # llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, aerolinea &lt;chr&gt;, #&gt; # vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, destino &lt;chr&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, minuto &lt;dbl&gt;, #&gt; # fecha_hora &lt;dttm&gt; Cuando ejecutas esa línea de código, dplyr ejecuta la operación de filtrado y devuelve un nuevo data frame. Las funciones dplyr nunca modifican sus entradas, por lo que si deseas guardar el resultado, necesitarás usar el operador de asignación, &lt;-: ene1 &lt;- filter(vuelos, mes == 1, dia == 1) R imprime los resultados o los guarda en una variable. Si deseas hacer ambas cosas, puedes escribir toda la línea entre paréntesis: (dic25 &lt;- filter(vuelos, mes == 12, dia == 25)) #&gt; # A tibble: 719 x 19 #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 12 25 456 500 -4 #&gt; 2 2013 12 25 524 515 9 #&gt; 3 2013 12 25 542 540 2 #&gt; 4 2013 12 25 546 550 -4 #&gt; 5 2013 12 25 556 600 -4 #&gt; 6 2013 12 25 557 600 -3 #&gt; # … with 713 more rows, and 13 more variables: horario_llegada &lt;int&gt;, #&gt; # llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, aerolinea &lt;chr&gt;, #&gt; # vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, destino &lt;chr&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, minuto &lt;dbl&gt;, #&gt; # fecha_hora &lt;dttm&gt; 26.2.1 Comparaciones Para usar el filtrado de manera efectiva, debes saber cómo seleccionar las observaciones que deseas utilizando los operadores de comparación. R proporciona el conjunto estándar: &gt;, &gt;=, &lt;, &lt;=, != (no igual) y == (igual). Cuando comienzas con R, el error más fácil de cometer es usar = en lugar de == cuando se prueba la igualdad. Cuando esto sucede, obtendrás un error informativo: filter(vuelos, mes = 1) #&gt; `mes` (`mes = 1`) must not be named, do you need `==`? Hay otro problema común que puedes encontrar al usar ==: números de coma flotante. ¡Estos resultados pueden sorprenderte! sqrt(2)^2 == 2 #&gt; [1] FALSE 1 / 49 * 49 == 1 #&gt; [1] FALSE Las computadoras usan aritmética de precisión finita (obviamente no pueden almacenar una cantidad infinita de dígitos), así que recuerda que cada número que ves es una aproximación. En lugar de confiar en ==, usa near() (cercano, en inglés): near(sqrt(2)^2, 2) #&gt; [1] TRUE near(1 / 49 * 49, 1) #&gt; [1] TRUE 26.2.2 Operadores lógicos Múltiples argumentos para filter() se combinan con “y”: cada expresión debe ser verdadera para que una fila se incluya en el output. Para otros tipos de combinaciones, necesitarás usar operadores Booleanos: &amp; es “y”, | es “o”, y ! es “no”. La figura @ref(fig:bool-ops) muestra el conjunto completo de operaciones Booleanas. Figure 26.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. El siguiente código encuentra todos los vuelos que partieron en noviembre o diciembre: filter(vuelos, mes == 11 | mes == 12) El orden de las operaciones no funciona como en español. No puedes escribir filter(vuelos, mes == 11 | 12), que literalmente puede traducirse como “encuentra todos los vuelos que partieron en noviembre o diciembre”. En cambio, encontrará todos los meses que son iguales a 11 | 12, una expresión que resulta en ‘TRUE’ (verdadero). En un contexto numérico (como aquí), ‘TRUE’ se convierte en uno, por lo que encuentra todos los vuelos en enero, no en noviembre o diciembre. ¡Esto es bastante confuso! Una manera rápida y útil para resolver este problema es x %in% y (del inglés x en y). Esto seleccionará cada fila donde x es uno de los valores eny. Podríamos usarlo para reescribir el código de arriba: nov_dic &lt;- filter(vuelos, mes %in% c(11, 12)) A veces puedes simplificar subconjuntos complicados al recordar la ley de De Morgan: !(x &amp; y) es lo mismo que !x | !y, y !(x | y) es lo mismo que !x &amp; !y. Por ejemplo, si deseas encontrar vuelos que no se retrasaron (en llegada o partida) en más de dos horas, puedes usar cualquiera de los dos filtros siguientes: filter(vuelos, !(atraso_llegada &gt; 120 | atraso_salida &gt; 120)) filter(vuelos , atraso_llegada &lt;= 120, atraso_salida &lt;= 120) Además de &amp; y |, R también tiene &amp;&amp; y ||. ¡No los uses aquí! Aprenderás cuándo deberías usarlos en [conditional execution]. Siempre que empieces a usar expresiones complejas de varias partes en filter(), considera convertirlas en variables explícitas. Eso hace que sea mucho más fácil verificar tu trabajo. Aprenderás cómo crear nuevas variables en breve. 26.2.3 Valores faltantes Una característica importante de R que puede hacer que la comparación sea difícil son los valores faltantes, o NAs (del inglés “no disponibles”). NA representa un valor desconocido por lo que los valores perdidos son “contagiosos”: casi cualquier operación que involucre un valor desconocido también será desconocida. NA &gt; 5 #&gt; [1] NA 10 == NA #&gt; [1] NA NA + 10 #&gt; [1] NA NA / 2 #&gt; [1] NA El resultado más confuso es este: NA == NA #&gt; [1] NA Es más fácil entender por qué esto es cierto con un poco más de contexto: # Sea x la edad de María. No sabemos qué edad tiene. x &lt;- NA # Sea y la edad de Juan. No sabemos qué edad tiene. y &lt;- NA # ¿Tienen Juan y María la misma edad? x == y #&gt; [1] NA # ¡No sabemos! Si deseas determinar si falta un valor, usa is.na(): is.na(x) #&gt; [1] TRUE filter() solo incluye filas donde la condición es TRUE; excluye ambos valores FALSE y NA. Si deseas conservar valores perdidos, solicítalos explícitamente: df &lt;- tibble(x = c(1, NA, 3)) filter(df, x &gt; 1) #&gt; # A tibble: 1 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 3 filter(df, is.na(x) | x &gt; 1) #&gt; # A tibble: 2 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 NA #&gt; 2 3 26.2.4 Ejercicios Encuentra todos los vuelos que: Tuvieron un retraso de llegada de dos o más horas Volaron a Houston (IAH oHOU) Fueron operados por United, American o Delta Partieron en verano (julio, agosto y septiembre) Llegaron más de dos horas tarde, pero no salieron tarde Se retrasaron por lo menos una hora, pero repusieron más de 30 minutos en vuelo Partieron entre la medianoche y las 6 a.m. (incluyente) Otra función de dplyr útil para usar filtros es between(). ¿Qué hace? ¿Puedes usarlo para simplificar el código necesario para responder a los desafíos anteriores? ¿Cuántos vuelos tienen datos faltantes de horario_salida? ¿Qué otras variables tienen valores faltantes? ¿Qué representan estas filas? ¿Por qué NA ^ 0 no es faltante? ¿Por qué NA | TRUE no es faltante? ¿Por qué FALSE &amp; NA no es faltante? ¿Puedes descubrir la regla general? (¡NA * 0 es un contraejemplo complicado!) 26.3 Reordenar las filas con arrange() arrange() funciona de manera similar a filter() excepto que en lugar de seleccionar filas, cambia su orden. La función toma un data frame y un conjunto de nombres de columnas (o expresiones más complicadas) para ordenar acorde. Si proporcionas más de un nombre de columna, cada columna adicional se utilizará para romper empates en los valores de las columnas anteriores: arrange(vuelos, anio, mes, dia) #&gt; # A tibble: 336,776 x 19 #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 533 529 4 #&gt; 3 2013 1 1 542 540 2 #&gt; 4 2013 1 1 544 545 -1 #&gt; 5 2013 1 1 554 600 -6 #&gt; 6 2013 1 1 554 558 -4 #&gt; # … with 3.368e+05 more rows, and 13 more variables: #&gt; # horario_llegada &lt;int&gt;, llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, #&gt; # aerolinea &lt;chr&gt;, vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, #&gt; # destino &lt;chr&gt;, tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, #&gt; # minuto &lt;dbl&gt;, fecha_hora &lt;dttm&gt; Usa desc() para reordenar por una columna en orden descendente: `{r} arrange(vuelos, desc(atraso_salida)) ` Los valores faltantes siempre se ordenan al final: df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 2 #&gt; 2 5 #&gt; 3 NA arrange(df, desc(x)) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 5 #&gt; 2 2 #&gt; 3 NA 26.3.1 Ejercicios ¿Cómo podrías usar arrange() para ordenar todos los valores faltantes al comienzo? (Sugerencia: usa is.na()). Ordena vuelos para encontrar los vuelos más retrasados. Encuentra los vuelos que salieron más temprano. Ordena vuelos para encontrar los vuelos más rápidos. ¿Cuáles vuelos viajaron más tiempo? ¿Cuál viajó menos tiempo? 26.4 Seleccionar columnas con select() No es raro obtener conjuntos de datos con cientos o incluso miles de variables. En este caso, el primer desafío a menudo se reduce a las variables que realmente te interesan. select() te permite seleccionar rápidamente un subconjunto útil utilizando operaciones basadas en los nombres de las variables. select() no es muy útil con los datos de los vuelos porque solo tenemos 19 variables, pero aún podemos describir la idea general: # Seleccionar columnas por nombre select(vuelos, anio, mes, dia) #&gt; # A tibble: 336,776 x 3 #&gt; anio mes dia #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 3.368e+05 more rows # Seleccionar todas las columnas entre anio y dia (incluyente) select(vuelos, anio:dia) #&gt; # A tibble: 336,776 x 3 #&gt; anio mes dia #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 3.368e+05 more rows # Seleccionar todas las columnas excepto aquellas entre anio en dia (incluyente) select(vuelos, -(anio:dia)) #&gt; # A tibble: 336,776 x 16 #&gt; horario_salida salida_programa… atraso_salida horario_llegada #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 517 515 2 830 #&gt; 2 533 529 4 850 #&gt; 3 542 540 2 923 #&gt; 4 544 545 -1 1004 #&gt; 5 554 600 -6 812 #&gt; 6 554 558 -4 740 #&gt; # … with 3.368e+05 more rows, and 12 more variables: #&gt; # llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, aerolinea &lt;chr&gt;, #&gt; # vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, destino &lt;chr&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, minuto &lt;dbl&gt;, #&gt; # fecha_hora &lt;dttm&gt; Hay una serie de funciones auxiliares que puedes usar dentro de select(): starts_with(&quot;abc&quot;): coincide con los nombres que comienzan con “abc”. ends_with(&quot;xyz&quot;): coincide con los nombres que terminan con “xyz”. contains(&quot;ijk&quot;): coincide con los nombres que contienen “ijk”. matches(&quot;(.)\\\\1&quot;): selecciona variables que coinciden con una expresión regular. Éste coincide con cualquier variable que contenga caracteres repetidos. Aprenderás más sobre expresiones regulares en strings. num_range(&quot;x&quot;, 1:3): coincide con x1,x2 y x3. Consulta ?select para ver más detalles. select() se puede usar para cambiar el nombre de las variables, pero rara vez es útil porque descarta todas las variables que no se mencionan explícitamente. En su lugar, utiliza rename(), que es una variante de select() que mantiene todas las variables que no se mencionan explícitamente: rename(vuelos, cola_num = codigo_cola) #&gt; # A tibble: 336,776 x 19 #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 533 529 4 #&gt; 3 2013 1 1 542 540 2 #&gt; 4 2013 1 1 544 545 -1 #&gt; 5 2013 1 1 554 600 -6 #&gt; 6 2013 1 1 554 558 -4 #&gt; # … with 3.368e+05 more rows, and 13 more variables: #&gt; # horario_llegada &lt;int&gt;, llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, #&gt; # aerolinea &lt;chr&gt;, vuelo &lt;int&gt;, cola_num &lt;chr&gt;, origen &lt;chr&gt;, #&gt; # destino &lt;chr&gt;, tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, #&gt; # minuto &lt;dbl&gt;, fecha_hora &lt;dttm&gt; Otra opción es usar select() junto con el auxiliar everything() (todo, en inglés). Esto es útil si tienes un grupo de variables que te gustaría mover al comienzo del data frame. select(vuelos, fecha_hora, tiempo_vuelo, everything()) #&gt; # A tibble: 336,776 x 19 #&gt; fecha_hora tiempo_vuelo anio mes dia horario_salida #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013-01-01 05:00:00 227 2013 1 1 517 #&gt; 2 2013-01-01 05:00:00 227 2013 1 1 533 #&gt; 3 2013-01-01 05:00:00 160 2013 1 1 542 #&gt; 4 2013-01-01 05:00:00 183 2013 1 1 544 #&gt; 5 2013-01-01 06:00:00 116 2013 1 1 554 #&gt; 6 2013-01-01 05:00:00 150 2013 1 1 554 #&gt; # … with 3.368e+05 more rows, and 13 more variables: #&gt; # salida_programada &lt;int&gt;, atraso_salida &lt;dbl&gt;, horario_llegada &lt;int&gt;, #&gt; # llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, aerolinea &lt;chr&gt;, #&gt; # vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, destino &lt;chr&gt;, #&gt; # distancia &lt;dbl&gt;, hora &lt;dbl&gt;, minuto &lt;dbl&gt; 26.4.1 Ejercicios Haz una lluvia de ideas de tantas maneras como sea posible para seleccionar horario_salida,atraso_salida,horario_llegada, yatraso_llegada de vuelos. ¿Qué sucede si incluyes el nombre de una variable varias veces en una llamada select()? ¿Qué hace la función one_of()? Por qué podría ser útil en conjunto con este vector? vars &lt;- c (&quot;anio&quot;, &quot;mes&quot;, &quot;dia&quot;, &quot;atraso_salida&quot;, &quot;atraso_llegada&quot;) ¿Te sorprende el resultado de ejecutar el siguiente código? ¿Cómo tratan por defecto las funciones auxiliares de select() a las palabras en mayúsculas o en minúsculas? ¿Cómo puedes cambiar ese comportamiento predeterminado? select(vuelos, contains(&quot;SALIDA&quot;)) 26.5 Añadir nuevas variables con mutate() Además de seleccionar conjuntos de columnas existentes, a menudo es útil crear nuevas columnas en función de columnas existentes. Ese es el trabajo de mutate() (del inglés mutar o transformar). mutate() siempre agrega nuevas columnas al final de un conjunto de datos, así que comenzaremos creando un conjunto de datos más pequeño para que podamos ver las nuevas variables. Recuerda que cuando usas RStudio, la manera más fácil de ver todas las columnas es View(). vuelos_sml &lt;- select(vuelos, anio:dia, starts_with(&quot;atraso&quot;), distancia, tiempo_vuelo ) mutate(vuelos_sml, ganancia = atraso_salida - atraso_llegada, velocidad = distancia / tiempo_vuelo * 60 ) #&gt; # A tibble: 336,776 x 9 #&gt; anio mes dia atraso_salida atraso_llegada distancia tiempo_vuelo #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 #&gt; 2 2013 1 1 4 20 1416 227 #&gt; 3 2013 1 1 2 33 1089 160 #&gt; 4 2013 1 1 -1 -18 1576 183 #&gt; 5 2013 1 1 -6 -25 762 116 #&gt; 6 2013 1 1 -4 12 719 150 #&gt; # … with 3.368e+05 more rows, and 2 more variables: ganancia &lt;dbl&gt;, #&gt; # velocidad &lt;dbl&gt; Ten en cuenta que puedes consultar las columnas que acabas de crear: mutate(vuelos_sml, ganancia = atraso_salida - atraso_llegada, horas = tiempo_vuelo / 60, ganacia_por_hora = ganancia / horas ) #&gt; # A tibble: 336,776 x 10 #&gt; anio mes dia atraso_salida atraso_llegada distancia tiempo_vuelo #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 #&gt; 2 2013 1 1 4 20 1416 227 #&gt; 3 2013 1 1 2 33 1089 160 #&gt; 4 2013 1 1 -1 -18 1576 183 #&gt; 5 2013 1 1 -6 -25 762 116 #&gt; 6 2013 1 1 -4 12 719 150 #&gt; # … with 3.368e+05 more rows, and 3 more variables: ganancia &lt;dbl&gt;, #&gt; # horas &lt;dbl&gt;, ganacia_por_hora &lt;dbl&gt; Si solo quieres conservar las nuevas variables, usa transmute(): transmute(vuelos, ganancia = atraso_salida - atraso_llegada, horas = tiempo_vuelo / 60, ganancia_por_hora = ganancia / horas ) #&gt; # A tibble: 336,776 x 3 #&gt; ganancia horas ganancia_por_hora #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -9 3.78 -2.38 #&gt; 2 -16 3.78 -4.23 #&gt; 3 -31 2.67 -11.6 #&gt; 4 17 3.05 5.57 #&gt; 5 19 1.93 9.83 #&gt; 6 -16 2.5 -6.4 #&gt; # … with 3.368e+05 more rows 26.5.1 Funciones de creación útiles Hay muchas funciones para crear nuevas variables que puedes usar con mutate(). La propiedad clave es que la función debe ser vectorizada: debe tomar un vector de valores como input, y devolver un vector con el mismo número de valores como output. No hay forma de enumerar todas las posibles funciones que podrías usar, pero aquí hay una selección de funciones que son útiles frecuentemente: Operadores aritméticos: +, -,*,/,^. Todos están vectorizados, usando las llamadas “reglas de reciclaje”. Si un parámetro es más corto que el otro, se extenderá automáticamente para tener la misma longitud. Esto es muy útil cuando uno de los argumentos es un solo número: tiempo_vuelo / 60, horas * 60 + minuto, etc. Los operadores aritméticos también son útiles junto con las funciones de agregar que aprenderás más adelante. Por ejemplo, x / sum(x) calcula la proporción de un total, y y - mean(y) calcula la diferencia de la media. Aritmética modular: %/% (división entera) y %% (resto), donde x == y * (x %/% y) + (x %% y). La aritmética modular es una herramienta útil porque te permite dividir enteros en partes. Por ejemplo, en el conjunto de datos de vuelos, puedes calcular hora yminutos de horario_salida con: transmute(vuelos, horario_salida, hora = horario_salida %/% 100, minuto = horario_salida %% 100 ) #&gt; # A tibble: 336,776 x 3 #&gt; horario_salida hora minuto #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 517 5 17 #&gt; 2 533 5 33 #&gt; 3 542 5 42 #&gt; 4 544 5 44 #&gt; 5 554 5 54 #&gt; 6 554 5 54 #&gt; # … with 3.368e+05 more rows Logaritmos: log(), log2(), log10(). Los logaritmos son increíblemente útiles como transformación para trabajar con datos con múltiples órdenes de magnitud. También convierten las relaciones multiplicativas en aditivas, una característica que retomaremos en modelamiento. En igualdad de condiciones, recomendamos usar log2() porque es más fácil de interpretar: una diferencia de 1 en la escala de registro corresponde a la duplicación de la escala original y una diferencia de -1 corresponde a dividir a la mitad. Rezagos: lead() y lag() te permiten referirte a un valor adelante o un valor atrás (con rezago). Esto te permite calcular las diferencias móviles (por ejemplo, x - lag(x)) o encontrar cuándo cambian los valores (x! = lag (x)). Estos comandos son más útiles cuando se utilizan junto con group_by(), que lo aprenderás en breve. (x &lt;- 1:10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 lag(x) #&gt; [1] NA 1 2 3 4 5 6 7 8 9 lead(x) #&gt; [1] 2 3 4 5 6 7 8 9 10 NA Agregados acumulativos y móviles: R proporciona funciones para ejecutar sumas, productos, mínimos y máximos: cumsum(), cumprod(), cummin(), cummax(); y dplyr proporciona cummean() para las medias acumuladas. Si necesitas calcular agregados móviles (es decir, una suma calculada en una ventana móvil), prueba el paquete RcppRoll. x #&gt; [1] 1 2 3 4 5 6 7 8 9 10 cumsum(x) #&gt; [1] 1 3 6 10 15 21 28 36 45 55 cummean(x) #&gt; [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 Comparaciones lógicas: &lt;, &lt;=, &gt;, &gt;=, != sobre las cuales aprendiste antes. Si estás haciendo una secuencia compleja de operaciones lógicas, es a menudo una buena idea almacenar los valores provisionales en nuevas variables para que puedas comprobar que cada paso funciona como se espera. Ordenamiento: hay una serie de funciones de ordenamiento, pero deberías comenzar con min_rank(). Esta función realiza el tipo más común de ordenamiento (por ejemplo, primero, segundo, tercero, etc.). El valor predeterminado otorga la menor posición a los valores más pequeños; usa desc(x) para dar la menor posición a los valores más grandes. y &lt;- c (1, 2, 2, NA, 3, 4) min_rank(y) #&gt; [1] 1 2 2 NA 4 5 min_rank(desc(y)) #&gt; [1] 5 3 3 NA 2 1 Si min_rank() no hace lo que necesitas, consulta las variantes row_number(), dense_rank(), percent_rank(), cume_dist(),quantile(). Mira sus páginas de ayuda para más detalles. row_number(y) #&gt; [1] 1 2 3 NA 4 5 dense_rank(y) #&gt; [1] 1 2 2 NA 3 4 percent_rank(y) #&gt; [1] 0.00 0.25 0.25 NA 0.75 1.00 cume_dist(y) #&gt; [1] 0.2 0.6 0.6 NA 0.8 1.0 26.5.2 Ejercicios Las variables horario_salida y salida_programada tienen un formato conveniente para leer, pero es difícil realizar cualquier cálculo con ellas porque no son realmente números continuos. Transfórmalas hacia un formato más conveniente como número de minutos desde la medianoche. Compara tiempo_vuelo con horario_llegada - horario_salida. ¿Qué esperas ver? ¿Qué ves? ¿Qué necesitas hacer para arreglarlo? Compara horario_salida, salida_programada, y atraso_salida. ¿Cómo esperarías que esos tres números estén relacionados? Encuentra los 10 vuelos más retrasados utilizando una función de ordenamiento. ¿Cómo quieres manejar los empates? Lee atentamente la documentación de min_rank(). ¿Qué devuelve 1:3 + 1:10? ¿Por qué? ¿Qué funciones trigonométricas proporciona R? 26.6 Resúmenes agrupados con summarise() El último verbo clave es summarise() (de resumir en inglés). Se encarga de colapsar un data frame en una sola fila: summarise(vuelos, atraso = mean(atraso_salida, na.rm = TRUE)) #&gt; # A tibble: 1 x 1 #&gt; atraso #&gt; &lt;dbl&gt; #&gt; 1 12.6 (Volveremos a lo que significa na.rm = TRUE en muy poco tiempo). summarise() no es muy útil a menos que lo enlacemos con group_by(). Esto cambia la unidad de análisis del conjunto de datos completo a grupos individuales. Luego, cuando uses los verbos dplyr en un data frame agrupado, estos se aplicaran automáticamente “por grupo”. Por ejemplo, si aplicamos exactamente el mismo código a un data frame agrupado por fecha, obtenemos el retraso promedio por fecha: por_dia &lt;- group_by(vuelos, anio, mes, dia) summarise(por_dia, atraso = mean(atraso_salida, na.rm = TRUE)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia atraso #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows Juntos group_by() y summarise() proporcionan una de las herramientas que más comúnmente usarás cuando trabajes con dplyr: resúmenes agrupados. Pero antes de ir más allá con esto, tenemos que introducir una idea nueva y poderosa: el pipe (/paip/) (del inglés tubo o conducto). 26.6.1 Combinación de múltiples operaciones con el pipe Imagina que queremos explorar la relación entre la distancia y la demora promedio para cada ubicación. Usando lo que sabes acerca de dplyr, podrías escribir un código como este: por_destino &lt;- group_by(vuelos, destino) atraso &lt;- summarise(por_destino, conteo = n(), distancia = mean(distancia, na.rm = TRUE), atraso = mean(atraso_llegada, na.rm = TRUE) ) atraso &lt;- filter(atraso, conteo &gt; 20, destino != &quot;HNL&quot;) # Parece que las demoras aumentan con una distancia de hasta ~ 750 millas # y luego disminuyen. Tal vez a medida que los vuelos se hacen más # largos, hay más habilidad para compensar las demoras en el aire? ggplot(data = atraso, mapping = aes(x = distancia, y = atraso)) + geom_point(aes(size = conteo), alpha = 1/3) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Hay tres pasos para preparar esta información: Agrupa los vuelos por destino. Resume en cada grupo para calcular la distancia, la demora promedio y el número de vuelos. Filtra para eliminar puntos ruidosos y el aeropuerto de Honolulu, que está casi dos veces más lejos que el próximo aeropuerto más cercano. Nombrar cosas es difícil y enlentece nuestro análisis. Es entonces un poco frustrante escribir este código porque tenemos que dar un nombre a cada data frame intermedio, incluso si el data frame en sí mismo no nos importa. Hay otra forma de abordar el mismo problema con el pipe, %&gt;%: atrasos &lt;- vuelos %&gt;% group_by(destino) %&gt;% summarise( conteo = n(), distancia = mean(distancia, na.rm = TRUE), atraso = mean(atraso_llegada, na.rm = TRUE) ) %&gt;% filter(conteo &gt; 20, destino != &quot;HNL&quot;) Esto se enfoca en las transformaciones, no en lo que se está transformando, lo que hace que el código sea más fácil de leer. Puedes leerlo como una serie de declaraciones imperativas: agrupa, luego resume y por último filtra. Como sugiere esta lectura, una buena forma de pronunciar %&gt;% cuando se lee el código es “y luego”. Lo que encuentras detrás del código, es que x %&gt;% f(y) se convierte en f(x, y), y x %&gt;% f(y) %&gt;% g(z) se convierte en g(f(x, y), z) y así sucesivamente. Puedes usar el pipe para reescribir múltiples operaciones de forma que puedas leer de izquierda a derecha, de arriba hacia abajo. Usaremos pipes con frecuencia a partir de ahora porque mejora considerablemente la legibilidad del código, y volveremos al tema con más detalle en pipes. Trabajar con el pipe es uno de los criterios clave para pertenecer al tidyverse. La única excepción es ggplot2: se escribió antes de que se descubriera el pipe. Lamentablemente, la siguiente iteración de ggplot2, ggvis, que sí utiliza el pipe, aún no está lista para la audiencia general. 26.6.2 Valores faltantes Es posible que te hayas preguntado sobre el argumento na.rm que utilizamos anteriormente. ¿Qué pasa si no lo configuramos? vuelos %&gt;% group_by(anio, mes, dia) %&gt;% summarise(mean = mean(atraso_salida)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 NA #&gt; 2 2013 1 2 NA #&gt; 3 2013 1 3 NA #&gt; 4 2013 1 4 NA #&gt; 5 2013 1 5 NA #&gt; 6 2013 1 6 NA #&gt; # … with 359 more rows ¡Obtenemos muchos valores faltantes! Esto se debe a que las funciones de agregación obedecen la regla habitual de valores faltantes: si hay algún valor faltante en el input, el output será un valor faltante. Afortunadamente, todas las funciones de agregación tienen un argumento na.rm que elimina los valores faltantes antes del cálculo: vuelos %&gt;% group_by(anio, mes, dia) %&gt;% summarise(mean = mean(atraso_salida, na.rm = TRUE)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows En este caso, donde los valores faltantes representan vuelos cancelados, también podríamos abordar el problema eliminando primero los vuelos cancelados. Guardaremos este conjunto de datos para poder reutilizarlo en los siguientes ejemplos. no_cancelados &lt;- vuelos %&gt;% filter(!is.na(atraso_salida), !is.na(atraso_llegada)) no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise(mean = mean(atraso_salida)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.4 #&gt; 2 2013 1 2 13.7 #&gt; 3 2013 1 3 10.9 #&gt; 4 2013 1 4 8.97 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows 26.6.3 Conteos Siempre que realices una agregación, es una buena idea incluir un conteo (n()) o un recuento de valores no faltantes (sum(!is.na(x))). De esta forma, puedes verificar que no estás sacando conclusiones basadas en cantidades muy pequeñas de datos. Por ejemplo, veamos los aviones (identificados por su número de cola) que tienen las demoras promedio más altas: atrasos &lt;- no_cancelados %&gt;% group_by(codigo_cola) %&gt;% summarise( atraso = mean(atraso_llegada) ) ggplot(data = atrasos, mapping = aes(x = atraso)) + geom_freqpoly(binwidth = 10) ¡Hay algunos aviones que tienen una demora promedio de 5 horas (300 minutos)! La historia es en realidad un poco más matizada. Podemos obtener más información si hacemos un diagrama de dispersión del número de vuelos contra la demora promedio: atrasos &lt;- no_cancelados %&gt;% group_by(codigo_cola) %&gt;% summarise( atraso = mean(atraso_llegada, na.rm = TRUE), n = n() ) ggplot(data = atrasos, mapping = aes(x = n, y = atraso)) + geom_point(alpha = 1/10) No es sorprendente que haya una mayor variación en el promedio de retraso cuando hay pocos vuelos. La forma de este gráfico es muy característica: cuando trazas un promedio (o cualquier otra medida de resumen) contra el tamaño del grupo, verás que la variación decrece a medida que el tamaño de muestra aumenta. Cuando se observa este tipo de gráficos, resulta útil eliminar los grupos con menor número de observaciones, ya que puedes ver más del patrón y menos de la variación extrema de los grupos pequeños. Esto es lo que te muestra cómo hacer el código siguiente, así como también te ofrece una manera muy útil para integrar ggplot2 en el flujo de trabajo de dplyr. Es un poco incómodo tener que cambiar de %&gt;% a +, pero una vez que entiendas el código, verás que es bastante conveniente. atrasos %&gt;% filter(n &gt; 25) %&gt;% ggplot(mapping = aes(x = n, y = atraso)) + geom_point(alpha = 1/10) RStudio tip: un atajo en tu teclado que puede ser muy útil es Cmd/Ctrl + Shift + P. Este reenvía el fragmento enviado previamente del editor a la consola. Esto es muy útil cuando por ejemplo estás explorando el valor de n en el ejemplo anterior. Envías todo el bloque a la consola una vez con Cmd / Ctrl + Enter, y luego modificas el valor de n y presionas Cmd / Ctrl + Shift + P para reenviar el bloque completo. Hay otra variación común de este tipo de patrón. Veamos cómo el rendimiento promedio de los bateadores en el béisbol está relacionado con el número de veces que están bateando. Aquí utilizas el conjunto de datos de bateadores para calcular el promedio de bateo (número de bateos / número de intentos) de cada jugador de béisbol de Grandes Ligas. Cuando graficas la habilidad del bateador (medido por el promedio de bateo, ba) contra el número de oportunidades para golpear la pelota (medido por al bate,ab), verás dos patrones: Como en el ejemplo anterior, la variación en nuestro estadístico de resumen disminuye a medida que obtenemos más observaciones. Existe una correlación positiva entre la habilidad (ba) y las oportunidades para golpear la pelota (ab). Esto se debe a que los equipos controlan quién puede jugar, y obviamente elegirán a sus mejores jugadores. # Convierte a tibble para puedas imprimirlo de una manera legible bateo &lt;- as_tibble(bateadores) rendimiento_bateadores &lt;- bateo %&gt;% group_by(ID_jugador) %&gt;% summarise( ba = sum(golpes, na.rm = TRUE) / sum(al_bate, na.rm = TRUE), ab = sum(al_bate, na.rm = TRUE) ) rendimiento_bateadores %&gt;% filter(ab &gt; 100) %&gt;% ggplot(mapping = aes(x = ab, y = ba)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Esto también tiene implicaciones importantes para la clasificación. Si ingenuamente ordenas desc(ba), verás que las personas con los mejores promedios de bateo tienen claramente mucha suerte, pero no son necesariamente habilidosos: rendimiento_bateadores %&gt;% arrange(desc(ba)) #&gt; # A tibble: 19,428 x 3 #&gt; ID_jugador ba ab #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 abramge01 1 1 #&gt; 2 alberan01 1 1 #&gt; 3 allarko01 1 1 #&gt; 4 banisje01 1 1 #&gt; 5 bartocl01 1 1 #&gt; 6 bassdo01 1 1 #&gt; # … with 1.942e+04 more rows Puedes encontrar una buena explicación de este problema en http://varianceexplained.org/r/empirical_bayes_baseball/ y http://www.evanmiller.org/how-not-to-sort-by-average-rating.html. 26.6.4 Funciones de resumen útiles Solo el uso de medias, conteos y sumas puede llevarte muy lejos, pero R proporciona muchas otras funciones de resumen útiles: Medidas de centralidad: hemos usado mean(x), pero median(x) también resulta muy útil. La media es la suma dividida por el número de observaciones; la mediana es un valor donde el 50% de x está por encima de él, y el 50% está por debajo. A veces es útil combinar agregación con un subconjunto lógico. Todavía no hemos hablado sobre este tipo de subconjuntos, pero aprenderás más al respecto en [subsetting]. no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise( prom_atraso1 = mean(atraso_llegada), prom_atraso2 = mean(atraso_llegada[atraso_llegada &gt; 0]) # el promedio de atrasos positivos ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia prom_atraso1 prom_atraso2 #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 12.7 32.5 #&gt; 2 2013 1 2 12.7 32.0 #&gt; 3 2013 1 3 5.73 27.7 #&gt; 4 2013 1 4 -1.93 28.3 #&gt; 5 2013 1 5 -1.53 22.6 #&gt; 6 2013 1 6 4.24 24.4 #&gt; # … with 359 more rows Medidas de dispersión: sd(x), IQR(x), mad(x). La raíz de la desviación media al cuadrado o desviación estándar o SD para abreviar, es una medida estándar de dispersión. El rango intercuartil IQR() y la desviación media absoluta mad(x) son medidas robustas equivalentes que pueden ser más útiles si tienes valores atípicos. # ¿Por qué la distancia a algunos destinos es más variable que la de otros? no_cancelados %&gt;% group_by(destino) %&gt;% summarise(distancia_sd = sd(distancia)) %&gt;% arrange(desc(distancia_sd)) #&gt; # A tibble: 104 x 2 #&gt; destino distancia_sd #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 EGE 10.5 #&gt; 2 SAN 10.4 #&gt; 3 SFO 10.2 #&gt; 4 HNL 10.0 #&gt; 5 SEA 9.98 #&gt; 6 LAS 9.91 #&gt; # … with 98 more rows Medidas de rango: min(x), quantile(x, 0.25), max(x). Los cuantiles son una generalización de la mediana. Por ejemplo, quantile(x, 0.25) encontrará un valor de x que sea mayor a 25% de los valores, y menor que el 75% restante. # ¿Cuándo salen los primeros y los últimos vuelos cada día? no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise( primero = min(horario_salida), ultimo = max(horario_salida) ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia primero ultimo #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 2356 #&gt; 2 2013 1 2 42 2354 #&gt; 3 2013 1 3 32 2349 #&gt; 4 2013 1 4 25 2358 #&gt; 5 2013 1 5 14 2357 #&gt; 6 2013 1 6 16 2355 #&gt; # … with 359 more rows Medidas de posición: first(x), nth(x, 2), last(x). Estas trabajan de forma similar a x[1], x[2] y x[length (x)], pero te permiten establecer un valor predeterminado en el caso de que esa posición no exista (es decir, si estás tratando de obtener el tercer elemento de un grupo que solo tiene dos elementos). Por ejemplo, podemos encontrar la primera y última salida para cada día: no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise( primera_salida = first(horario_salida), ultima_salida = last(horario_salida) ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia primera_salida ultima_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 2356 #&gt; 2 2013 1 2 42 2354 #&gt; 3 2013 1 3 32 2349 #&gt; 4 2013 1 4 25 2358 #&gt; 5 2013 1 5 14 2357 #&gt; 6 2013 1 6 16 2355 #&gt; # … with 359 more rows Estas funciones son complementarias al filtrado en rangos. El filtrado te proporciona todas las variables, con cada observación en una fila separada: no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% mutate(r = min_rank(desc(horario_salida))) %&gt;% filter(r %in% range(r)) #&gt; # A tibble: 770 x 20 #&gt; # Groups: anio, mes, dia [365] #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 2356 2359 -3 #&gt; 3 2013 1 2 42 2359 43 #&gt; 4 2013 1 2 2354 2359 -5 #&gt; 5 2013 1 3 32 2359 33 #&gt; 6 2013 1 3 2349 2359 -10 #&gt; # … with 764 more rows, and 14 more variables: horario_llegada &lt;int&gt;, #&gt; # llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, aerolinea &lt;chr&gt;, #&gt; # vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, destino &lt;chr&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, minuto &lt;dbl&gt;, #&gt; # fecha_hora &lt;dttm&gt;, r &lt;int&gt; Conteos: has visto n(), que no toma argumentos, y devuelve el tamaño del grupo actual. Para contar la cantidad de valores no faltantes, usa sum(!is.na (x)). Para contar la cantidad de valores distintos (únicos), usa n_distinct(x). # ¿Qué destinos tienen la mayoría de las aerolíneas? no_cancelados %&gt;% group_by(destino) %&gt;% summarise(aerolineas = n_distinct(aerolinea)) %&gt;% arrange(desc(aerolineas)) #&gt; # A tibble: 104 x 2 #&gt; destino aerolineas #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 7 #&gt; 2 BOS 7 #&gt; 3 CLT 7 #&gt; 4 ORD 7 #&gt; 5 TPA 7 #&gt; 6 AUS 6 #&gt; # … with 98 more rows Los conteos son tan útiles que dplyr proporciona un ayudante simple si todo lo que quieres es un conteo: no_cancelados %&gt;% count(destino) #&gt; # A tibble: 104 x 2 #&gt; destino n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ABQ 254 #&gt; 2 ACK 264 #&gt; 3 ALB 418 #&gt; 4 ANC 8 #&gt; 5 ATL 16837 #&gt; 6 AUS 2411 #&gt; # … with 98 more rows Opcionalmente puedes proporcionar una variable de ponderación. Por ejemplo, podrías usar esto para “contar” (sumar) el número total de millas que un avión voló: no_cancelados %&gt;% count(codigo_cola, wt = distancia) #&gt; # A tibble: 4,037 x 2 #&gt; codigo_cola n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # … with 4,031 more rows Conteos y proporciones de valores lógicos: sum(x &gt; 10), mean(y == 0). Cuando se usan con funciones numéricas, TRUE se convierte en 1 y FALSE en 0. Esto hace que sum() y mean() sean muy útiles: sum(x) te da la cantidad de TRUEs en x, y mean(x) te da la proporción. # ¿Cuántos vuelos salieron antes de las 5 a.m.? # (estos generalmente son vuelos demorados del día anterior) no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise(n_temprano = sum(horario_salida &lt; 500)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia n_temprano #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 0 #&gt; 2 2013 1 2 3 #&gt; 3 2013 1 3 4 #&gt; 4 2013 1 4 3 #&gt; 5 2013 1 5 3 #&gt; 6 2013 1 6 2 #&gt; # … with 359 more rows # ¿Qué proporción de vuelos se retrasan más de una hora? no_cancelados %&gt;% group_by(anio, mes, dia) %&gt;% summarise(hora_perc = mean(atraso_llegada &gt; 60)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia hora_perc #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 0.0722 #&gt; 2 2013 1 2 0.0851 #&gt; 3 2013 1 3 0.0567 #&gt; 4 2013 1 4 0.0396 #&gt; 5 2013 1 5 0.0349 #&gt; 6 2013 1 6 0.0470 #&gt; # … with 359 more rows 26.6.5 Agrupación por múltiples variables Cuando agrupas por múltiples variables, cada resumen se desprende de un nivel de la agrupación. Eso hace que sea más fácil acumular progresivamente en un conjunto de datos: diario &lt;- group_by(vuelos, anio, mes, dia) (por_dia &lt;- summarise(diario, vuelos = n())) #&gt; # A tibble: 365 x 4 #&gt; # Groups: anio, mes [12] #&gt; anio mes dia vuelos #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 842 #&gt; 2 2013 1 2 943 #&gt; 3 2013 1 3 914 #&gt; 4 2013 1 4 915 #&gt; 5 2013 1 5 720 #&gt; 6 2013 1 6 832 #&gt; # … with 359 more rows (por_mes &lt;- summarise(por_dia, vuelos = sum(vuelos))) #&gt; # A tibble: 12 x 3 #&gt; # Groups: anio [1] #&gt; anio mes vuelos #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 27004 #&gt; 2 2013 2 24951 #&gt; 3 2013 3 28834 #&gt; 4 2013 4 28330 #&gt; 5 2013 5 28796 #&gt; 6 2013 6 28243 #&gt; # … with 6 more rows (por_anio &lt;- summarise(por_mes, vuelos = sum(vuelos))) #&gt; # A tibble: 1 x 2 #&gt; anio vuelos #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 336776 Ten cuidado al acumular resúmenes progresivamente: está bien para las sumas y los recuentos, pero debes pensar en la ponderación de las medias y las varianzas, además de que no es posible hacerlo exactamente para estadísticas basadas en rangos como la mediana. En otras palabras, la suma de las sumas agrupadas es la suma total, pero la mediana de las medianas agrupadas no es la mediana general. 26.6.6 Desagrupar Si necesitas eliminar la agrupación y regresar a las operaciones en datos desagrupados, usa ungroup(). diario %&gt;% ungroup() %&gt;% # ya no está agrupado por fecha summarise(vuelos = n()) # todos los vuelos #&gt; # A tibble: 1 x 1 #&gt; vuelos #&gt; &lt;int&gt; #&gt; 1 336776 26.6.7 Ejercicios Haz una lluvia de ideas de al menos 5 formas diferentes de evaluar las características de un retraso típico de un grupo de vuelos. Considera los siguientes escenarios: * Un vuelo llega 15 minutos antes 50% del tiempo, y 15 minutos tarde 50% del tiempo. * Un vuelo llega siempre 10 minutos tarde. * Un vuelo llega 30 minutos antes 50% del tiempo, y 30 minutos tarde 50% del tiempo. * Un vuelo llega a tiempo en el 99% de los casos. 1% de las veces llega 2 horas tarde. ¿Qué es más importante: retraso de la llegada o demora de salida? Sugiere un nuevo enfoque que te de el mismo output que no_cancelados %&gt;% count(destino) y no_cancelado %&gt;% count(codigo_cola, wt = distancia) (sin usar count()). Nuestra definición de vuelos cancelados (is.na(atraso_salida) | is.na (atraso_llegada)) es un poco subóptima. ¿Por qué? ¿Cuál es la columna más importante? Mira la cantidad de vuelos cancelados por día. ¿Hay un patrón? ¿La proporción de vuelos cancelados está relacionada con el retraso promedio? ¿Qué compañía tiene los peores retrasos? Desafío: ¿puedes desenredar el efecto de malos aeropuertos vs. el efecto de malas aerolíneas? ¿Por qué o por qué no? (Sugerencia: piensa en vuelos %&gt;% group_by(aerolinea, destino) %&gt;% summarise(n())) ¿Qué hace el argumento sort a count(). ¿Cuándo podrías usarlo? 26.7 Transformaciones agrupadas (y filtros) La agrupación es más útil si se utiliza junto con summarise(), pero también puedes hacer operaciones convenientes con mutate() y filter(): Encuentra los peores miembros de cada grupo: vuelos_sml %&gt;% group_by(anio, mes, dia) %&gt;% filter(rank(desc(atraso_llegada)) &lt; 10) #&gt; # A tibble: 3,306 x 7 #&gt; # Groups: anio, mes, dia [365] #&gt; anio mes dia atraso_salida atraso_llegada distancia tiempo_vuelo #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 853 851 184 41 #&gt; 2 2013 1 1 290 338 1134 213 #&gt; 3 2013 1 1 260 263 266 46 #&gt; 4 2013 1 1 157 174 213 60 #&gt; 5 2013 1 1 216 222 708 121 #&gt; 6 2013 1 1 255 250 589 115 #&gt; # … with 3,300 more rows Encuentra todos los grupos más grandes que un umbral: popular_destinos &lt;- vuelos %&gt;% group_by(destino) %&gt;% filter(n() &gt; 365) popular_destinos #&gt; # A tibble: 332,577 x 19 #&gt; # Groups: destino [77] #&gt; anio mes dia horario_salida salida_programa… atraso_salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 515 2 #&gt; 2 2013 1 1 533 529 4 #&gt; 3 2013 1 1 542 540 2 #&gt; 4 2013 1 1 544 545 -1 #&gt; 5 2013 1 1 554 600 -6 #&gt; 6 2013 1 1 554 558 -4 #&gt; # … with 3.326e+05 more rows, and 13 more variables: #&gt; # horario_llegada &lt;int&gt;, llegada_programada &lt;int&gt;, atraso_llegada &lt;dbl&gt;, #&gt; # aerolinea &lt;chr&gt;, vuelo &lt;int&gt;, codigo_cola &lt;chr&gt;, origen &lt;chr&gt;, #&gt; # destino &lt;chr&gt;, tiempo_vuelo &lt;dbl&gt;, distancia &lt;dbl&gt;, hora &lt;dbl&gt;, #&gt; # minuto &lt;dbl&gt;, fecha_hora &lt;dttm&gt; Estandariza para calcular las métricas por grupo: popular_destinos %&gt;% filter(atraso_llegada &gt; 0) %&gt;% mutate(prop_atraso = atraso_llegada / sum(atraso_llegada)) %&gt;% select(anio:dia, destino, atraso_llegada, prop_atraso) #&gt; # A tibble: 131,106 x 6 #&gt; # Groups: destino [77] #&gt; anio mes dia destino atraso_llegada prop_atraso #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 IAH 11 0.000111 #&gt; 2 2013 1 1 IAH 20 0.000201 #&gt; 3 2013 1 1 MIA 33 0.000235 #&gt; 4 2013 1 1 ORD 12 0.0000424 #&gt; 5 2013 1 1 FLL 19 0.0000938 #&gt; 6 2013 1 1 ORD 8 0.0000283 #&gt; # … with 1.311e+05 more rows Un filtro agrupado es una transformación agrupada seguida de un filtro desagrupado. En general, los evito, excepto para las manipulaciones rápidas y sucias: de lo contrario, es difícil comprobar que has hecho la manipulación correctamente. Las funciones que trabajan de forma más natural en transformaciones agrupadas y filtros se conocen como funciones de ventana (frente a las funciones de resumen utilizadas para los resúmenes). Puedes obtener más información sobre las funciones de ventana útiles en la viñeta correspondiente: vignette(&quot;window-functions&quot;). 26.7.1 Ejercicios Remítete a las listas de funciones útiles de mutación y filtrado. Describe cómo cambia cada operación cuando las combinas con la agrupación. ¿Qué avión (codigo_cola) tiene el peor registro de tiempo? ¿A qué hora del día deberías volar si quieres evitar los retrasos lo más posible? Para cada destino, calcula los minutos totales de demora. Para cada vuelo, calcula la proporción de la demora total para su destino. Los retrasos suelen estar temporalmente correlacionados: incluso una vez que el problema que causó el retraso inicial se ha resuelto, los vuelos posteriores se retrasan para permitir que salgan los vuelos anteriores. Usando lag(), explora cómo el retraso de un vuelo está relacionado con el retraso del vuelo inmediatamente anterior. Mira cada destino. ¿Puedes encontrar vuelos sospechosamente rápidos? (es decir, vuelos que representan un posible error de entrada de datos). Calcula el tiempo en el aire de un vuelo relativo al vuelo más corto a ese destino. ¿Cuáles vuelos se retrasaron más en el aire? Encuentra todos los destinos que son volados por al menos dos operadores. Usa esta información para clasificar a las aerolíneas. Para cada avión, cuenta el número de vuelos antes del primer retraso de más de 1 hora. "],
["vectores.html", "27 Vectores 27.1 Introducción 27.2 Pre-requisitos 27.3 Vectores básicos 27.4 Tipos importantes de vectores atómicos 27.5 Usando vectores atómicos 27.6 Vectores Recursivos (listas) 27.7 Visualizando listas 27.8 Listas de Condimentos 27.9 Atributos 27.10 Vectores Aumentados", " 27 Vectores 27.1 Introducción Hasta ahora este libro se ha enfocado en tibbles y sus paquetes correspondientes. Pero como empezaste a escribir tus propias funciones, y a profundizar en R, es que necesitas aprender sobre vectores, es decir, sobre los objetos que soportan los tibbles. Por esto, es mejor empezar con tibbles ya que inmediatamente puedes ver su utilidad, y luego trabajar a tu manera con los componentes que están debajo, los vectores. Los vectores son particularmente importantes, al igual que la mayoría de las funciones que escribirás y utilizarás con dichos vectores. Es posible desarrollar funciones que trabajen con tibbles (como ggplot2, dplyr and tidyr) pero las herramientas que necesitas para ello son peculiares e inmaduras. Por esto, estoy desarrollando un mejor enfoque, el cual puedes consultar en https://github.com/hadley/lazyeval, pero este no estará listo a tiempo para la publicación del libro. Incluso aún cuando esté completo, de todas maneras necesitarás entender el concepto de vectores, esto solo facilitará la escritura de capas finales que sean user-friendly (amigables al usuario). 27.2 Pre-requisitos Este capítulo se enfoca en las estructuras de datos de R base, por lo que no es esencial cargar ningún paquete. Sin embargo, usaremos un conjunto de funciones del paquete purrr para evitar algunas insonsistencias en R Base. library(tidyverse) 27.3 Vectores básicos There are two types of vectors: _Hay dos tipos de vectores: Vectores atómicos, de los cuales existen seis tipos: lógico o booleano, entero, doble o real, caracter, complejo y raw (que consisten en datos sin procesar). Los vectores de tipo entero y doble son ampliamente conocidos como vectores númericos. Las listas, las cuales son denominadas en ciertas ocasiones como vectores recursivos debido a que pueden contener otras listas.. La diferencia principal entre vectores atómicos y listas es que los vectores atomicos son homogéneos, mientras las listas pueden ser heterogéneas. Existe, otro objeto relacionado: Null (nulo). El nulo es a menudo usado para representar la ausencia de un vector (como el opuesto a NA el cual es usado para representar la ausencia de un valor en un vector.) Null típicamente se comporta como un vector de longitud cero 0. Figura @ref (fig:datatypes) resume las interrelaciones. Figure 27.1: The hierarchy of R’s vector types Cada vector tiene dos propiedades claves: Su tipo (type), el cual puedes determinarlo con la sentencia typeof() (del inglés tipode). typeof(letters) #&gt; [1] &quot;character&quot; typeof(1:10) #&gt; [1] &quot;integer&quot; Su longitud (length), la cual puedes determinarla con la sentencia length() (del ingés longitud). x &lt;- list(&quot;a&quot;, &quot;b&quot;, 1:10) length(x) #&gt; [1] 3 Los vectores pueden contener también arbitrariamente metadata adicional en forma de atributos. Estos atributos son usados para crear vectores aumentados los cuales implican un comportamiento distinto. Existen tres tipos de vectores aumentados:  Los factores (factors) construidos a partir de vectores de enteros.  Las fechas y fechas-tiempo (date-time) construidos a partir de vectores numéricos.  Los Dataframes y tibbles construidos a partir de listas. Este capítulo te introducirá a lo temas más importantes de vectores, desde lo más simple a lo más complicado. Comenzarás con vectores atómicos, luego seguirás con listas, y finalizarás con vectores aumentados. 27.4 Tipos importantes de vectores atómicos Los cuatro tipos más importantes de vectores atómicos son lógico, entero, real y carácter. Los tipos raw y complejo son raramente usados durante el análisis de datos, por lo tanto no discutiremos sobre ellos aquí. 27.4.1 Lógico o Booleano Los vectores de tipo lógico o booleano son el tipo más sencillo de vectores atómicos porque ellos solo pueden tomar tres valores posibles: falso, verdadero y Na. Los vectores lógicos son construidos usualmente con operadores de comparación, como se describe en comparaciones. También puedes crearlos manualmente con la función c (): 1:10 %% 3 == 0 #&gt; [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE c(TRUE, TRUE, FALSE, NA) #&gt; [1] TRUE TRUE FALSE NA 27.4.2 Numérico Los vectores compuestos por enteros o reales son conocidos ampliamente como vectores numéricos. En R, los números son por defecto, reales. Por lo que, para generar un entero, debes colocar una L después del número: typeof(1) #&gt; [1] &quot;double&quot; typeof(1L) #&gt; [1] &quot;integer&quot; 1.5L #&gt; [1] 1.5 La distinción entre enteros y reales no es realmente importante aunque existen dos diferencias relevantes de las que debes ser consciente: 1. Los números dobles o reales son aproximaciones. Los mismos representan números de punto flotante que no pueden ser precisamente representados con un monto fijo de memoria. Esto significa que debes considerar que todos los reales sean aproximaciones. Por ejemplo, ¿cuál es el cuadrado de la raíz cuadrada de dos? x &lt;- sqrt(2) ^ 2 x #&gt; [1] 2 x - 2 #&gt; [1] 4.44e-16 Este compartamiento es común cuando trabajas con números de punto flotante: la mayoría de los cálculos incluyen algunos errores de aproximación. En lugar de comparar números de punto flotante usando ==, debes usar ´dplyr::near()`, el cual provee tolerancia numérica. Los números reales tienen cuatro tipos de valores posibles: NA, NaN, Inf and –Inf. Estos tres valores especiales NaN, Inf and -Inf pueden surgir a partir de la división de c(-1, 0, 1) / 0 Evita usar == para chequear estos valores especiales. En su lugar usa la funciones de ayuda is.finite(), is.infinite(), y is.nan(): 0 Inf NA NaN is.finite() x is.infinite() x is.na() x x is.nan() x 27.4.3 Caracter Los vectores compuestos por carácteres son los tipos más complejos de vectores atómicos, porque cada elemento del mismo es un string, y un string puede contener una cantidad arbitraria de datos. Ya has aprendido un montón acerca de cómo trabajar con strings en strings. En este punto quiero mencionar una característica importante y fundamental en la implementación de un string: R usa una reserva global de strings. Esto significa que cada string solo es almacenado en memoria una vez, y en cada uso de puntos del string a la representación. Esto reduce la cantidad de memoria necesaria por strings duplicados. Puedes ver este comportamiento en práctica con pryr::object_size(): x &lt;- &quot;Esto es un string razonablemente largo.&quot; pryr::object_size(x) #&gt; Registered S3 method overwritten by &#39;pryr&#39;: #&gt; method from #&gt; print.bytes Rcpp #&gt; 152 B y &lt;- rep(x, 1000) pryr::object_size(y) #&gt; 8.14 kB y no utiliza más de 1000x ni tanta memoria como x, porque cada elemento de y es sólo un puntero al mismo string. Un puntero utiliza 8 bytes, entonces 1000 punteros a 136 B string es igual a 8 * 1000 + 136 = 8.13 kB. 27.4.4 Valores perdidos (Missing values) Nota que cada tipo de vector atómico tiene su propio valor perdido (o missing value): NA # logico #&gt; [1] NA NA_integer_ # entero #&gt; [1] NA NA_real_ # real #&gt; [1] NA NA_character_ # caracter #&gt; [1] NA Normalmente no necesitas saber sobre los diferentes tipos porque puedes siempre usar el valor NA (not Available), es decir el valor faltante, y se convertirá al tipo correcto usando las reglas de la coerción implícitas. Sin embargo, existen algunas funciones que son estrictas acerca de sus inputs, por lo tanto es útil tener presente este conocimiento así puedes ser especifico cuando lo necesites. ###Ejercicios Describe la diferencia entre is.finite(x) y !is.infinite(x). Lee el código fuente de dplyr:: near() (Consejo: para ver el código fuente, escribe lo siguiente ()) ¿Funcionó? Un vector de tipo lógico puede tomar 3 valores posibles. ¿Cuántos valores posibles puede tomar un vector de tipo entero? ¿Cuántos valores posibles puede tomar un vector de tipo real? Usa google para realizar buscar información respecto a lo planteado anteriormente. Idea al menos 4 funciones que te permitan convertir un vector del tipo real a entero. ¿En qué difieren las funciones? Sé preciso. ¿Cuáles funciones del paquete readr te permiten convertir un vector del tipo string en un vector del tipo lógico, entero y doble? 27.5 Usando vectores atómicos Ahora que conoces los diferentes tipos de vectores atómicos, es útil repasar algunas herramientas importantes para así poder utilizarlas. Esto incluye: 1. Cómo realizar una conversión de un determinado tipo a otro, y en cuáles casos esto sucede automáticamente. 2. Cómo decidir si un objeto es un tipo específico de un vector. 3. Qué sucede cuando trabajas con vectores de diferentes longitudes. 4. Cómo nombrar los elementos de un vector 5. Cómo obtener los elementos de interés de un vector. 27.5.1 Coerción Existen dos maneras de convertir, o coercer, un tipo de vector a otro: 1. La Coerción explicita sucede cuando defines a una función como as.logical(), as.integer(), as.double(), o as.character(). Cuando te encuentres usando coerción explicita, siempre debes comprobar que sea posible realizar la corrección en sentido ascendente, de esta manera, en primer lugar, estamos seguros que ese vector nunca tuvo tipos incorrectos. Por ejemplo, quizás necesites la especificación de col_types (‘tipos de columna’) del paquete readr. La Coerción implícita sucede cuando usas un vector en un contexto especifico del cual se espera un cierto tipo de vector. Por ejemplo, cuando usas un vector del tipo lógico con la función numérica ‘summary’ (del inglés resumen), o cuando usas un vector del tipo doble donde se espera un vector del tipo entero. Porque la coerción explicita es usada raramente, y es ampliamente fácil de entender, enfocaré sobre la coerción implicita aquí. Anteriormente vimos el tipo más importante de coerción implicita: usando un vector de tipo lógico en un contexto numérico. En ese caso, el valor TRUE (‘VERDADERO’) es convertido a 1 y ‘FALSE’ (‘FALSO’) convertido a 0. Esto significa que la suma de un vector de tipo lógico es el número de los valores verdaderos, y el significado de un vector de tipo lógico es la proporción de valores verdaderos: x &lt;- sample(20, 100, replace = TRUE) y &lt;- x &gt; 10 sum(y) # ¿cuántos valores son más grandes que 10? #&gt; [1] 38 mean(y) # ¿cuál es la porporción de valores que son mayores que 10? #&gt; [1] 0.38 Quizás veas algún código (tipicamente más antiguo) basado en la coerción implicita pero en la dirección opuesta, es decir, de un valor entero a uno lógico if (length(x)) { # do something } En este caso, 0 es convertido a FALSO y todo lo demás es convertido a VERDADERO. Pienso que esto hace más dificil entender el código, por lo que no lo recomiendo. En su lugar, de ser explicito, sugiero utilizar: length(x) &gt; 0. Es también importante entender que pasa cuando creas un vector que contiene múltiples tipos con c(): los tipos más complejos siempre ganan. typeof(c(TRUE, 1L)) #&gt; [1] &quot;integer&quot; typeof(c(1L, 1.5)) #&gt; [1] &quot;double&quot; typeof(c(1.5, &quot;a&quot;)) #&gt; [1] &quot;character&quot; Un vector atómico no puede contener un mix de diferentes tipos porque el tipo es una propiedad de un vector completo, no de elementos individuales. Si necesitas un mix de múltiples tipos en el mismo vector, entonces debes usar una lista, la cual aprenderás en breve. 27.5.2 Funciones de test Algunas veces quieres hacer las cosas de una manera diferente basadas en el tipo de vector. Una de las opciones es el uso de la sentencia typeof(). Otra es usar una función test la cual devuelva un valor TRUE o ‘FALSO’ . R base provee varias funciones como is.vector() y is.atomic(), pero estas a menudo devuelven resultados inesperados. En su lugar, es más acertado usar las funciones is_* provistas por el paquete purrr, las cual están resumidas en la tabla que se muestra a continuación. lgl int dbl chr list is_logical() x is_integer() x is_double() x is_numeric() x x is_character() x is_atomic() x x x x is_list() x is_vector() x x x x x Cada predicado además viene con una version para “escalares”, donde la función is_scalar_atomic(), chequea que la longitud sea 1. Esto es útil, por ejemplo, si quieres chequear en algún argumento que tu función sea un solo valor lógico. 27.5.3 Escalares y reglas de reciclado Así como implicitamente se coercionan los tipos de vectores que son compatibles, R también implicitamente coerciona la longitud de los vectores. Esto se denomina vector recycling, o reciclado de vectores, debido a que el vector de menor longitud se repite, o recicla, hasta igualar la longitud del vector de mayor longitud. Esto es generalmente lo más útil cuando estás trabajando con vectores y “escalares”. Los escalares están puestos en notas porque R en realidad no tiene definido los escalares: en su lugar, un solo número conforma un vector de longitud 1. Debido a que no existen los escalares, la mayoría de las funciones están construidas como vectorizadas, esto significa que operan sobre un vector del tipo númerico. Esto es así porque, por ejemplo, este código funciona: sample(10) + 100 # (del inglés muestreo) #&gt; [1] 107 104 103 109 102 101 106 110 105 108 runif(10) &gt; 0.5 #&gt; [1] FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE En R, las operaciones matemáticas básicas funcionan con vectores. Lo que significa que no necesitarás la ejecución de una interación explicita cuando realices cálculos matemáticos sencillos. Es intuitivo lo que debería pasar si agregas dos vectores de la misma longitud, o un vector y un “escalar”, pero ¿qué sucede si agregas dos vectores de diferentes longitudes? 1:10 + 1:2 #&gt; [1] 2 4 4 6 6 8 8 10 10 12 Aquí, R expandirá el vector de menor longitud a la misma longitud del vector de mayor longitud, a esto es lo que denominamos reciclaje o reutilización de un vector. Esto es una excepción cuando la longitud del vector de mayor longitud no es un múltiplo entero de la longitud del vector más corto: 1:10 + 1:3 #&gt; Warning in 1:10 + 1:3: longer object length is not a multiple of shorter #&gt; object length #&gt; [1] 2 4 6 5 7 9 8 10 12 11 Mientras el vector reciclado puede ser usado para crear código claro y conciso, también puede ocultar problemas de manera silenciosa. Por esta razón, las funciones vectorizadas en tidyverse mostrarán errores cuando recicles cualquier otra cosa que no sea un escalar. Si quieres reutilzar, necesitarás hacerlo tu mismo con la sentencia rep(): tibble(x = 1:4, y = 1:2) #&gt; Tibble columns must have consistent lengths, only values of length one are recycled: #&gt; * Length 2: Column `y` #&gt; * Length 4: Column `x` tibble(x = 1:4, y = rep(1:2, 2)) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 2 tibble(x = 1:4, y = rep(1:2, each = 2)) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 3 2 #&gt; 4 4 2 27.5.4 Nombrando vectores Todos los tipos de vectores pueden ser nombrados. Puedes asignarles un nombre al momento de crearlos con c(): c(x = 1, y = 2, z = 4) #&gt; x y z #&gt; 1 2 4 O después de la creación con purrr::set_names(): set_names(1:3, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) #&gt; a b c #&gt; 1 2 3 Los vectores con nombres son más útiles para subconjuntos, como se describe a continuación. 27.5.5 Subsetting (Subdivisión o creación de subconjuntos) {#vector-subsetting, subdivisión de vectores} Hasta ahora hemos usado dplyr::filter() para filtrar filas en una TIBBLE. La sentencia filter() sólo funciona con TIBBLES, por lo que necesitaremos una nueva herramienta para trabajar con vectores: [. [ representa a la función Subdivisión (Subsetting), la cual nos permite crear subconjuntos o subdivisiones a partir de vectores, y se indica como x[a]. Existen cuatro tipos de cosas en las que puedes subdividir un vector: Un vector numérico contiene sólo enteros. Los enteros deben ser todos positivos, todos negativos, o cero. La Subdivisión con enteros positivos mantiene los elementos en aquellas posiciones: x &lt;- c(&quot;uno&quot;, &quot;dos&quot;, &quot;tres&quot;, &quot;cuatro&quot;, &quot;cinco&quot;) x[c(3, 2, 5)] #&gt; [1] &quot;tres&quot; &quot;dos&quot; &quot;cinco&quot; x[c(3, 2, 5)] #&gt; [1] &quot;tres&quot; &quot;dos&quot; &quot;cinco&quot; Repitiendo una posición, puedes en realidad generar un output de mayor longitud que el input:: x[c(1, 1, 5, 5, 5, 2)] #&gt; [1] &quot;uno&quot; &quot;uno&quot; &quot;cinco&quot; &quot;cinco&quot; &quot;cinco&quot; &quot;dos&quot; Los valores negativos eliminan elementos en posiciones especificas: x[c(-1, -3, -5)] #&gt; [1] &quot;dos&quot; &quot;cuatro&quot; Es un error mezclar valores positivos y negativos: x[c(1, -1)] #&gt; Error in x[c(1, -1)]: only 0&#39;s may be mixed with negative subscripts El mensaje menciona subdivisiones utilizando cero, lo cual no returna valores. x[0] #&gt; character(0) Esto a menudo no es útil, pero puede ser de ayuda si quieres crear estructuras de datos inusuales para testear tus funciones. La subdivisión de un vector lógico mantiene/almacena todos los valores correspondientes al valor TRUE VERDADERO. Esto es a menudo mayormente útil en conjunto con las funciones de comparación. x &lt;- c(10, 3, NA, 5, 8, 1, NA) # Todos los valores non-missing, es decir, distintos de NA de x x[!is.na(x)] #&gt; [1] 10 3 5 8 1 # Todos, incluso los valores (missing) de x x[x %% 2 == 0] #&gt; [1] 10 NA 8 NA Si tienes un vector con nombre, puedes subdivirlo en un vector de tipo caracter. x &lt;- c(abc = 1, def = 2, xyz = 5) x[c(&quot;xyz&quot;, &quot;def&quot;)] #&gt; xyz def #&gt; 5 2 Como con los enteros positivos, también puedes usar un vector del tipo caracter para duplicar entradas individuales. El tipo más sencillo de subsetting es el valor vacío, x[], el cual retorna el valor completo de x. Esto no es útil para vectores subdivididos, aunque si lo es para matrices subdivididas(y otras estructuras de grandes dimensiones) ya que te permite seleccionar toda las filas o todas las columnas, dejando el indice en blanco. Por ejemplo, si x está en la segunda posición, x[1, ] selecciona la primera fila y todas las columnas, y la expresión x[, -1] selecciona todas las filas y todas las columnas excepto la primera. Para aprender más acerca de las aplicaciones de subsetting, puedes leer el capítulo de Subsetting de R Avanzado: http://adv-r.had.co.nz/Subsetting.html#applications. Existe una importante variación de [, la cual consiste en [[. Esta expresión [[ sólo extrae un único elemento, y siempre omite nombres. Es una buena idea usarla siempre que quieras dejar en claro que estás extrayendo un único item, como en un bucle for. La diferencia entre [ y[[ es más importante para las listas (lists), como veremos en breve. 27.5.6 Ejercicios La expresión mean(is.na(x)), ¿qué dice acerca del vector ‘x’? ¿y qué sucede con la expresión sum(!is.finite(x))? Detenidamente lee la documentación de is.vector(). ¿Para qué se prueba la función realmente? ¿Por qué la función is.atomic() no concuerda con la definición de vectores atómicos vista anteriormente? Compara y contraste setNames() con purrr::set_names(). Crea funciones que tomen un vector como entrada y devuelva: El último valor. ¿Deberás usar [ o [[?. Los elementos en posiciones pares. Cada elemento excepto el último valor. Sólo las posiciones pares (y sin valores perdidos (missing values)). ¿Por qué x[-which(x &gt; 0)] no es lo mismo que x[x &lt;= 0]? ¿Qué sucede cuando realizas un subset (subdivisión) con un entero positivo que es mayor que la longitud del vector? ¿Qué sucede cuando realizas un subset (subdivisión) con un nombre que no existe? 27.6 Vectores Recursivos (listas) Las listas son un escalon más en complejidad partiendo de los vectores atómicos, debido a que las listas pueden contener otras listas. Lo cual las hace adecuadas para representar una estructura jerárquica o de tipo árbol. Puedes crear una lista con ´list()´: x &lt;- list(1, 2, 3) (del inglés lista) x Un herramienta muy útil para trabajar con listas es ´str()´ ya que se enfoca en la estructura, no en los contenidos. str(x) #&gt; Named num [1:3] 1 2 5 #&gt; - attr(*, &quot;names&quot;)= chr [1:3] &quot;abc&quot; &quot;def&quot; &quot;xyz&quot; x_nombrada &lt;- list(a = 1, b = 2, c = 3) str(x_nombrada) #&gt; List of 3 #&gt; $ a: num 1 #&gt; $ b: num 2 #&gt; $ c: num 3 A diferencia de los vectores atómicos, el objeto ´list()´ puede contener una variedad de diferentes objetos: y &lt;- list(&quot;a&quot;, 1L, 1.5, TRUE) str(y) #&gt; List of 4 #&gt; $ : chr &quot;a&quot; #&gt; $ : int 1 #&gt; $ : num 1.5 #&gt; $ : logi TRUE ¡Incluso las listas pueden contener otras listas! z &lt;- list(list(1, 2), list(3, 4)) str(z) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ : num 1 #&gt; ..$ : num 2 #&gt; $ :List of 2 #&gt; ..$ : num 3 #&gt; ..$ : num 4 27.7 Visualizando listas Para explicar funciones de manipulacion de listas más complicadas, es útil tener una representacion visual de las mismas. Por ejemplo, defino estas tres listas: x1 &lt;- list(c(1, 2), c(3, 4)) x2 &lt;- list(list(1, 2), list(3, 4)) x3 &lt;- list(1, list(2, list(3))) Y a continuación, las grafico: Existen tres principios, al momento de observer el gráfico anterior: 1. Las listas tienen esquinas redondeadas, en cambio, los vectores atómicos tienen esquinas cuadradas. 2. Los hijos son representados dentro de sus listas padres, y tienen un fondo ligeramente más oscuro para facilitar la visualización de la jerarquía. 3. No es importante la orientación de los hijos (p.ej. las filas o columnas), entonces utilizaré la orientacion de una fila o columna para almacenar espacio o incluso para ilustrar una propiedad importante en el ejemplo. Subdivisión (Subsetting) Existen tres maneras de subdividir una lista, lo cual ilustraré con una lista denominada ´a´: a &lt;- list(a = 1:3, b = &quot;a string&quot;, c = pi, d = list(-1, -5)) El corchete simple ´[´ extrae una sub-lista. Por lo que, el resultado siempre será una lista. str(a[1:2]) #&gt; List of 2 #&gt; $ a: int [1:3] 1 2 3 #&gt; $ b: chr &quot;a string&quot; str(a[4]) #&gt; List of 1 #&gt; $ d:List of 2 #&gt; ..$ : num -1 #&gt; ..$ : num -5 Al igual que con vectores, puedes subdividirla, en un vector lógico, de enteros o caracteres. El doble corchete ´[[´ extrae un solo componente de una lista. Y remueve un nivel de la jerarquía de la lista. str(a[[1]]) #&gt; int [1:3] 1 2 3 str(a[[4]]) #&gt; List of 2 #&gt; $ : num -1 #&gt; $ : num -5 El símbolo $ es un atajo para extraer elementos con nombres de una lista. Este funciona de modo similar al doble corchete´[[´ excepto que en el primer caso no es necesario el uso de comillas dobles. a$a #&gt; [1] 1 2 3 a[[&quot;a&quot;]] #&gt; [1] 1 2 3 La diferencia entre el corchete simple [ y el doble [[ es realmente importante para las listas, porque el doble [[ profundiza en una lista mientras que el simple [ retorna una nueva, lista más pequeña. Compara el código y el output de arriba con la representacion visual de la Figura @ref(fig:lists-subsetting). lists-subsetting, echo = FALSE, out.width = &quot;75%&quot;, fig.cap = &quot;Subdividir una lista, de manera visual.&quot;} knitr::include_graphics(&quot;diagrams_w_text_as_path/es/lists-subsetting.png&quot;) 27.8 Listas de Condimentos La diferencia entre ambos [ y [[ es muy importante, pero es muy fácil confundirlos. Para ayudarte a recordar, permiteme mostrarte un pimientero inusual Si este pimientero es tu lista x, entonces, `x[1] es un pimientero que contiene un simple paquete de pimienta: La expresión x[2] luciría del mismo modo, pero podría contener el segundo paquete. La expresión x[1:2] sería un pimientero que contiente dos paquetes de pimienta. La expresión x[[1]] es: Si quisieras obtener el contenido del paquete de pimiento, necesitarías utilizar la siguiente expresión `x[[1]][[1]: 27.8.1 Ejercicios 1.Dibuja las listas siguientes como sets anidados: 1. `list(a, b, list(c, d), list(e, f))` 1. `list(list(list(list(list(list(a))))))` 1.¿Qué pasaría si subdividieras un tibble como si fuera una lista? ¿Cuáles son las principales diferencias entre una lista y un tibble? 27.9 Atributos Cualquier vector puede contener metadata arbitraria adicional mediante sus atributos. Puedes pensar en los atributos como una lista de vectores que pueden ser adjuntadas a cualquier otro objeto. Puedes obtener y setear valores de atributos individuales con attr() o verlos todos al mismo tiempo con attributes(). x &lt;- 1:10 attr(x, &quot;saludo&quot;) #&gt; NULL attr(x, &quot;saludo&quot;) &lt;- &quot;Hola!&quot; attr(x, &quot; despedida&quot;) &lt;- &quot;Adiós!&quot; attributes(x) #&gt; $saludo #&gt; [1] &quot;Hola!&quot; #&gt; #&gt; $` despedida` #&gt; [1] &quot;Adiós!&quot; Existen tres atributos muy importantes que son utilizados para implementar partes fundamentals de R: 1. Los Nombres son utilizados para nombrar los elementos de un vector. 2. Las Dimensiones (o dims, denominación más corta) hacen que un vector se comporte como una matriz o arreglo. 3. Una Clase es utilizada para implementar el sistema S3 orientado a objetos. A los atributos nombres los vimos arriba, y no cubriremos las dimensiones porque no se usan matrices en este libro. Resta describir el atributo clase, el cual controla como las funciones genéricas funcionan. Las funciones genéricas son clave para la programacion orientada a objetos en R, porque ellas hacen que las funciones se comporten de manera diferente de acuerdo a las diferentes clases de inputs. Una discusión más profunda sobre programacion orientada a objetos no está contemplada en el ámbito de este libro, pero puedes leer más al respecto en el documento R Avanzado en: http://adv-r.had.co.nz/OO-essentials.html#s3. Así es como una función genérica típica luce: as.Date #&gt; function (x, ...) #&gt; UseMethod(&quot;as.Date&quot;) #&gt; &lt;bytecode: 0x4be5938&gt; #&gt; &lt;environment: namespace:base&gt; La llamada al método “UseMethod” significa que esta es una función genérica, y llamará a un metódo específico, una función, basada en la clase del primer argumento. (Todos los métodos son funciones; no todas las funciones son métodos). Puedes listar todos los métodos existentes para una función genérica utilizando la función: methods(): methods(&quot;as.Date&quot;) #&gt; [1] as.Date.character as.Date.default as.Date.factor #&gt; [4] as.Date.numeric as.Date.POSIXct as.Date.POSIXlt #&gt; [7] as.Date.vctrs_sclr* as.Date.vctrs_vctr* #&gt; see &#39;?methods&#39; for accessing help and source code Por ejemplo, si x es un vector de caracteres, as.Date() llamará a as.Date.character(); si es un factor, llamará a as.Date.factor(). Puedes ver la implementación específica de un método con: getS3method(): getS3method(&quot;as.Date&quot;, &quot;default&quot;) #&gt; function (x, ...) #&gt; { #&gt; if (inherits(x, &quot;Date&quot;)) #&gt; x #&gt; else if (is.logical(x) &amp;&amp; all(is.na(x))) #&gt; .Date(as.numeric(x)) #&gt; else stop(gettextf(&quot;do not know how to convert &#39;%s&#39; to class %s&quot;, #&gt; deparse(substitute(x)), dQuote(&quot;Date&quot;)), domain = NA) #&gt; } #&gt; &lt;bytecode: 0x2ce50e8&gt; #&gt; &lt;environment: namespace:base&gt; getS3method(&quot;as.Date&quot;, &quot;numeric&quot;) #&gt; function (x, origin, ...) #&gt; { #&gt; if (missing(origin)) #&gt; stop(&quot;&#39;origin&#39; must be supplied&quot;) #&gt; as.Date(origin, ...) + x #&gt; } #&gt; &lt;bytecode: 0x2ce0b10&gt; #&gt; &lt;environment: namespace:base&gt; Lo mas importante del S3 genérico; sistema OO, es decir, orientado a objetos; es la función print(): el cual controla como el objeto es impreso cuando tipeas su nombre en la consola. Otras funciones genéricas importantes son las funciones de subdivisión [, [[, and $. 27.10 Vectores Aumentados Los vectores atómicos y las listas son los bloques sobre los que se construyen otros tipos importantes de vectores como los tipos factores y fechas (dates). A estos vectores , los llamo __ vectores aumentados __, porque son vectores con attributos adicionales, incluyendo la clase. Los vectores aumentados tienen una clase, por ello se comportan de manera diferente a los vectores atómicos sobre los cuales son construidos. En este libro, hacemos uso de cuatro importantes vectores aumentados: • Los Factores • Las Dates (Fechas) • Los Date-times (Fecha-tiempo) • Los Tibbles Estos son descriptos a continuación: Factores Los factores son diseñados para representar datos categoricos que pueden tomar un set fijo de valores posibles, son construidos sobre los enteros, y tienen x &lt;- factor(c(&quot;ab&quot;, &quot;cd&quot;, &quot;ab&quot;), levels = c(&quot;ab&quot;, &quot;cd&quot;, &quot;ef&quot;)) typeof(x) #&gt; [1] &quot;integer&quot; attributes(x) #&gt; $levels #&gt; [1] &quot;ab&quot; &quot;cd&quot; &quot;ef&quot; #&gt; #&gt; $class #&gt; [1] &quot;factor&quot; 27.10.1 Dates and date-times (Fechas y Fecha – Hora) Las vectores del tipo date en R son vectores numéricos que representan el número de días desde el 1° de enero de 1970. x &lt;- as.Date(&quot;1971-01-01&quot;) unclass(x) #&gt; [1] 365 typeof(x) #&gt; [1] &quot;double&quot; attributes(x) #&gt; $class #&gt; [1] &quot;Date&quot; Los vectores date-time son vectores numéricos de clase POSIXct que representan el número de segundos desde el 1° de enero de 1970. (En caso de que te preguntes sobre “POSIXct”; “POSIXct” significa “Portable Operating System Interface, lo que significa “Interfaz portable de sistema operativo”; tiempo de calendario.) x &lt;- lubridate::ymd_hm(&quot;1970-01-01 01:00&quot;) unclass(x) #&gt; [1] 3600 #&gt; attr(,&quot;tzone&quot;) #&gt; [1] &quot;UTC&quot; typeof(x) #&gt; [1] &quot;double&quot; attributes(x) #&gt; $class #&gt; [1] &quot;POSIXct&quot; &quot;POSIXt&quot; #&gt; #&gt; $tzone #&gt; [1] &quot;UTC&quot; El atributo tzone es opcional. Este controla como se muestra la hora, y no hace referencia al tiempo en términos absolutos. attr(x, &quot;tzone&quot;) &lt;- &quot;US/Pacifico&quot; x #&gt; [1] &quot;1970-01-01 01:00:00&quot; attr(x, &quot;tzone&quot;) &lt;- &quot;US/ Oriental&quot; x #&gt; [1] &quot;1970-01-01 01:00:00&quot; Existe otro tipo de vector date-time llamado POSIXlt. Éstos son construidos en base a listas con nombres (named lists). y &lt;- as.POSIXlt(x) typeof(y) #&gt; [1] &quot;list&quot; attributes(y) #&gt; $names #&gt; [1] &quot;sec&quot; &quot;min&quot; &quot;hour&quot; &quot;mday&quot; &quot;mon&quot; &quot;year&quot; &quot;wday&quot; #&gt; [8] &quot;yday&quot; &quot;isdst&quot; &quot;zone&quot; &quot;gmtoff&quot; #&gt; #&gt; $class #&gt; [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; #&gt; #&gt; $tzone #&gt; [1] &quot;US/ Oriental&quot; &quot;&quot; &quot;&quot; Los vectores POSIXlts son pocos comunes dentro del paquete tidyverse. Surgen en base a R, porque son necesarios para extraer components específicos de una fecha, como el año o el mes. Desde que el paquete lubridate te provee helpers para efectuar dicha extracción, los vectores POSIXlts no son necesarios. Siempre es más sencillo trabajar con loa vectores POSIXct’s, por lo tanto si tenés un vector POSIXlt, deberías convertirlo a un vector regular del tipo data-time con lubridate::as_date_time(). 27.10.2 Tibbles Los Tibbles son listas aumentadas: los cuales tienen las clases “tbl_df”, “tbl” y “data.frame”, y los atributos names (para nombrar una columna) y row.names (para nombrar una fila): tb &lt;- tibble::tibble(x = 1:5, y = 5:1) typeof(tb) #&gt; [1] &quot;list&quot; attributes(tb) #&gt; $names #&gt; [1] &quot;x&quot; &quot;y&quot; #&gt; #&gt; $row.names #&gt; [1] 1 2 3 4 5 #&gt; #&gt; $class #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; La diferencia entre un tibble y una lista, consiste en que todos los elementos de un data frame deben ser vectores con la misma longitud. Por lo tanto, todas las funciones que utilizan tibbles refuerzan esta condición. Además, los data.frames tradicionales tienen una estructura muy similar a los tibbles: df &lt;- data.frame(x = 1:5, y = 5:1) typeof(df) #&gt; [1] &quot;list&quot; attributes(df) #&gt; $names #&gt; [1] &quot;x&quot; &quot;y&quot; #&gt; #&gt; $class #&gt; [1] &quot;data.frame&quot; #&gt; #&gt; $row.names #&gt; [1] 1 2 3 4 5 Se puede decir, que la diferencia principal entre ambos es la clase. Debido a que un tibble incluye el tipo “data.frame” lo que significa que los tibbles heredan el comportamiento regular de un data frame por defecto. Ejecicios: 1. ¿Qué valor retorna la siguiente expresión hms::hms(3600)? ¿Cómo se muestra? ¿Cuál es la tipo primario sobre el cual se basa el vector aumentado? ¿Qué atributos utiliza el mismo? 2. Intenta y crea un tibble que tenga columnas con diferentes longitudes ¿Qué es lo que ocurre? 3. Teniendo en cuenta "],
["visualizacion-de-datos.html", "28 Visualización de datos 28.1 Introducción 28.2 Primeros pasos 28.3 Mapeos estéticos 28.4 Problemas comúnes 28.5 Separar en facetas 28.6 Objetos geométricos 28.7 Transformaciones estadísticas 28.8 Ajustes de posición 28.9 Sistemas de coordenadas 28.10 La gramática de gráficos en capas", " 28 Visualización de datos 28.1 Introducción “Un simple gráfico ha brindado más información a la mente del analista de datos que cualquier otro dispositivo”. — John Tukey En este capítulo aprenderás cómo visualizar tus datos usando el paquete ggplot2. De los muchos sistemas que posee R para hacer gráficos, ggplot2 es uno de los más elegantes y versátiles. Esto se debe a que ggplot2 implementa un sistema coherente para describir y construir gráficos, conocido como la gramática de gráficos. Con ggplot2 puedes hacer más cosas en menor tiempo, aprendiendo un único sistema y aplicándolo en diferentes ámbitos. Si deseas obtener más información sobre los fundamentos teóricos de ggplot2 antes de comenzar, te recomendamos leer “La gramática de gráficos en capas”, http://vita.had.co.nz/papers/layered-grammar.pdf. 28.1.1 Prerrequisitos Este capítulo se centra en ggplot2, uno de los paquetes principales de tidyverse. Para acceder a los conjuntos de datos, las páginas de ayuda y las funciones que utilizaremos en este capítulo, debes cargar tidyverse ejecutando este código: library(tidyverse) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() Esa única línea de código carga el núcleo del tidyverse; compuesto por los paquetes que usarás en casi todos tus análisis de datos. Al correr esta línea también verás cuáles funciones de tidyverse pueden tener conflicto con funciones de R base (o de otros paquetes que puedas haber cargado previamente). Si ejecutas este código y recibes el mensaje de error “no hay ningún paquete llamado ‘tidyverse’”, primero deberás instalarlo y luego ejecutar library() una vez más. install.packages(&quot;tidyverse&quot;) library(tidyverse) Solo necesitas instalar el paquete una única vez, pero debes volver a cargarlo siempre que inicies una nueva sesión. Cuando necesitemos especificar la procedencia de una función (o un conjunto de datos), usaremos el formato especial paquete::funcion(). Por ejemplo, ggplot2::ggplot() dice explícitamente que estamos usando la función ggplot() del paquete ggplot2. 28.2 Primeros pasos Usemos nuestro primer gráfico para responder una pregunta: ¿Los automóviles con motores grandes consumen más combustible que los automóviles con motores pequeños? Probablemente ya tengas una respuesta, pero trata de responder de forma precisa. ¿Cómo es la relación entre el tamaño del motor y la eficiencia del combustible? ¿Es positiva? ¿Es negativa? ¿Es lineal o no lineal? 28.2.1 El data frame millas Puedes poner a prueba tu respuesta empleando el data frame millas que se encuentra en el paquete datos (datos::millas). Un data frame es una colección rectangular de variables (columnas) y observaciones (filas). El data frame millas contiene observaciones para 38 modelos de automóviles recopiladas por la Agencia de Protección Ambiental de los EE. UU. library(datos) millas #&gt; # A tibble: 234 x 11 #&gt; fabricante modelo motor anio cilindros transmision traccion ciudad #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 audi a4 1.8 1999 4 auto(l5) d 18 #&gt; 2 audi a4 1.8 1999 4 manual(m5) d 21 #&gt; 3 audi a4 2 2008 4 manual(m6) d 20 #&gt; 4 audi a4 2 2008 4 auto(av) d 21 #&gt; 5 audi a4 2.8 1999 6 auto(l5) d 16 #&gt; 6 audi a4 2.8 1999 6 manual(m5) d 18 #&gt; # … with 228 more rows, and 3 more variables: autopista &lt;int&gt;, #&gt; # combustible &lt;chr&gt;, clase &lt;chr&gt; Entre las variables en millas encontramos: motor. Tamaño del motor de un automóvil, en litros. autopista. La eficiencia del uso de combustible de un automóvil en la carretera, en millas por galón. Al recorrer la misma distancia, un automóvil de baja eficiencia consume más combustible que un automóvil de alta eficiencia. Para obtener más información sobre el data frame millas, puedes abrir la página de ayuda ejecutando ?millas. 28.2.2 Creando un gráfico con ggplot Para graficar millas, corre este código usando motor en el eje x y autopista en el eje y. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) El gráfico muestra una relación negativa entre el tamaño del motor (motor) y la eficiencia del combustible (autopista). En otras palabras, los vehículos con motores grandes usan más combustible. Este resultado, ¿confirma o refuta tu hipótesis acerca de la relación entre la eficiencia del combustible y el tamaño del motor? Para comenzar un gráfico con ggplot2 se utiliza la función ggplot(). ggplot() crea un sistema de coordenadas al cual puedes agregar capas. El primer argumento de ggplot() es el conjunto de datos para usar en el gráfico. Si corres ggplot(data = millas), obtendrás un gráfico vacío. Como no es muy interesante, no vamos a mostrarlo aquí. Para completar tu gráfico debes agregar una o más capas a ggplot(). La función geom_point() agrega una capa de puntos al gráfico, que crea un diagrama de dispersión (scatterplot). ggplot2 incluye muchas funciones geom, cada una de las cuales agrega un tipo de capa diferente a un gráfico. Aprenderás muchas de ellas a lo largo de este capítulo. Cada función geom en ggplot2 tiene un argumento de mapping. Este define cómo se “mapean” o se asignan las variables del conjunto de datos a propiedades visuales. El argumento de mapping siempre aparece emparejado con aes(), y los argumentos x e y dentro de aes() especifican qué variables asignar a los ejes x e y. ggplot2 busca la variable asignada en el argumento data, en este caso, millas. 28.2.3 Una plantilla de gráficos Convirtamos ahora este código en una plantilla reutilizable para hacer gráficos con ggplot2. Para hacer un gráfico, reemplaza las secciones entre corchetes en el siguiente código con un conjunto de datos, una función geom o una colección de mapeos. ggplot(data = &lt;DATOS&gt;) + &lt;GEOM_FUNCION&gt;(mapping = aes(&lt;MAPEOS&gt;)) El resto de este capítulo te mostrará cómo utilizar y adaptar esta plantilla para crear diferentes tipos de gráficos. Comenzaremos por el componente &lt;MAPEOS&gt; 28.2.4 Ejercicios Corre ggplot(data = millas). ¿Qué observas? ¿Cuántas filas hay en millas? ¿Cuántas columnas? ¿Qué describe la variable traccion? Lee la ayuda de ?millas para encontrar la respuesta. Realiza un gráfico de dispersión de autopista versus cilindros. ¿Qué sucede cuando haces un gráfico de dispersión de clase versus traccion? ¿Por qué no es útil este gráfico? 28.3 Mapeos estéticos “El mayor valor de una imagen es cuando nos obliga a observar lo que no esperabamos ver”. — John Tukey En el siguiente gráfico, un grupo de puntos (resaltados en rojo) parece quedar fuera de la tendencia lineal. Estos vehículos tienen un kilometraje mayor de lo que esperaríamos. ¿Cómo puedes explicar estos vehículos? Supongamos que estos automóviles son híbridos. Una forma de probar esta hipótesis es observando la variable que indica la clase de cada automóvil. La variable clase del conjunto de datos de millas clasifica los autos en grupos como compacto, mediano y SUV. Si los puntos periféricos corresponden a automóviles híbridos, deberían estar clasificados como compactos o, tal vez, subcompactos (ten en cuenta que estos datos se recopilaron antes de que los camiones híbridos y SUV se hicieran populares). Puedes agregar una tercera variable, como clase, a un diagrama de dispersión bidimensional asignándolo a una estética. Una estética es una propiedad visual de los objetos de un gráfico. La estética incluye cosas como el tamaño, la forma o el color de tus puntos. Puedes mostrar un punto (como el siguiente) de diferentes maneras cambiando los valores de sus propiedades estéticas. Como ya usamos la palabra “valor” para describir los datos, usemos la palabra “nivel” para describir las propiedades estéticas. Aquí cambiamos los niveles del tamaño, la forma y el color de un punto para que el punto sea pequeño, triangular o azul: El mapeo entre las propiedades estéticas del gráfico y las variables del conjunto de datos te permite comunicar información de los mismos. Por ejemplo, puedes asignar los colores de los puntos de acuerdo con la variable clase para indicar la clase de cada automóvil. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, color = clase)) (Si prefieres el inglés británico, como Hadley, puedes usar colour en lugar de color). Para mapear (o asignar) una estética a una variable, debes asociar el nombre de la estética al de la variable dentro de aes(). ggplot2 asignará automáticamente un nivel único de la estética (en este ejemplo, un color) a cada valor único de la variable. Este proceso es conocido como escalamiento (scaling). ggplot2 acompañará el gráfico con una leyenda que explica qué niveles corresponden a qué valores. Los colores revelan que muchos de los puntos inusuales son los automóviles de dos asientos. ¡Estos automóviles no parecen híbridos, y son, de hecho, automóviles deportivos! Los automóviles deportivos tienen motores grandes, así como las camionetas todo terreno o pickups, a diferencia de los automóviles pequeños, medianos y compactos, lo que mejora su consumo de gasolina. En retrospectiva, es poco probable que estos automóviles sean híbridos ya que tienen motores grandes. En el ejemplo anterior, asignamos la variable clase a la estética del color , pero podríamos haber asignado a la estética del tamaño de la misma manera. En este caso, el tamaño exacto de cada punto revelaría clase. Recibimos aquí una advertencia (warning), porque mapear una variable desordenada (clase) a una estética ordenada (size) no es una buena idea. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, size = clase)) #&gt; Warning: Using size for a discrete variable is not advised. También podríamos haber asignado la clase a la estética alfa, que controla la transparencia de los puntos o a la estética shape que controla la forma (shape) de los puntos. # Izquierda ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, alpha = clase)) # Derecha ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, shape = clase)) ¿Qué pasó con los SUV? ggplot2 solo puede usar seis formas a la vez. De forma predeterminada, los grupos adicionales no se grafican cuando se emplea la estética de la forma. Para cada estética, se usa aes() para asociar el nombre de la estética con la variable seleccionada para graficar. La función aes() reúne cada una de las asignaciones estéticas utilizadas por una capa y las pasa al argumento de mapeo de la capa. La sintaxis resalta una visión útil sobre x e y: las ubicaciones de x e y de un punto son en sí mismas también estéticas, es decir propiedades visuales que se puede asignar a las variables para mostrar información sobre los datos. Una vez que asignas una estética, ggplot2 se ocupa del resto. El paquete selecciona una escala razonable para usar con la estética elegida y construye una leyenda que explica la relación entre niveles y valores. Para la estética x e y, ggplot2 no crea una leyenda, pero crea una línea que delimita el eje con sus marcas de graduación y una etiqueta. La línea del eje actúa como una leyenda; explica el mapeo entre ubicaciones y valores. También puedes fijar las propiedades estéticas de tu geom manualmente. Por ejemplo, podemos hacer que todos los puntos del gráfico sean azules: ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista), color = &quot;blue&quot;) Aquí, el color no transmite información sobre una variable, sino que cambia la apariencia del gráfico. Para establecer una estética de forma manual, debes usar el nombre de la estética como un argumento de la función geom; es decir, va fuera de aes(). Tendrás que elegir un nivel que tenga sentido para esa estética: El nombre de un color como una cadena de caracteres. El tamaño de un punto en mm. La forma de un punto como un número, como se muestra en la Figura 28.1. Figure 28.1: R tiene 25 formas de default que están identificadas por números. Hay algunas que parecen duplicados: por ejemplo 0, 15 y 22 son todos cuadrados. La diferencia viene de la interacción entre las estéticas color y fill (relleno). Las formas vacías (0–14) tienen un borde determinado por color; las formas sólidas (15–18) están rellenas con color; las formas rellenas (21–24) tienen un borde de color y están renellas por fill. 28.3.1 Ejercicios ¿Qué no va bien en este código? ¿Por qué hay puntos que no son azules? ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, color = &quot;blue&quot;)) ¿Qué variables en millas son categóricas? ¿Qué variables son continuas? (Sugerencia: escribe ? millas para leer la documentación de ayuda para este conjunto de datos). ¿Cómo puedes ver esta información cuando ejecutas millas? Asigna una variable continua a color, size, y shape. ¿Cómo se comportan estas estéticas de manera diferente para variables categóricas y variables continuas? ¿Qué ocurre si asignas o mapeas la misma variable a múltiples estéticas? ¿Qué hace la estética stroke? ¿Con qué formas trabaja? (Sugerencia: consultar ?geom_point) ¿Qué ocurre si se asigna o mapea una estética a algo diferente del nombre de una variable, como ser aes(color = motor &lt; 5)? 28.4 Problemas comúnes Es probable que encuentres problemas con los primeros códigos que ejecutes en R, e. No te preocupes, es lo más común. He estado escribiendo código en R durante años, ¡y todos los días sigo escribiendo código que no funciona! Comienza comparando cuidadosamente el código que estás ejecutando con el código en este libro. R es extremadamente exigente, y un carácter fuera de lugar puede marcar la diferencia. Asegúrate de que cada ( coincida con un ) y cada &quot; esté emparejado con otro&quot;. Algunas veces ejecutarás el código y no pasará nada. Comprueba la parte izquierda de tu consola: si es un +, significa que R no cree que hayas escrito una expresión completa y está esperando que la termines. En este caso, normalmente es fácil comenzar nuevamente desde cero presionando ESCAPE para cancelar el procesamiento del comando actual. Un problema común al crear gráficos con ggplot2 es colocar el + en el lugar equivocado: debe encontrarse al final de la línea, no al inicio. En otras palabras, asegúrate de no haber escrito accidentalmente un código como este: ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) Si esto no resuelve el problema, prueba la ayuda. Puedes obtener ayuda sobre cualquier función R ejecutando ?nombre_de_la_funcion en la consola, o seleccionando el nombre de la función y presionando F1 en RStudio. No te preocupes si la ayuda no te parece tan útil, trata entonces de saltar a los ejemplos y buscar un pedazo de código que coincida con lo que intentas hacer. Si eso no ayuda, lee cuidadosamente el mensaje de error. ¡A veces la respuesta estará oculta allí! Cuando eres nuevo en R, la respuesta puede estar en el mensaje de error, pero aún no sabes cómo entenderlo. Otra gran herramienta es Google: intenta buscar allí el mensaje de error, ya que es probable que otra persona haya tenido el mismo problema y haya obtenido ayuda en línea. 28.5 Separar en facetas Una forma de agregar variables adicionales es con las estéticas. Otra forma particularmente útil para las variables categóricas consiste en dividir el gráfico en facetas, sub-gráficos que muestran cada uno un subconjunto de los datos. Para separar en facetas un gráfico según una sola variable, usa facet_wrap() - del inglés envolver una faceta. El primer argumento de facet_wrap() debería ser una fórmula creada con ~ seguido por el nombre de una de las variable (aquí “fórmula” es el nombre de un tipo de estructura en R, no un sinónimo de “ecuación”). La variable que uses en facet_wrap() debe ser discreta. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_wrap(~ clase, nrow = 2) Para separar en facetas un gráfico según las combinaciones de dos variables, agregua facet_grid() a tu código del gráfico. El primer argumento de facet_grid() también corresponde a una fórmula. Esta vez, la fórmula debe contener dos nombres de variables separados por un ~. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(traccion ~ cilindros) Si prefieres no separar en facetas las filas o columnas, remplaza por un . el nombre de alguna de las variables, por ejemplo + facet_grid(. ~ cyl). 28.5.1 Ejercicios Qué ocurre si intentas separar en facetas a una variable continua? ¿Qué significan las celdas vacías que aparecen en el gráfico generado usando facet_grid(traccion ~ cilindros)? ¿Cómo se relacionan con este gráfico? ggplot(data = millas) + geom_point(mapping = aes(x = traccion, y = cilindros)) ¿Qué gráfica el siguiente código? ¿Qué hace . ? ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(traccion ~ .) ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(. ~ cilindros) Mira de nuevo el primer gráfico en facetas presentado en esta sección: ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_wrap(~ clase, nrow = 2) ¿Cuáles son las ventajas de separar en facetas en lugar de aplicar una estética de color? ¿Cuáles son las desventajas? ¿Cómo cambiaría este balance si tuvieras un conjunto de datos más grande? Lee ?facet_wrap. ¿Qué hace nrow? ¿Qué hace ncol? ¿Qué otras opciones controlan el diseño de los paneles individuales? ¿Por qué facet_grid() no tiene argumentos nrow y ncol? Cuando usas facet_grid(), generalmente deberías poner la variable con un mayor número de niveles únicos en las columnas. ¿Por qué? 28.6 Objetos geométricos ¿Cómo son estos dos gráficos similares? Ambos gráficos contienen las mismas variables x e y, y describen los mismos datos. Pero los gráficos no son idénticos. Cada gráfico usa un objeto visual diferente para representar los datos. En la sintaxis ggplot2, decimos que usan diferentes geoms. Un geom es el objeto geométrico usado para representar datos de forma gráfica. La gente a menudo llama los gráficos por el tipo de geom que utiliza. Por ejemplo, los diagramas de barras usan geoms de barra (bar), los diagramas de líneas usan geoms de línea (line), los diagramas de caja usan geoms de diagrama de caja (boxplot), y así sucesivamente. En inglés, los diagramas de puntos (llamados scatterplots) rompen la tendencia; ellos usan geom de punto (o point). Como vemos arriba, puedes usar diferentes geoms para graficar los mismos datos. La gráfica de la izquierda usa el geom de punto (geom_point()), y la gráfica de la derecha usa el geom liso (geom_smooth()), una línea suave ajustada a los datos. Para cambiar el geom de tu gráfico, modifica la función geom que acompaña a ggplot(). Por ejemplo, para hacer los gráficos que se muestran arriba, puedes usar este código: # izquierda ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) # derecha ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) Cada función geom en ggplot2 toma un argumento de mapping. Sin embargo, no todas las estéticas funcionan con todos los geom. Podrías establecer la forma para un punto, pero no podrías establecer la “forma” de una línea. Por otro lado, para una línea es posible elegir el tipo de línea (linetype). geom_smooth() dibujará una línea diferente, con un tipo de línea diferente, para cada valor único de la variable que asignes al tipo de línea. ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, linetype = traccion)) Aquí geom_smooth() separa los automóviles en tres líneas en función de su valor de traccion, que describe el tipo de transmisión de un automóvil. Una línea describe todos los puntos con un valor de 4, otra línea los de valor d, y una tercera línea describe los puntos con un valor t. Aquí, 4 significa tracción en las cuatro ruedas, d tracción delantera y t tracción trasera. Si esto suena extraño, podemos hacerlo más claro al superponer las líneas sobre los datos brutos y luego colorear todo según traccion. ¡Observa que generamos un gráfico que contiene dos geoms! Si esto te emociona, abróchate el cinturón. En la siguiente sección aprenderemos cómo colocar múltiples geoms en el mismo gráfico. ggplot2 proporciona más de 30 geoms, y los paquetes de extensión proporcionan aún más (consulta https://www.ggplot2-exts.org para obtener una muestra). La mejor forma de obtener un panorama completo sobre las posibilidades que brinda ggplot2 es consultando la hoja de referencia (cheatsheet), que puedes encontrar en http://rstudio.com/cheatsheets. Para obtener más información sobre un tipo dado de geoms, usa la ayuda: ?geom_smooth. Muchos geoms, tal como geom_smooth(), usan un único objeto geométrico para mostrar múltiples filas de datos. Para estos geoms, puedes asignar la estética de group a una variable categórica para graficar múltiples objetos. ggplot2 representará un objeto distinto por cada valor único de la variable de agrupamiento. En la práctica, ggplot2 agrupará automáticamente los datos para estos geoms siempre que se asigne una estética a una variable discreta (como en el ejemplo del tipo de línea o linetype). Es conveniente confiar en esta característica porque la estética del grupo en sí misma no agrega una leyenda o características distintivas a los geoms. ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista)) ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, group = traccion)) ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, color = traccion), show.legend = FALSE) Para mostrar múltiples geoms en el mismo gráfico, agrega varias funciones geom a ggplot(): ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + geom_smooth(mapping = aes(x = motor, y = autopista)) Esto introduce sin embargo cierta duplicación en nuestro código. Imagina que deseas cambiar el eje y para mostrar ciudad en lugar de autopista. Necesitarías cambiar la variable en dos lugares, y podrías olvidarte de actualizar uno. Puedes evitar este tipo de repetición pasando un conjunto de mapeos a ggplot(). ggplot2 tratará estos mapeos como mapeos globales que se aplican a cada geom en el gráfico. En otras palabras, este código producirá la misma gráfica que el código anterior: ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point() + geom_smooth() Si colocas mapeos en una función geom, ggplot2 los tratará como mapeos locales para la capa. Estas asignaciones serán usadas para extender o sobrescribir los mapeos globales de solo esa capa. Esto permite mostrar diferentes estéticas en diferentes capas. ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point(mapping = aes(color = clase)) + geom_smooth() La misma idea se puede emplear para especificar distintos conjuntos de datos (data) para cada capa. Aquí, nuestra línea suave muestra solo un subconjunto del conjunto de datos de millas, los autos subcompactos. El argumento de datos locales en geom_smooth() anula el argumento de datos globales en ggplot() solo para esa capa. ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point(mapping = aes(color = clase)) + geom_smooth(data = filter(millas, clase == &quot;subcompacto&quot;), se = FALSE) (Aprenderás cómo funciona filter() en el próximo capítulo: por ahora, solo recuerda que este comando selecciona los automóviles subcompactos). 28.6.1 Ejercicios ¿Qué geom usarías para generar un gráfico de líneas? ¿Un diagrama de caja? ¿Un histograma? ¿Un gráfico de área? Ejecuta este código en tu mente y predice cómo se verá el output. Luego, ejecuta el código en R y verifica tus predicciones. ggplot(data = millas, mapping = aes(x = motor, y = autopista, color = traccion)) + geom_point() + geom_smooth(se = FALSE) ¿Qué muestra show.legend = FALSE? ¿Qué pasa si lo quitas? ¿Por qué crees que lo usé antes en el capítulo? ¿Qué hace el argumento se en geom_smooth()? ¿Se verán distintos estos gráficos? ¿Por qué sí o por qué no? ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point() + geom_smooth() ggplot() + geom_point(data = millas, mapping = aes(x = motor, y = autopista)) + geom_smooth(data = millas, mapping = aes(x = motor, y = autopista)) Recrea el código R necesario para generar los siguientes gráficos: 28.7 Transformaciones estadísticas A continuación, echemos un vistazo a un gráfico de barras. Los gráficos de barras parecen simples, pero son interesantes porque revelan algo sutil sobre los gráficos. Considera un gráfico de barras básico, como se realizó con geom_bar(). El siguiente cuadro muestra la cantidad total de diamantes en el conjunto de datos de diamantes, agrupados por la variable corte. El conjunto de datos de diamantes se encuentra en el paquete datos y contiene información sobre ~ 54000 diamantes, incluido el precio, el quilate, el color, la claridad y el corte de cada diamante. El gráfico muestra que hay más diamantes disponibles con cortes de alta calidad que con cortes de baja calidad. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte)) En el eje x, el gráfico muestra corte, una variable de diamantes. En el eje y muestra recuento, ¡pero el recuento no es una variable en diamantes! ¿De dónde viene el recuento? Muchos gráficos, como los diagramas de dispersión, grafican los valores brutos de su conjunto de datos. Otros gráficos, como los gráficos de barras, calculan nuevos valores para presentar: los gráficos de barras, los histogramas y los polígonos de frecuencia almacenan los datos y luego grafican los conteos de contenedores, sea el número de puntos que caen en cada contenedor. los suavizadores ajustan un modelo a los datos y luego grafican las predicciones del modelo. los diagramas de caja calculan un sólido resumen de la distribución y luego muestran un cuadro con formato especial. El algoritmo utilizado para calcular nuevos valores para un gráfico se llama stat, abreviatura en inglés de transformación estadística. La siguiente figura describe cómo funciona este proceso con geom_bar(). Puedes aprender qué stat usa cada geom inspeccionando el valor predeterminado para el argumento stat. Por ejemplo, ?geom_bar muestra que el valor predeterminado para stat es “count”, lo que significa que geom_bar() usa stat_count(). stat_count() está documentado en la misma página que geom_bar(), y si te desplazas hacia abajo puedes encontrar una sección llamada “Variables calculadas” (Computed variables). Eso describe cómo calcula dos nuevas variables: count y prop. Por lo general puedes usar geoms y estadísticas de forma intercambiable. Por ejemplo, puedes volver a crear la gráfica anterior usando stat_count() en lugar de geom_bar(): ggplot(data = diamantes) + stat_count(mapping = aes(x = corte)) Esto funciona porque cada geom tiene una estadística predeterminada; y cada estadística tiene un geom predeterminado. Esto significa que generalmente puedes usar geoms sin preocuparte por la transformación estadística subyacente. Hay tres razones por las que podrías necesitar usar una estadística explícitamente: Es posible que desees anular la estadística predeterminada. En el siguiente código, cambio la estadística de geom_bar() de recuento (el valor predeterminado) a identidad. Esto me permite asignar la altura de las barras a los valores brutos de una variable \\(y\\). Desafortunadamente, cuando la gente habla de gráficos de barras casualmente, podrían estar refiriéndose a este tipo de gráfico de barras, donde la altura de la barra ya está presente en los datos, o al gráfico de barras anterior, donde la altura de la barra se determina contando filas. demo &lt;- tribble( ~corte, ~freq, &quot;Regular&quot;, 1610, &quot;Bueno&quot;, 4906, &quot;Muy Bueno&quot;, 12082, &quot;Premium&quot;, 13791, &quot;Ideal&quot;, 21551 ) ggplot(data = demo) + geom_bar(mapping = aes(x = corte, y = freq), stat = &quot;identity&quot;) (No te preocupes si nunca has visto &lt;- o tribble(). Puede que seas capaz de adivinar su significado por el contexto. ¡Aprenderás lo que hacen exactamente pronto!) Es posible que desees anular el mapeo predeterminado de las variables transformadas a la estética. Por ejemplo, es posible que desees mostrar un gráfico de barras de proporciones, en lugar de un recuento: ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, y = ..prop.., group = 1)) Para encontrar las variables calculadas por la estadística, busca la sección de ayuda titulada “Variables calculadas”. Es posible que desees resaltar la transformación estadística en tu código. Por ejemplo, puedes usar stat_summary(), que resume los valores de y para cada valor único de x, para resaltar el resumen que se está computando: ggplot(data = diamantes) + stat_summary( mapping = aes(x = corte, y = profundidad), fun.ymin = min, fun.ymax = max, fun.y = median ) ggplot2 proporciona más de 20 estadísticas para que uses. Cada estadística es una función, por lo que puedes obtener ayuda de la manera habitual, por ejemplo: ?stat_bin. Para ver una lista completa de estadísticas disponibles para ggplot2, consulta la hoja de referencia. 28.7.1 Ejercicios ¿Cuál es el geom predeterminado asociado con stat_summary()? ¿Cómo podrías reescribir el gráfico anterior para usar esa función geom en lugar de la función stat? ¿Qué hace geom_col()? ¿Cómo es diferente a geom_bar()? La mayoría de los geoms y las estadísticas vienen en pares que casi siempre se usan en conjunto. Lee la documentación y has una lista de todos los pares. ¿Qué tienen en común? ¿Qué variables calcula stat_smooth()? ¿Qué parámetros controlan su comportamiento? En nuestro gráfico de barras de proporción , necesitamos establecer group = 1. ¿Por qué? En otras palabras, ¿cuál es el problema con estos dos gráficos? ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, y = ..prop..)) ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = color, y = ..prop..)) 28.8 Ajustes de posición Hay una pieza más de magia asociada con los gráficos de barras. Puede colorear un gráfico de barras usando la estética de color o, tal vez con el más útil fill: ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, colour = corte)) ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = corte)) Mira lo que sucede si asigna la estética de relleno a otra variable, como claridad: las barras se apilan automáticamente. Cada rectángulo de color representa una combinación de corte y claridad. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad)) El apilamiento se realiza automáticamente mediante el ajuste de posición especificado por el argumento position. Si no deseas un gráfico de barras apiladas , puedes usar una de las otras tres opciones: &quot;identity&quot;, &quot;dodge&quot; o &quot;fill&quot;, del inglés identidad, esquivar y llenar respectivamente. position = &quot;identity&quot; colocará cada objeto exactamente donde cae en el contexto del gráfico. Esto no es muy útil al momento de graficar barras, porque las superpone. Para ver esa superposición, debemos hacer que las barras sean ligeramente transparentes al configurar alfa a un valor pequeño, o completamente transparente al establecer fill = NA. ggplot(data = diamantes, mapping = aes(x = corte, fill = claridad)) + geom_bar(alpha = 1/5, position = &quot;identity&quot;) ggplot(data = diamantes, mapping = aes(x = corte, colour = claridad)) + geom_bar(fill = NA, position = &quot;identity&quot;) El ajuste de position = identity es más útil para geoms 2-D, como puntos, donde es la opción predeterminada. position = &quot;fill&quot; funciona como el apilamiento, pero hace que cada conjunto de barras apiladas tenga la misma altura. Esto hace que sea más fácil comparar proporciones entre grupos. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad), position = &quot;dodge&quot;) position = &quot;dodge&quot; coloca objetos superpuestos directamente uno al lado del otro. Esto hace que sea más fácil comparar valores individuales. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad), position = &quot;dodge&quot;) Hay otro tipo de ajuste que no es útil para gráficos de barras, pero puede ser muy útil para diagramas de dispersión. Recuerda nuestro primer diagrama de dispersión. ¿Notaste que la trama muestra solo 126 puntos, a pesar de que hay 234 observaciones en el conjunto de datos? Los valores de las variables autopista y motor se redondean de modo que los puntos aparecen en una cuadrícula y muchos se superponen entre sí. Este problema se conoce como sobregraficado (overplotting). Esta disposición hace que sea difícil ver dónde está la masa de datos. ¿Los puntos de datos se distribuyen equitativamente a lo largo de la gráfica, o hay una combinación especial de autopista y motor que contiene 109 valores? Puedes evitar esta grilla estableciendo el ajuste de posición en “jitter”. position = &quot;jitter&quot; agrega una pequeña cantidad de ruido aleatorio a cada punto. Esto dispersa los puntos ya que no es probable que dos puntos reciban la misma cantidad de ruido aleatorio. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista), position = &quot;jitter&quot;) Si bien agregar aleatoriedad a los puntos puede parecer una forma extraña de mejorar tu gráfico ya que hace que sea menos preciso a escalas pequeñas, lo hace ser más revelador a gran escala. Como esta es una operación tan útil, ggplot2 viene con una abreviatura de geom_point(position = &quot;jitter&quot;): geom_jitter(). Para obtener más información sobre ajustes de posición, busca la página de ayuda asociada con cada ajuste: ?position_dodge, ?position_fill, ?position_identity, ?position_jitter y ?position_stack. 28.8.1 Ejercicios ¿Cuál es el problema con este gráfico? ¿Cómo podrías mejorarlo? ggplot(data = millas, mapping = aes(x = ciudad, y = autopista)) + geom_point() ¿Qué parámetros de geom_jitter() controlan la cantidad de ruido? Compara y contrasta geom_jitter() con geom_count() ¿Cuál es el ajuste de posición predeterminado de geom_boxplot()? Crea una visualización del conjunto de datos de millas que lo demuestre. 28.9 Sistemas de coordenadas Los sistemas de coordenadas son probablemente la parte más complicada de ggplot2. El sistema predeterminado es el sistema de coordenadas cartesianas, donde las posiciones x e y actúan independientemente para determinar la ubicación de cada punto. Hay varios otros sistemas de coordenadas que ocasionalmente son útiles. coord_flip() cambia los ejes x e y. Esto es útil (por ejemplo), si quieres diagramas de caja horizontales. También es útil para etiquetas largas: es difícil ajustarlas sin superposición en el eje x. ggplot(data = millas, mapping = aes(x = clase, y = autopista)) + geom_boxplot() ggplot(data = millas, mapping = aes(x = clase, y = autopista)) + geom_boxplot() + coord_flip() coord_quickmap() establece la relación de aspecto correctamente para los mapas. Esto es muy importante si graficas datos espaciales con ggplot2 (tema que desafortunadamente no contamos con espacio para desarrollar en este libro). nz &lt;- map_data(&quot;nz&quot;) ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) + coord_quickmap() coord_polar() usa coordenadas polares. Las coordenadas polares revelan una conexión interesante entre un gráfico de barras y un gráfico de Coxcomb. bar &lt;- ggplot(data = diamantes) + geom_bar( mapping = aes(x = corte, fill = corte), show.legend = FALSE, width = 1 ) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar + coord_flip() bar + coord_polar() 28.9.1 Ejercicios Convierte un gráfico de barras apiladas en un gráfico circular usando coord_polar(). ¿Qué hace labs()? Lee la documentación. ¿Cuál es la diferencia entre coord_quickmap() y coord_map()? ¿Qué te dice la gráfica siguiente sobre la relación entre la ciudad y la autopista? ¿Por qué es coord_fixed() importante? ¿Qué hace geom_abline()? ggplot(data = millas, mapping = aes(x = ciudad, y = autopista)) + geom_point() + geom_abline() + coord_fixed() 28.10 La gramática de gráficos en capas En las secciones anteriores, aprendiste mucho más que cómo hacer diagramas de dispersión, gráficos de barras y diagramas de caja. Aprendiste una base que se puede usar para hacer cualquier tipo de gráfico con ggplot2. Para ver esto, agreguemos ajustes de posición, estadísticas, sistemas de coordenadas y facetas a nuestra plantilla de código: ggplot(data = &lt;DATOS&gt;) + &lt;GEOM_FUNCION&gt;( mapping = aes(&lt;MAPEOS&gt;), stat = &lt;ESTADISTICA&gt;, position = &lt;POSICION&gt; ) + &lt;FUNCION_COORDENADAS&gt; + &lt;FUNCION_FACETAS&gt; Nuestra nueva plantilla tiene siete parámetros que se corresponde con las palabras entre corchetes que aparecen en la plantilla. En la práctica, rara vez necesitas proporcionar los siete parámetros para hacer un gráfico porque ggplot2 proporcionará valores predeterminados útiles para todos excepto para los datos, las asignaciones y la función geom. Los siete parámetros en la plantilla componen la gramática de los gráficos, un sistema formal de construcción de gráficos. La gramática de los gráficos se basa en la idea de que puedes describir de manera única cualquier gráfico como una combinación de un conjunto de datos, un geom, un conjunto de asignaciones, una estadística, un ajuste de posición, un sistema de coordenadas y un esquema de facetado. Para ver cómo funciona esto, considera cómo podrías construir un gráfico básico desde cero: podrías comenzar con un conjunto de datos y luego transformarlo en la información que deseas mostrar (con una estadística). A continuación, podrías elegir un objeto geométrico para representar cada observación en los datos transformados. Luego podrías usar las propiedades estéticas de los geoms para representar variables de los datos. Asignarías los valores de cada variable a los niveles de una estética. Posteriormente, seleccionarías un sistema de coordenadas para colocar los geoms. Podrías utilizar la ubicación de los objetos (que es en sí misma una propiedad estética) para mostrar los valores de las variables x e y. Ya en este punto podrías tener un gráfico completo, pero también podrías ajustar aún más las posiciones de los geoms dentro del sistema de coordenadas (un ajuste de posición) o dividir el gráfico en subtramas (facetas). También podrías extender el gráfico agregando una o más capas adicionales, donde cada capa adicional usaría un conjunto de datos, un geom, un conjunto de asignaciones, una estadística y un ajuste de posición. Puedes usar este método para construir cualquier gráfico que imagines. En otras palabras, puedes usar la plantilla de código aprendiste en este capítulo para construir cientos de miles de gráficos únicos. "],
["flujo-de-trabajo-conocimientos-basicos.html", "29 Flujo de trabajo: conocimientos básicos 29.1 Conocimientos básicos de programación 29.2 La importancia de los nombres 29.3 Usando funciones 29.4 Práctica", " 29 Flujo de trabajo: conocimientos básicos Ya tienes un poco de experiencia ejecutando código R. No te he dado demasiados detalles, pero es evidente que has podido resolver lo básico ¡o ya habrías arrojado lejos este libro en un acceso de frustración! Es natural frustrarte cuando empiezas a programar en R debido a que es muy estricto en cuanto a la puntuación, e incluso un único caracter fuera de lugar provocará que se queje. Así como deberías esperar sentir un poco de frustración, confía en que esta sensación es normal y transitoria: le pasa a todos y la única forma de superarla es seguir intentando. Antes de avanzar vamos a asegurarnos de que tengas una base sólida en ejecutar código R, y que conozcas algunas de las características más útiles de RStudio. 29.1 Conocimientos básicos de programación Revisemos algunos conocimientos básicos que omitimos hasta ahora para que pudieras empezar a hacer gráficos lo más rápido posible. Puedes usar R como una calculadora: 1 / 200 * 30 #&gt; [1] 0.15 (59 + 73 + 2) / 3 #&gt; [1] 44.7 sin(pi / 2) #&gt; [1] 1 donde sin calcula por defecto la función trigonométrica seno Puedes crear objetos nuevos usando &lt;-: x &lt;- 3 * 4 Todas las instrucciones en R en las que crees objetos, instrucciones de asignación, tienen la misma estructura: nombre_objeto &lt;- valor Cuando leas esa línea de código di mentalmente “nombre_objeto recibe valor” Harás una gran cantidad de asignaciones y &lt;- es incómodo de escribir. Que no te gane la pereza de usar =: sí, funcionará, pero provocará confusión más adelante. En cambio, usa el atajo de teclado de RStudio Alt + - (signo menos). RStudio automágicamente rodeará &lt;- con espacios, lo que es un buena costumbre para dar formato al código. El código puede ser horrible para leer incluso en un buen día, por lo que ayudará a tu vista usar espacios. 29.2 La importancia de los nombres Los nombres de los objetos deben comenzar con una letra, y sólo pueden contener letras, números, _ y .. Es mejor que los nombres sean descriptivos. Por eso necesitarás una convención para usar más de una palabra. Yo recomiendo guion_bajo donde las palabras en minúscula y sin tilde se separan con _. yo_uso_guion_bajo OtraGenteUsaMayusculas algunas.personas.usan.puntos Y_algunasPocas.Personas_RENIEGANdelasconvenciones Volveremos a tratar el estilo del código más adelante, en funciones. Puedes examinar un objeto escribiendo su nombre: x #&gt; [1] 12 Hagamos otra asignación: este_es_un_nombre_muy_largo &lt;- 2.5 Para examinar este objeto utiliza la capacidad de RStudio para completar: escribe “este”, presiona TAB, agrega caracteres hasta conseguir una única opción y finaliza apretando Enter. ¡Oh, cometiste un error! este_es_un_nombre_muy_largo debería valer 3.5 y no 2.5. Usa otro atajo del teclado para corregirlo. Escribe “este”, luego presiona Cmd/Ctrl + ↑. Aparecerá una lista con todas las instrucciones que has escrito que empiezan con esas letras. Usa las flechas para navegar y presiona Enter para reescribir la instrucción elegida. Cambia 2.5 por 3.5 y vuelve a ejecutar la instrucción. Hagamos una asignación más: viva_r &lt;- 2 ^ 3 Probemos examinar el objeto viv_r #&gt; Error: object &#39;viv_r&#39; not found viva_R #&gt; Error: object &#39;viva_R&#39; not found Los mensajes de error señalan que R no encontró entre los objetos definidos ninguno que se llame viv_r ni viva_R. Existe un acuerdo implícito entre tú y R: R hará todos los tediosos cálculos por ti, pero a cambio tu debes dar las instrucciones con total precisión. Cada caracter es importante. Si está en mayúscula o minúscula es importante. 29.3 Usando funciones R tiene una gran colección de funciones integradas que se usan así: nombre_funcion(arg1 = val1, arg2 = val2, ...) Probemos usar seq() que construye secuencias regulares de números y, mientras tanto, aprendemos otras características útiles de RStudio. Escribe se y presiona TAB. Una ventana emergente te mostrará opciones para completar tu instrucción. Especifica seq() agregando caracteres que permitan desambiguar (agrega una q), o usando las flechas ↑/↓. Si necesitas ayuda, presiona F1 para obtener información detallada en la pestaña de ayuda del panel inferior derecho. Presiona TAB una vez más cuando hayas seleccionado la función que quieras. RStudio colocará por tí paréntesis de apertura (() y cierre ()) de a pares. Escribe los argumentos 1, 10 y presiona Enter. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Escribe este código y observa que RStudio también te asiste al utilizar comillas: x &lt;- &quot;hola mundo&quot; Comillas y paréntesis siempre se usan de a pares. RStudio hace lo mejor que puede para ayudar, sin embargo puede ocurrir que nos enredemos y terminemos con una disparidad. Si esto ocurre R te mostrará el caracter de continuación “+”: &gt; x &lt;- &quot;hola + El + dice que R está esperando que completes la instrucción; no cree que hayas terminado. Usualmente eso señala que olvidaste escribir &quot; o ). Puedes agregar el caracter par faltante, o presionar ESCAPE para abandonar la expresión y escribirla de nuevo. Cuando realizas una asignación no obtienes la visualización del valor asignado. Es una tentación confirmar inmediatamente el resultado: y &lt;- seq(1, 10, length.out = 5) y #&gt; [1] 1.00 3.25 5.50 7.75 10.00 Esta acción común puede acortarse rodeando la instrucción con paréntesis, lo que resulta en una asignación e impresión en la pantalla. (y &lt;- seq(1, 10, length.out = 5)) #&gt; [1] 1.00 3.25 5.50 7.75 10.00 Mira tu entorno de trabajo en el panel superior derecho: Allí puedes ver todos los objetos que creaste. 29.4 Práctica ¿Por qué no funciona este código? mi_variable &lt;- 10 mi_varıable #&gt; Error in eval(expr, envir, enclos): object &#39;mi_varıable&#39; not found ¡Mira detenidamente! (Esto puede parecer un ejercicio inútil, pero entrenar tu cerebro para detectar incluso las diferencias más pequeñas será muy útil cuando comiences a programar.) Modifica cada una de las instrucciones de R a continuación para que puedan ejecutarse correctamente: library(tidyverse) ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) fliter(millas, cilindros = 8) filter(diamantes, quilate &gt; 3) Presiona Alt + Shift + K. ¿Qué ocurrió? ¿Cómo puedes llegar al mismo lugar utilizando los menús? "],
["workflow-projects.html", "30 Workflow: projects 30.1 What is real? 30.2 Where does your analysis live? 30.3 Paths and directories 30.4 RStudio projects 30.5 Summary", " 30 Workflow: projects One day you will need to quit R, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 30.1 What is real? As a beginning R user, it’s OK to consider your environment (i.e. the objects listed in the environment pane) “real”. However, in the long run, you’ll be much better off if you consider your R scripts as “real”. With your R scripts (and your data files), you can recreate the environment. It’s much harder to recreate your R scripts from your environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your R history. To foster this behaviour, I highly recommend that you instruct RStudio not to preserve your workspace between sessions: This will cause you some short-term pain, because now when you restart RStudio it will not remember the results of the code that you ran last time. But this short-term pain will save you long-term agony because it forces you to capture all important interactions in your code. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code. There is a great pair of keyboard shortcuts that will work together to make sure you’ve captured the important parts of your code in the editor: Press Cmd/Ctrl + Shift + F10 to restart RStudio. Press Cmd/Ctrl + Shift + S to rerun the current script. I use this pattern hundreds of times a week. 30.2 Where does your analysis live? R has a powerful notion of the working directory. This is where R looks for files that you ask it to load, and where it will put any files that you ask it to save. RStudio shows your current working directory at the top of the console: And you can print this out in R code by running getwd(): getwd() # &gt; [1] &quot;/Users/hadley/Documents/r4ds/r4ds&quot; As a beginning R user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be R’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organising your analytical projects into directories and, when working on a project, setting R’s working directory to the associated directory. I do not recommend it, but you can also set the working directory from within R: setwd(&quot;/path/to/my/CoolProject&quot;) But you should never do this because there’s a better way; a way that also puts you on the path to managing your R work like an expert. 30.3 Paths and directories Paths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ: The most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). R can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to R, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes. Absolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you. The last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory. 30.4 RStudio projects R experts keep all the files associated with a project together — input data, R scripts, analytical results, figures. This is such a wise and common practice that RStudio has built-in support for this via projects. Let’s make a project for you to use while you’re working through the rest of this book. Click File &gt; New Project, then: Call your project r4ds and think carefully about which subdirectory you put the project in. If you don’t store it somewhere sensible, it will be hard to find it in the future! Once this process is complete, you’ll get a new RStudio project just for this book. Check that the “home” directory of your project is the current working directory: getwd() # &gt; [1] /Users/hadley/Documents/r4ds/r4ds Whenever you refer to a file with a relative path it will look for it here. Now enter the following commands in the script editor, and save the file, calling it “diamonds.R”. Next, run the complete script which will save a PDF and CSV file into your project directory. Don’t worry about the details, you’ll learn them later in the book. library(tidyverse) ggplot(diamonds, aes(carat, price)) + geom_hex() ggsave(&quot;diamonds.pdf&quot;) write_csv(diamonds, &quot;diamonds.csv&quot;) Quit RStudio. Inspect the folder associated with your project — notice the .Rproj file. Double-click that file to re-open the project. Notice you get back to where you left off: it’s the same working directory and command history, and all the files you were working on are still open. Because you followed my instructions above, you will, however, have a completely fresh environment, guaranteeing that you’re starting with a clean slate. In your favorite OS-specific way, search your computer for diamonds.pdf and you will find the PDF (no surprise) but also the script that created it (diamonds.R). This is huge win! One day you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with R code and never with the mouse or the clipboard, you will be able to reproduce old work with ease! 30.5 Summary In summary, RStudio projects give you a solid workflow that will serve you well in the future: Create an RStudio project for each data analysis project. Keep data files there; we’ll talk about loading them into R in data import. Keep scripts there; edit them, run them in bits or as a whole. Save your outputs (plots and cleaned data) there. Only ever use relative paths, not absolute paths. Everything you need is in one place, and cleanly separated from all the other projects that you are working on. "],
["workflow-scripts.html", "31 Workflow: scripts 31.1 Running code 31.2 RStudio diagnostics 31.3 Practice", " 31 Workflow: scripts So far you’ve been using the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File, then R script, or using the keyboard shortcut Cmd/Ctrl + Shift + N. Now you’ll see four panes: The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. 31.1 Running code The script editor is also a great place to build up complex ggplot2 plots or long sequences of dplyr manipulations. The key to using the script editor effectively is to memorise one of the most important keyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R expression in the console. For example, take the code below. If your cursor is at █, pressing Cmd/Ctrl + Enter will run the complete command that generates not_cancelled. It will also move the cursor to the next statement (beginning with not_cancelled %&gt;%). That makes it easy to run your complete script by repeatedly pressing Cmd/Ctrl + Enter. library(dplyr) library(nycflights13) not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) Instead of running expression-by-expression, you can also execute the complete script in one step: Cmd/Ctrl + Shift + S. Doing this regularly is a great way to check that you’ve captured all the important parts of your code in the script. I recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see what packages they need to install. Note, however, that you should never include install.packages() or setwd() in a script that you share. It’s very antisocial to change settings on someone else’s computer! When working through future chapters, I highly recommend starting in the editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it. 31.2 RStudio diagnostics The script editor will also highlight syntax errors with a red squiggly line and a cross in the sidebar: Hover over the cross to see what the problem is: RStudio will also let you know about potential problems: 31.3 Practice Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it! What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out. "],
["domardatos-intro.html", "32 Introducción", " 32 Introducción En esta parte del libro, aprenderás cómo domar datos: el arte de tener tus datos en R de una forma conveniente para su visualización y modelado. La doma de datos es muy importante: ¡sin ella no puedes trabajar con tus propios datos! Existen tres partes principales para la doma de datos: Esta parte del libro continúa de la siguiente forma: En tibbles, aprenderás sobre la variante de data frame que usamos en este libro: el tibble. Conocerás qué los hace diferentes de los data frames comunes, y cómo puedes construirlos “a mano”. En [importar datos], aprenderás cómo traer tus datos del disco y dejarlos en R. Nos enfocaremos en los formatos rectangulares de texto plano, pero daremos referencias a paquetes que ayudan con otros tipos de datos. En datos ordenados, aprenderás una manera consistente de almacenar tus datos que facilita la transformación, la visualización y el modelado de los mismos. Aprenderás los principios subyacentes, y cómo poner tus datos en una forma ordenada. La doma de datos también abarca la transformación de los mismos, sobre lo cual ya has aprendido un poco. Ahora nos enfocaremos en nuevas habilidades para tres tipos de datos específicos que encontrarás frecuentemente en la práctica: Los [datos relacionales] te darán herramientas para trabajar con múltiples conjuntos de datos interrelacionados. Las [cadenas de caracteres] te introducirán en las expresiones regulares (regular expressions), las cuales son una herramienta poderosa para manipular cadenas de caracteres. Los Factores indican cómo R almacena los datos categóricos. Se usan cuando una variable tiene un conjunto fijo de posibles valores, o cuando quieres usar una cadena de caracteres en un orden distinto al alfabético. Las [Fechas y horas] te darán herramientas clave para trabajar con fechas y fecha-horas. "]
]
